{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GenJAX API Reference","text":"<p>JAX-based probabilistic programming with programmable inference.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install genjax\n</code></pre>"},{"location":"#modules","title":"Modules","text":"<ul> <li>genjax.core - Core functionality and Generative Function Interface</li> <li>genjax.distributions - Probability distributions</li> <li>genjax.inference.mcmc - Markov Chain Monte Carlo</li> <li>genjax.inference.smc - Sequential Monte Carlo</li> <li>genjax.pjax - Probabilistic JAX primitives</li> <li>genjax.adev - Automatic differentiation of expected values</li> <li>genjax.state - State interpreter</li> <li>genjax.sp - Structural primitives</li> </ul>"},{"location":"api/distributions/","title":"Distributions","text":"<p>GenJAX provides a comprehensive set of probability distributions that implement the Generative Function Interface. All distributions can be used directly in <code>@gen</code> functions with the <code>@</code> addressing operator.</p>"},{"location":"api/distributions/#continuous-distributions","title":"Continuous Distributions","text":""},{"location":"api/distributions/#normal-gaussian","title":"Normal (Gaussian)","text":"<pre><code>from genjax import normal\n\n# Standard normal\nx = normal(0, 1) @ \"x\"\n\n# With parameters\ny = normal(mu=5.0, sigma=2.0) @ \"y\"\n\n# In a model\n@gen\ndef model():\n    mean = normal(0, 10) @ \"mean\"\n    data = normal(mean, 1) @ \"data\"\n    return data\n</code></pre> <p>Parameters: - <code>mu</code>: Mean (location parameter) - <code>sigma</code>: Standard deviation (scale parameter, must be positive)</p>"},{"location":"api/distributions/#beta","title":"Beta","text":"<pre><code>from genjax import beta\n\n# Beta(2, 5)\np = beta(2, 5) @ \"probability\"\n\n# Uniform prior (Beta(1, 1))\nuniform_p = beta(1, 1) @ \"uniform\"\n</code></pre> <p>Parameters: - <code>alpha</code>: First shape parameter (must be positive) - <code>beta</code>: Second shape parameter (must be positive)</p> <p>Support: [0, 1]</p>"},{"location":"api/distributions/#gamma","title":"Gamma","text":"<pre><code>from genjax import gamma\n\n# Gamma(shape=2, rate=1)\nx = gamma(2, 1) @ \"x\"\n\n# For inverse scale parameterization\n# Gamma(shape=\u03b1, scale=1/\u03b2) has mean \u03b1/\u03b2\nprecision = gamma(1, 1) @ \"precision\"\n</code></pre> <p>Parameters: - <code>shape</code>: Shape parameter \u03b1 (must be positive) - <code>rate</code>: Rate parameter \u03b2 (must be positive)</p> <p>Support: (0, \u221e)</p>"},{"location":"api/distributions/#exponential","title":"Exponential","text":"<pre><code>from genjax import exponential\n\n# Exponential with rate 2.0\nwaiting_time = exponential(2.0) @ \"wait\"\n\n# Mean = 1/rate, so rate=0.1 gives mean=10\nlong_wait = exponential(0.1) @ \"long_wait\"\n</code></pre> <p>Parameters: - <code>rate</code>: Rate parameter \u03bb (must be positive)</p> <p>Support: [0, \u221e)</p>"},{"location":"api/distributions/#uniform","title":"Uniform","text":"<pre><code>from genjax import uniform\n\n# Uniform on [0, 1]\nu = uniform(0, 1) @ \"u\"\n\n# Uniform on [-5, 5]  \nx = uniform(-5, 5) @ \"x\"\n</code></pre> <p>Parameters: - <code>low</code>: Lower bound - <code>high</code>: Upper bound (must be greater than low)</p> <p>Support: [low, high]</p>"},{"location":"api/distributions/#dirichlet","title":"Dirichlet","text":"<pre><code>from genjax import dirichlet\nimport jax.numpy as jnp\n\n# Symmetric Dirichlet\nprobs = dirichlet(jnp.ones(3)) @ \"probs\"\n\n# Asymmetric Dirichlet\nalphas = jnp.array([1.0, 2.0, 3.0])\nweights = dirichlet(alphas) @ \"weights\"\n</code></pre> <p>Parameters: - <code>alpha</code>: Concentration parameters (array, all elements must be positive)</p> <p>Support: Simplex (sums to 1)</p>"},{"location":"api/distributions/#multivariate-normal","title":"Multivariate Normal","text":"<pre><code>from genjax import multivariate_normal\nimport jax.numpy as jnp\n\n# 2D standard normal\nx = multivariate_normal(\n    jnp.zeros(2), \n    jnp.eye(2)\n) @ \"x\"\n\n# With correlation\nmean = jnp.array([1.0, 2.0])\ncov = jnp.array([[1.0, 0.5], \n                 [0.5, 2.0]])\ny = multivariate_normal(mean, cov) @ \"y\"\n</code></pre> <p>Parameters: - <code>mean</code>: Mean vector - <code>cov</code>: Covariance matrix (must be positive definite)</p>"},{"location":"api/distributions/#discrete-distributions","title":"Discrete Distributions","text":""},{"location":"api/distributions/#bernoulli","title":"Bernoulli","text":"<pre><code>from genjax import bernoulli\n\n# Fair coin\ncoin = bernoulli(0.5) @ \"coin\"\n\n# Biased coin\nbiased = bernoulli(0.7) @ \"biased\"\n\n# In a model\n@gen\ndef coin_flips(n):\n    p = beta(1, 1) @ \"bias\"\n    flips = []\n    for i in range(n):\n        flip = bernoulli(p) @ f\"flip_{i}\"\n        flips.append(flip)\n    return flips\n</code></pre> <p>Parameters: - <code>p</code>: Probability of success (must be in [0, 1])</p> <p>Support: {0, 1} (False, True)</p>"},{"location":"api/distributions/#categorical","title":"Categorical","text":"<pre><code>from genjax import categorical\nimport jax.numpy as jnp\n\n# Three categories with equal probability\nx = categorical(jnp.ones(3) / 3) @ \"x\"\n\n# With specified probabilities\nprobs = jnp.array([0.1, 0.3, 0.6])\ncategory = categorical(probs) @ \"category\"\n\n# In a mixture model\n@gen\ndef mixture(n_components):\n    weights = dirichlet(jnp.ones(n_components)) @ \"weights\"\n\n    # Assign to components\n    assignments = []\n    for i in range(n_data):\n        z = categorical(weights) @ f\"z_{i}\"\n        assignments.append(z)\n    return assignments\n</code></pre> <p>Parameters: - <code>probs</code>: Probability vector (must sum to 1)</p> <p>Support: {0, 1, ..., len(probs)-1}</p>"},{"location":"api/distributions/#poisson","title":"Poisson","text":"<pre><code>from genjax import poisson\n\n# Poisson with rate 3.0\ncount = poisson(3.0) @ \"count\"\n\n# Modeling count data\n@gen\ndef count_model(exposure):\n    rate = gamma(2, 1) @ \"rate\"\n    counts = []\n    for i in range(len(exposure)):\n        count = poisson(rate * exposure[i]) @ f\"count_{i}\"\n        counts.append(count)\n    return counts\n</code></pre> <p>Parameters: - <code>rate</code>: Rate parameter \u03bb (must be positive)</p> <p>Support: {0, 1, 2, ...}</p>"},{"location":"api/distributions/#flip","title":"Flip","text":"<p>Alias for Bernoulli with boolean output:</p> <pre><code>from genjax import flip\n\n# Equivalent to bernoulli but more intuitive for booleans\nif flip(0.8) @ \"success\":\n    reward = normal(10, 1) @ \"reward\"\nelse:\n    reward = normal(0, 1) @ \"reward\"\n</code></pre>"},{"location":"api/distributions/#using-distributions-outside-gen-functions","title":"Using Distributions Outside <code>@gen</code> Functions","text":"<p>All distributions implement the full GFI and can be used directly:</p> <pre><code># Direct sampling (requires explicit key)\nfrom genjax import seed\nimport jax.random as random\n\nkey = random.PRNGKey(0)\nsample = seed(normal.simulate)(key, mu=0, sigma=1)\n\n# Log probability\nlog_prob, _ = normal.assess({\"value\": 1.5}, mu=0, sigma=1)\n\n# Generate with constraints  \ntrace, weight = normal.generate({\"value\": 2.0}, mu=0, sigma=1)\n</code></pre>"},{"location":"api/distributions/#custom-distributions","title":"Custom Distributions","text":"<p>You can create custom distributions by implementing the GFI:</p> <pre><code>from genjax import Distribution\nimport jax.numpy as jnp\n\nclass Laplace(Distribution):\n    \"\"\"Laplace (double exponential) distribution.\"\"\"\n\n    def sample(self, key, loc, scale):\n        u = random.uniform(key, minval=-0.5, maxval=0.5)\n        return loc - scale * jnp.sign(u) * jnp.log(1 - 2 * jnp.abs(u))\n\n    def log_density(self, value, loc, scale):\n        return -jnp.log(2 * scale) - jnp.abs(value - loc) / scale\n\n# Use in a model\n@gen\ndef robust_regression(x):\n    # Laplace errors for robust regression\n    intercept = normal(0, 10) @ \"intercept\"\n    slope = normal(0, 5) @ \"slope\"\n\n    errors = []\n    for i in range(len(x)):\n        # Would need to register as GenJAX distribution\n        error = custom_laplace(0, 1) @ f\"error_{i}\"\n        errors.append(error)\n\n    return intercept + slope * x + jnp.array(errors)\n</code></pre>"},{"location":"api/distributions/#distribution-parameters","title":"Distribution Parameters","text":""},{"location":"api/distributions/#shape-conventions","title":"Shape Conventions","text":"<ul> <li>Scalar parameters: Single values (e.g., <code>normal(0, 1)</code>)</li> <li>Vector parameters: Use JAX arrays (e.g., <code>dirichlet(jnp.ones(3))</code>)</li> <li>Matrix parameters: For multivariate distributions (e.g., <code>multivariate_normal(mean, cov)</code>)</li> </ul>"},{"location":"api/distributions/#broadcasting","title":"Broadcasting","text":"<p>GenJAX distributions support JAX broadcasting:</p> <pre><code># Sample multiple values with different means\nmeans = jnp.array([0.0, 1.0, 2.0])\nx = normal(means, 1.0) @ \"x\"  # Shape: (3,)\n\n# Different means and sigmas\nsigmas = jnp.array([0.5, 1.0, 2.0])  \ny = normal(means, sigmas) @ \"y\"  # Shape: (3,)\n</code></pre>"},{"location":"api/distributions/#common-patterns","title":"Common Patterns","text":""},{"location":"api/distributions/#hierarchical-models","title":"Hierarchical Models","text":"<pre><code>@gen\ndef hierarchical():\n    # Global parameters\n    global_mean = normal(0, 10) @ \"global_mean\"\n    global_std = gamma(1, 1) @ \"global_std\"\n\n    # Group-level parameters\n    group_means = []\n    for g in range(n_groups):\n        group_mean = normal(global_mean, global_std) @ f\"group_{g}_mean\"\n        group_means.append(group_mean)\n\n    # Observations\n    for g in range(n_groups):\n        for i in range(n_obs_per_group):\n            obs = normal(group_means[g], 1.0) @ f\"obs_{g}_{i}\"\n</code></pre>"},{"location":"api/distributions/#prior-predictive-sampling","title":"Prior Predictive Sampling","text":"<pre><code>@gen\ndef model():\n    # Priors\n    theta = beta(2, 2) @ \"theta\"\n\n    # Likelihood\n    successes = 0\n    for i in range(n_trials):\n        if bernoulli(theta) @ f\"trial_{i}\":\n            successes += 1\n\n    return successes\n\n# Sample from prior predictive\ntrace = model.simulate()\nprior_predictive_sample = trace.get_retval()\n</code></pre>"},{"location":"api/distributions/#posterior-predictive","title":"Posterior Predictive","text":"<pre><code># After inference, use posterior samples\nposterior_trace = inference_algorithm(model, data)\ntheta_posterior = posterior_trace.get_choices()[\"theta\"]\n\n# Generate new predictions\n@gen\ndef predictive(theta):\n    predictions = []\n    for i in range(n_future):\n        pred = bernoulli(theta) @ f\"pred_{i}\"\n        predictions.append(pred)\n    return predictions\n\npred_trace = predictive.simulate(theta=theta_posterior)\n</code></pre>"},{"location":"api/generative-functions/","title":"Generative Functions","text":"<p>Generative functions are the core abstraction in GenJAX. They represent probabilistic computations that can be executed, scored, and manipulated through a unified interface.</p>"},{"location":"api/generative-functions/#creating-generative-functions","title":"Creating Generative Functions","text":""},{"location":"api/generative-functions/#the-gen-decorator","title":"The <code>@gen</code> Decorator","text":"<p>Transform regular Python functions into generative functions:</p> <pre><code>from genjax import gen, normal, bernoulli\n\n@gen\ndef weather_model(temp_yesterday):\n    # Sample today's temperature\n    temp_today = normal(temp_yesterday, 5.0) @ \"temp\"\n\n    # Determine if it rains based on temperature\n    rain_prob = 1 / (1 + jnp.exp(0.1 * (temp_today - 20)))\n    rains = bernoulli(rain_prob) @ \"rains\"\n\n    return {\"temperature\": temp_today, \"rains\": rains}\n</code></pre>"},{"location":"api/generative-functions/#addressing-random-choices","title":"Addressing Random Choices","text":"<p>Use the <code>@</code> operator to assign addresses to random choices:</p> <pre><code>@gen\ndef model():\n    # Simple address\n    x = normal(0, 1) @ \"x\"\n\n    # Hierarchical addresses\n    for i in range(3):\n        # Creates addresses: \"group_0\", \"group_1\", \"group_2\"\n        group_mean = normal(0, 1) @ f\"group_{i}\"\n\n        for j in range(5):\n            # Creates: \"obs_0_0\", \"obs_0_1\", ..., \"obs_2_4\"\n            obs = normal(group_mean, 0.1) @ f\"obs_{i}_{j}\"\n</code></pre> <p>Avoid Address Collisions</p> <p>Each address at the same level must be unique. GenJAX will raise an error if you reuse addresses:</p> <pre><code>@gen\ndef bad_model():\n    x = normal(0, 1) @ \"x\"\n    y = normal(1, 1) @ \"x\"  # Error: address \"x\" already used!\n</code></pre>"},{"location":"api/generative-functions/#the-generative-function-interface","title":"The Generative Function Interface","text":"<p>All generative functions implement these methods:</p>"},{"location":"api/generative-functions/#simulate","title":"simulate","text":"<p>Forward sampling from the generative function:</p> <pre><code># Without arguments\ntrace = model.simulate()\n\n# With arguments\ntrace = weather_model.simulate(temp_yesterday=25.0)\n\n# Access the trace\nchoices = trace.get_choices()\nreturn_value = trace.get_retval()\n</code></pre>"},{"location":"api/generative-functions/#assess","title":"assess","text":"<p>Evaluate the log probability density at given choices:</p> <pre><code>choices = {\n    \"temp\": 22.0,\n    \"rains\": True\n}\n\nlog_prob, retval = weather_model.assess(choices, temp_yesterday=25.0)\n# log_prob = log p(temp=22.0, rains=True | temp_yesterday=25.0)\n</code></pre>"},{"location":"api/generative-functions/#generate","title":"generate","text":"<p>Generate a trace with some choices constrained:</p> <pre><code># Observe that it rained\nconstraints = {\"rains\": True}\n\ntrace, weight = weather_model.generate(constraints, temp_yesterday=25.0)\n# weight = log p(rains=True, temp) / q(temp | rains=True)\n</code></pre> <p>The weight is the incremental importance weight, useful for: - Importance sampling - Particle filtering - MCMC acceptance probabilities</p>"},{"location":"api/generative-functions/#update","title":"update","text":"<p>Update an existing trace with new constraints:</p> <pre><code># Original trace\ntrace = weather_model.simulate(temp_yesterday=25.0)\n\n# Update with new observation\nnew_constraints = {\"rains\": False}\nnew_trace, weight, discard = weather_model.update(\n    trace, \n    new_constraints, \n    temp_yesterday=26.0  # Can also change arguments\n)\n</code></pre>"},{"location":"api/generative-functions/#regenerate","title":"regenerate","text":"<p>Selectively regenerate parts of a trace:</p> <pre><code>from genjax import sel\n\n# Regenerate only the temperature\nselection = sel(\"temp\")\nnew_trace, weight, discard = weather_model.regenerate(\n    trace,\n    selection,\n    temp_yesterday=25.0\n)\n</code></pre>"},{"location":"api/generative-functions/#composing-generative-functions","title":"Composing Generative Functions","text":""},{"location":"api/generative-functions/#calling-other-generative-functions","title":"Calling Other Generative Functions","text":"<pre><code>@gen\ndef prior():\n    mean = normal(0, 10) @ \"mean\"\n    std = gamma(1, 1) @ \"std\"\n    return mean, std\n\n@gen\ndef model(n_obs):\n    # Call another generative function\n    mean, std = prior() @ \"prior\"\n\n    # Use the results\n    observations = []\n    for i in range(n_obs):\n        obs = normal(mean, std) @ f\"obs_{i}\"\n        observations.append(obs)\n\n    return jnp.array(observations)\n</code></pre>"},{"location":"api/generative-functions/#using-fixed-values","title":"Using Fixed Values","text":"<p>Wrap deterministic values to preserve them during trace operations:</p> <pre><code>from genjax import Fixed\n\n@gen\ndef model_with_fixed():\n    # This value won't be regenerated\n    fixed_param = Fixed(1.0) @ \"param\"\n\n    # This can be regenerated\n    x = normal(fixed_param, 1.0) @ \"x\"\n\n    return x\n</code></pre>"},{"location":"api/generative-functions/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"api/generative-functions/#mixture-models","title":"Mixture Models","text":"<pre><code>@gen\ndef mixture_model(data):\n    # Mixture weights\n    weights = dirichlet(jnp.ones(3)) @ \"weights\"\n\n    # Component parameters\n    means = []\n    for k in range(3):\n        mean = normal(0, 10) @ f\"mean_{k}\"\n        means.append(mean)\n\n    # Assign data to components\n    for i, datum in enumerate(data):\n        component = categorical(weights) @ f\"z_{i}\"\n        obs = normal(means[component], 1.0) @ f\"obs_{i}\"\n</code></pre>"},{"location":"api/generative-functions/#recursive-models","title":"Recursive Models","text":"<pre><code>@gen\ndef geometric(p, max_depth=100):\n    \"\"\"Sample from geometric distribution recursively.\"\"\"\n    flip = bernoulli(p) @ f\"flip_0\"\n\n    if flip:\n        return 0\n    else:\n        # Recursive call\n        rest = geometric(p, max_depth-1) @ \"rest\"\n        return 1 + rest\n</code></pre>"},{"location":"api/generative-functions/#state-space-models","title":"State Space Models","text":"<pre><code>@gen\ndef state_space_model(T, observations):\n    # Initial state\n    state = normal(0, 1) @ \"state_0\"\n\n    states = [state]\n    for t in range(1, T):\n        # State transition\n        state = normal(state, 0.1) @ f\"state_{t}\"\n        states.append(state)\n\n        # Observation\n        if observations[t] is not None:\n            obs = normal(state, 0.5) @ f\"obs_{t}\"\n            # Could add constraint: obs == observations[t]\n\n    return jnp.array(states)\n</code></pre>"},{"location":"api/generative-functions/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive addresses: Make debugging easier with meaningful names</li> <li>Avoid address collisions: Each address at the same level must be unique</li> <li>Minimize Python loops: Use JAX/GenJAX combinators when possible</li> <li>Type annotations: Help with debugging and documentation</li> <li>Return structured data: Return dictionaries or named tuples for clarity</li> </ol>"},{"location":"api/generative-functions/#common-pitfalls","title":"Common Pitfalls","text":"<p>Python Control Flow in JAX</p> <p>Avoid Python <code>if</code>/<code>for</code> statements when you need JAX compilation:</p> <pre><code># Bad - won't work with JAX transformations\n@gen\ndef bad_model(n):\n    for i in range(n):  # Python loop with traced value!\n        x = normal(0, 1) @ f\"x_{i}\"\n\n# Good - use Scan combinator\nfrom genjax import Scan\n\n@gen\ndef step(carry, i):\n    x = normal(0, 1) @ \"x\"\n    return carry, x\n\nmodel = Scan(step, const(n))\n</code></pre> <p>Performance Tips</p> <ul> <li>Use <code>Fixed</code> for values that don't need regeneration</li> <li>Batch operations with <code>vmap</code> instead of loops</li> <li>Prefer built-in distributions over custom implementations</li> </ul>"},{"location":"api/overview/","title":"Core API Overview","text":"<p>GenJAX provides a powerful and composable API for probabilistic programming. The core concepts are:</p>"},{"location":"api/overview/#generative-functions","title":"Generative Functions","text":"<p>The fundamental abstraction in GenJAX is the generative function - a probabilistic program that can be executed, scored, and manipulated through the Generative Function Interface (GFI).</p>"},{"location":"api/overview/#the-gen-decorator","title":"The <code>@gen</code> Decorator","text":"<p>Transform Python functions into generative functions:</p> <pre><code>from genjax import gen, normal\n\n@gen\ndef my_model(x):\n    # Sample from distributions using @ for addressing\n    z = normal(0, 1) @ \"z\"\n    y = normal(z * x, 0.1) @ \"y\"\n    return y\n</code></pre>"},{"location":"api/overview/#addressing-with","title":"Addressing with <code>@</code>","text":"<p>The <code>@</code> operator assigns addresses to random choices, creating a hierarchical namespace:</p> <pre><code>@gen\ndef hierarchical_model():\n    # Top-level choice\n    global_mean = normal(0, 1) @ \"global_mean\"\n\n    # Nested choices\n    for i in range(3):\n        local_mean = normal(global_mean, 0.5) @ f\"group_{i}/mean\"\n        for j in range(5):\n            obs = normal(local_mean, 0.1) @ f\"group_{i}/obs_{j}\"\n</code></pre>"},{"location":"api/overview/#generative-function-interface-gfi","title":"Generative Function Interface (GFI)","text":"<p>Every generative function implements these core methods:</p>"},{"location":"api/overview/#simulateargs-trace","title":"<code>simulate(args...) -&gt; Trace</code>","text":"<p>Forward sampling from the model:</p> <pre><code>trace = model.simulate(x=2.0)\nchoices = trace.get_choices()  # {\"z\": 0.5, \"y\": 1.1}\nretval = trace.get_retval()    # 1.1\n</code></pre>"},{"location":"api/overview/#assesschoices-args-log_density-retval","title":"<code>assess(choices, args...) -&gt; (log_density, retval)</code>","text":"<p>Evaluate the log probability density:</p> <pre><code>choices = {\"z\": 0.5, \"y\": 1.0}\nlog_prob, retval = model.assess(choices, x=2.0)\n</code></pre>"},{"location":"api/overview/#generateconstraints-args-trace-weight","title":"<code>generate(constraints, args...) -&gt; (trace, weight)</code>","text":"<p>Generate a trace with some choices constrained:</p> <pre><code>constraints = {\"y\": 1.5}  # Fix observation\ntrace, weight = model.generate(constraints, x=2.0)\n# weight = log p(y=1.5, z) / q(z | y=1.5)\n</code></pre>"},{"location":"api/overview/#updatetrace-constraints-args-new_trace-weight-discard","title":"<code>update(trace, constraints, args...) -&gt; (new_trace, weight, discard)</code>","text":"<p>Update an existing trace with new constraints:</p> <pre><code>new_constraints = {\"y\": 2.0}\nnew_trace, weight, discard = model.update(trace, new_constraints, x=2.0)\n</code></pre>"},{"location":"api/overview/#regeneratetrace-selection-args-new_trace-weight-discard","title":"<code>regenerate(trace, selection, args...) -&gt; (new_trace, weight, discard)</code>","text":"<p>Selectively regenerate parts of a trace:</p> <pre><code>from genjax import sel\n\nselection = sel(\"z\")  # Regenerate only z\nnew_trace, weight, discard = model.regenerate(trace, selection, x=2.0)\n</code></pre>"},{"location":"api/overview/#traces","title":"Traces","text":"<p>Traces record the execution of generative functions:</p> <pre><code>trace = model.simulate(x=2.0)\n\n# Access components\nchoices = trace.get_choices()      # Random choices\nretval = trace.get_retval()        # Return value\nscore = trace.get_score()          # log(1/p(choices))\nargs = trace.get_args()            # Function arguments\ngen_fn = trace.get_gen_fn()        # Source generative function\n</code></pre>"},{"location":"api/overview/#distributions","title":"Distributions","text":"<p>Built-in probability distributions that implement the GFI:</p> <pre><code>from genjax import normal, beta, categorical, bernoulli\n\n# Continuous distributions\nx = normal(mu=0, sigma=1) @ \"x\"\np = beta(alpha=2, beta=2) @ \"p\"\n\n# Discrete distributions  \nk = categorical(probs=jnp.array([0.2, 0.3, 0.5])) @ \"k\"\nb = bernoulli(p=0.7) @ \"b\"\n</code></pre>"},{"location":"api/overview/#combinators","title":"Combinators","text":"<p>Higher-order generative functions for composition:</p>"},{"location":"api/overview/#mapvmap","title":"Map/Vmap","text":"<p>Vectorized execution:</p> <pre><code># Map model over multiple inputs\nvectorized = model.vmap()\ntraces = vectorized.simulate(jnp.array([1.0, 2.0, 3.0]))\n</code></pre>"},{"location":"api/overview/#scan","title":"Scan","text":"<p>Sequential execution with state threading:</p> <pre><code>from genjax import Scan, const\n\n@gen\ndef step(state, x):\n    new_state = normal(state + x, 0.1) @ \"state\"\n    return new_state, new_state\n\nscan_model = Scan(step, const(10))  # 10 steps\ntrace = scan_model.simulate(init_state=0.0, xs=jnp.ones(10))\n</code></pre>"},{"location":"api/overview/#cond","title":"Cond","text":"<p>Conditional execution:</p> <pre><code>from genjax import Cond\n\n@gen\ndef model_a():\n    return normal(0, 1) @ \"x\"\n\n@gen  \ndef model_b():\n    return normal(5, 2) @ \"x\"\n\ncond_model = Cond(model_a, model_b)\ntrace = cond_model.simulate(condition=True)  # Uses model_a\n</code></pre>"},{"location":"api/overview/#selections","title":"Selections","text":"<p>Target specific addresses for operations:</p> <pre><code>from genjax import sel, Selection, AllSel\n\n# Select specific addresses\ns1 = sel(\"x\")                    # Select \"x\"\ns2 = sel(\"group_0\", \"mean\")      # Select \"group_0/mean\"\n\n# Combine selections\ns_or = sel(\"x\") | sel(\"y\")       # Select x OR y\ns_and = sel(\"x\") &amp; sel(\"y\")      # Select x AND y (intersection)\ns_not = ~sel(\"x\")                # Select everything except x\n\n# Select all\ns_all = Selection(AllSel())      # Select all addresses\n</code></pre>"},{"location":"api/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Generative Functions in detail</li> <li>Explore available Distributions</li> <li>Understand Traces and their structure</li> <li>Master Combinators for model composition</li> </ul>"},{"location":"getting-started/best-practices/","title":"Best Practices","text":"<p>This guide covers essential best practices for writing efficient and idiomatic GenJAX code.</p>"},{"location":"getting-started/best-practices/#jax-control-flow","title":"JAX Control Flow","text":"<p>GenJAX is built on JAX, which requires special handling of control flow for JIT compilation.</p>"},{"location":"getting-started/best-practices/#avoid-python-control-flow","title":"\u274c Avoid Python Control Flow","text":"<pre><code># BAD: Python for loop\n@gen\ndef bad_model(n):\n    values = []\n    for i in range(n):\n        x = distributions.normal(0, 1) @ f\"x_{i}\"\n        values.append(x)\n    return jnp.array(values)\n</code></pre>"},{"location":"getting-started/best-practices/#use-jax-control-flow","title":"\u2705 Use JAX Control Flow","text":"<pre><code># GOOD: Vectorized operations\n@gen\ndef good_model(n: Const[int]):\n    # Use vmap for vectorized sampling\n    values = distributions.normal(0, 1).vmap().apply(jnp.arange(n))\n    return values\n</code></pre>"},{"location":"getting-started/best-practices/#control-flow-patterns","title":"Control Flow Patterns","text":""},{"location":"getting-started/best-practices/#conditionals","title":"Conditionals","text":"<pre><code># BAD: Python if/else\nif condition:\n    result = model_a()\nelse:\n    result = model_b()\n\n# GOOD: jax.lax.cond\nresult = jax.lax.cond(\n    condition,\n    lambda: model_a(),\n    lambda: model_b()\n)\n</code></pre>"},{"location":"getting-started/best-practices/#loops-with-state","title":"Loops with State","text":"<pre><code># BAD: Python loop with accumulation\ntotal = 0\nfor i in range(n):\n    total += compute(i)\n\n# GOOD: jax.lax.scan\ndef step(carry, i):\n    return carry + compute(i), None\n\ntotal, _ = jax.lax.scan(step, 0, jnp.arange(n))\n</code></pre>"},{"location":"getting-started/best-practices/#static-vs-dynamic-values","title":"Static vs Dynamic Values","text":"<p>Use <code>Const[T]</code> to mark static values that should not become JAX tracers:</p> <pre><code>from genjax import Const\n\n@gen\ndef model(n_samples: Const[int], scale: float):\n    # n_samples is static - safe to use in Python control\n    # scale is dynamic - will be traced by JAX\n    return distributions.normal(0, scale).vmap().apply(jnp.arange(n_samples))\n</code></pre>"},{"location":"getting-started/best-practices/#random-number-generation","title":"Random Number Generation","text":"<p>Always use JAX's functional RNG pattern:</p> <pre><code># Split keys for multiple uses\nkey, subkey1, subkey2 = jax.random.split(key, 3)\n\n# Pass keys explicitly\ntrace1 = model.simulate(subkey1, args)\ntrace2 = model.simulate(subkey2, args)\n</code></pre>"},{"location":"getting-started/best-practices/#vectorization-best-practices","title":"Vectorization Best Practices","text":""},{"location":"getting-started/best-practices/#use-built-in-vectorization","title":"Use Built-in Vectorization","text":"<pre><code># Vectorize distributions\n@gen\ndef vectorized_model(data):\n    # Vectorized prior\n    mu = distributions.normal(0, 1) @ \"mu\"\n\n    # Vectorized likelihood\n    obs = distributions.normal(mu, 0.1).vmap().apply(jnp.arange(len(data)))\n    return obs\n</code></pre>"},{"location":"getting-started/best-practices/#batch-operations","title":"Batch Operations","text":"<pre><code># Process multiple traces efficiently\nkeys = jax.random.split(key, n_chains)\ntraces = jax.vmap(lambda k: model.simulate(k, args))(keys)\n</code></pre>"},{"location":"getting-started/best-practices/#memory-efficiency","title":"Memory Efficiency","text":""},{"location":"getting-started/best-practices/#avoid-materializing-large-intermediate-arrays","title":"Avoid Materializing Large Intermediate Arrays","text":"<pre><code># BAD: Creates large intermediate array\n@gen\ndef inefficient(n):\n    all_samples = distributions.normal(0, 1).vmap().apply(jnp.arange(n))\n    return jnp.mean(all_samples)\n\n# GOOD: Use scan for memory efficiency\n@gen\ndef efficient(n: Const[int]):\n    def step(carry, i):\n        sample = distributions.normal(0, 1) @ f\"sample_{i}\"\n        return carry + sample, None\n\n    total, _ = jax.lax.scan(step, 0.0, jnp.arange(n))\n    return total / n\n</code></pre>"},{"location":"getting-started/best-practices/#type-annotations","title":"Type Annotations","text":"<p>Use type hints for better code clarity and IDE support:</p> <pre><code>from typing import Tuple\nimport jax.numpy as jnp\nfrom genjax import Trace\n\n@gen\ndef typed_model(x: jnp.ndarray) -&gt; jnp.ndarray:\n    mu = distributions.normal(0.0, 1.0) @ \"mu\"\n    y = distributions.normal(mu * x, 0.1) @ \"y\"\n    return y\n\ndef inference_step(trace: Trace, key: jax.random.PRNGKey) -&gt; Tuple[Trace, float]:\n    new_trace = metropolis_hastings(trace, select(\"mu\"), key)\n    return new_trace, new_trace[\"mu\"]\n</code></pre>"},{"location":"getting-started/best-practices/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"getting-started/best-practices/#1-forgetting-vmap-address-prefixes","title":"1. Forgetting vmap Address Prefixes","text":"<pre><code># When using vmap, addresses are prefixed\nconstraints = {\n    \"vmap/x_0\": 1.0,  # Correct\n    \"x_0\": 1.0,       # Wrong - won't match vmapped choice\n}\n</code></pre>"},{"location":"getting-started/best-practices/#2-using-python-randomness","title":"2. Using Python Randomness","text":"<pre><code># BAD: Python's random module\nimport random\nx = random.normal()\n\n# GOOD: JAX random\nkey, subkey = jax.random.split(key)\nx = jax.random.normal(subkey)\n</code></pre>"},{"location":"getting-started/best-practices/#3-modifying-arrays-in-place","title":"3. Modifying Arrays In-Place","text":"<pre><code># BAD: In-place modification\narr[0] = 1.0\n\n# GOOD: Functional update\narr = arr.at[0].set(1.0)\n</code></pre>"},{"location":"getting-started/best-practices/#performance-tips","title":"Performance Tips","text":"<ol> <li>JIT Compile Inference Loops: Wrap your inference code in <code>jax.jit</code></li> <li>Batch Operations: Use <code>vmap</code> instead of loops when possible</li> <li>Reuse Compiled Functions: JIT compilation has overhead, reuse compiled functions</li> <li>Profile Your Code: Use JAX's profiling tools to identify bottlenecks</li> </ol>"},{"location":"getting-started/best-practices/#testing-best-practices","title":"Testing Best Practices","text":"<pre><code>def test_model_deterministic():\n    \\\"\\\"\\\"Test with fixed random seed for reproducibility\\\"\\\"\\\"\n    key = jax.random.PRNGKey(42)\n    trace = model.simulate(key, args)\n\n    # Test should be deterministic\n    expected_value = 1.234\n    assert jnp.allclose(trace.retval, expected_value)\n\ndef test_gradients():\n    \\\"\\\"\\\"Ensure gradients flow correctly\\\"\\\"\\\"\n    def loss(params):\n        trace = model.generate(key, constraints, params)\n        return -trace.score  # Negative log probability\n\n    grad_fn = jax.grad(loss)\n    grads = grad_fn(params)\n\n    # Check gradients are finite\n    assert jnp.all(jnp.isfinite(grads))\n</code></pre>"},{"location":"getting-started/best-practices/#next-steps","title":"Next Steps","text":"<ul> <li>Review the API Reference for detailed documentation</li> <li>Explore Advanced Topics for deeper JAX integration</li> <li>Check out Examples for real-world patterns</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>GenJAX can be installed using pip or conda package managers.</p>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install genjax\n</code></pre>"},{"location":"getting-started/installation/#using-condamamba","title":"Using conda/mamba","text":"<pre><code>conda install -c conda-forge genjax\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development or to get the latest features:</p> <pre><code>git clone https://github.com/femtomc/genjax.git\ncd genjax\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>GenJAX requires: - Python &gt;= 3.12 - JAX &gt;= 0.6.0 - TensorFlow Probability (JAX substrate) - Beartype for runtime type checking - Penzai for visualization</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with GenJAX in minutes! This guide covers the essential concepts through practical examples.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<pre><code>pip install genjax\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-model","title":"Your First Model","text":"<p>Let's start with a simple Bayesian coin flipping model that follows JAX best practices:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom genjax import gen, distributions, Const\n\n@gen\ndef coin_model(n_flips: Const[int]):\n    # Prior belief about coin fairness\n    bias = distributions.beta(2.0, 2.0) @ \"bias\"\n\n    # Generate predictions using JAX-friendly vectorized operations\n    # This uses GenJAX's built-in vectorization support\n    flips = distributions.bernoulli(bias).vmap().apply(jnp.arange(n_flips))\n\n    return flips\n</code></pre> <p>JAX Best Practice</p> <p>Notice we use <code>Const[int]</code> for <code>n_flips</code> to indicate it's a static value. This allows JAX to compile efficiently without creating tracers for loop bounds.</p>"},{"location":"getting-started/quickstart/#running-inference","title":"Running Inference","text":""},{"location":"getting-started/quickstart/#forward-sampling","title":"Forward Sampling","text":"<p>Sample from the prior:</p> <pre><code>key = jax.random.PRNGKey(0)\n\n# Simulate without any observations\ntrace = coin_model.simulate(key, (4,))  # 4 flips\n\n# Extract the sampled bias\nbias_sample = trace[\"bias\"]\nprint(f\"Sampled bias: {bias_sample:.3f}\")\n\n# Get the predictions\npredictions = trace.retval\nprint(f\"Predicted flips: {predictions}\")\n</code></pre>"},{"location":"getting-started/quickstart/#conditioning-on-data","title":"Conditioning on Data","text":"<p>Observe some coin flips and infer the bias:</p> <pre><code># Observed data: 1 = Heads, 0 = Tails\nobserved_flips = jnp.array([1, 1, 0, 1])\n\n# Create constraints using JAX-friendly dictionary comprehension\nconstraints = {f\"vmap/flip_{i}\": observed_flips[i] for i in range(4)}\n\n# Generate a trace with these constraints\ntrace = coin_model.generate(key, constraints, (4,))\n\n# Extract posterior sample\nposterior_bias = trace[\"bias\"]\nprint(f\"Posterior bias sample: {posterior_bias:.3f}\")\n</code></pre> <p>Address Format</p> <p>When using <code>vmap</code>, addresses are prefixed with <code>vmap/</code>. This is important for correctly targeting vectorized random choices.</p>"},{"location":"getting-started/quickstart/#using-mcmc","title":"Using MCMC","text":"<p>For more complex models, use Markov Chain Monte Carlo with JAX-friendly patterns:</p> <pre><code>from genjax.inference.mcmc import metropolis_hastings\nfrom genjax import select\n\n# Initialize with constrained trace\nkey, subkey = jax.random.split(key)\ntrace = coin_model.generate(subkey, constraints, (4,))\n\n# Define MCMC kernel\nselection = select(\"bias\")  # Only update the bias\n\n# Run chain using JAX scan for efficiency\ndef mcmc_step(carry, key):\n    trace = carry\n    new_trace = metropolis_hastings(trace, selection, key)\n    return new_trace, new_trace[\"bias\"]  # Return trace and save bias\n\n# Generate keys for each MCMC step\nkeys = jax.random.split(key, 1000)\n\n# Run MCMC chain\nfinal_trace, bias_samples = jax.lax.scan(mcmc_step, trace, keys)\n\n# Analyze posterior (thin by taking every 100th sample)\nthinned_samples = bias_samples[::100]\nposterior_mean = jnp.mean(thinned_samples)\nprint(f\"Posterior mean bias: {posterior_mean:.3f}\")\n</code></pre> <p>JAX Best Practice</p> <p>Using <code>jax.lax.scan</code> instead of Python loops allows: - JIT compilation of the entire MCMC chain - Efficient memory usage - GPU/TPU acceleration</p>"},{"location":"getting-started/quickstart/#a-more-complex-example-linear-regression","title":"A More Complex Example: Linear Regression","text":"<pre><code>@gen\ndef linear_regression(x: jnp.ndarray):\n    # Priors\n    slope = distributions.normal(0.0, 10.0) @ \"slope\"\n    intercept = distributions.normal(0.0, 10.0) @ \"intercept\"\n    noise = distributions.gamma(1.0, 1.0) @ \"noise\"\n\n    # Vectorized likelihood - no Python loops!\n    mu = intercept + slope * x\n    y = distributions.normal(mu, noise).vmap().apply(jnp.arange(len(x)))\n\n    return y\n\n# Generate synthetic data\ntrue_slope = 2.0\ntrue_intercept = 1.0\nx_data = jnp.linspace(-2, 2, 20)\n\nkey, noise_key = jax.random.split(key)\ny_data = true_intercept + true_slope * x_data + jax.random.normal(noise_key, shape=(20,)) * 0.5\n\n# Create constraints for observed y values\nconstraints = {f\"vmap/y_{i}\": y_data[i] for i in range(len(y_data))}\n\n# Use SMC for inference\nfrom genjax.inference.smc import importance_sampling\n\n# Run importance sampling with multiple particles\nkeys = jax.random.split(key, 100)\ntraces = jax.vmap(lambda k: linear_regression.generate(k, constraints, (x_data,)))(keys)\n\n# Extract and analyze posterior samples\nslopes = traces[\"slope\"]  # Shape: (100,)\nintercepts = traces[\"intercept\"]  # Shape: (100,)\n\nprint(f\"Posterior mean slope: {jnp.mean(slopes):.3f} (true: {true_slope})\")\nprint(f\"Posterior mean intercept: {jnp.mean(intercepts):.3f} (true: {true_intercept})\")\n</code></pre> <p>Vectorized Operations</p> <p>GenJAX's <code>vmap()</code> method on distributions allows us to vectorize random choices across array dimensions, avoiding Python loops entirely.</p>"},{"location":"getting-started/quickstart/#jax-genjax-best-practices","title":"JAX &amp; GenJAX Best Practices","text":""},{"location":"getting-started/quickstart/#do","title":"\u2705 DO:","text":"<ul> <li>Use <code>jax.lax.scan</code> for loops with accumulation</li> <li>Use <code>jax.lax.fori_loop</code> for simple iterations</li> <li>Use <code>jax.lax.cond</code> for conditionals</li> <li>Use <code>Const[T]</code> for static values in generative functions</li> <li>Use <code>vmap()</code> for vectorizing operations</li> <li>Use JAX's functional random number generation</li> </ul>"},{"location":"getting-started/quickstart/#dont","title":"\u274c DON'T:","text":"<ul> <li>Use Python <code>for</code> loops in JIT-compiled code</li> <li>Use Python <code>if/else</code> statements with traced values</li> <li>Build Python lists and convert to arrays</li> <li>Use mutable state or side effects</li> </ul>"},{"location":"getting-started/quickstart/#key-concepts-summary","title":"Key Concepts Summary","text":"<ol> <li><code>@gen</code> decorator: Transforms functions into generative functions</li> <li><code>@</code> operator: Assigns addresses to random choices</li> <li>Traces: Immutable records of model execution</li> <li>Constraints: Fix random choices for conditioning</li> <li>Vectorization: Use <code>vmap()</code> for efficient batched operations</li> <li>Static values: Use <code>Const[T]</code> for compile-time constants</li> </ol>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Tutorial for in-depth examples</li> <li>Learn about Inference Algorithms</li> <li>Check out Advanced Examples</li> <li>Read the API Reference</li> </ul>"},{"location":"getting-started/quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: github.com/femtomc/genjax/issues</li> <li>Documentation: femtomc.github.io/genjax</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>Complete API documentation for GenJAX, automatically generated from source code.</p>"},{"location":"reference/#core-modules","title":"Core Modules","text":""},{"location":"reference/#genjaxcore","title":"genjax.core","text":"<p>Core functionality including the Generative Function Interface (GFI), traces, and the <code>@gen</code> decorator.</p>"},{"location":"reference/#genjaxdistributions","title":"genjax.distributions","text":"<p>Built-in probability distributions that implement the GFI.</p>"},{"location":"reference/#genjaxpjax","title":"genjax.pjax","text":"<p>Probabilistic JAX (PJAX) - foundational probabilistic programming primitives.</p>"},{"location":"reference/#inference","title":"Inference","text":""},{"location":"reference/#genjaxinferencemcmc","title":"genjax.inference.mcmc","text":"<p>Markov Chain Monte Carlo algorithms including Metropolis-Hastings and HMC.</p>"},{"location":"reference/#genjaxinferencesmc","title":"genjax.inference.smc","text":"<p>Sequential Monte Carlo methods for particle-based inference.</p>"},{"location":"reference/#genjaxinferencevi","title":"genjax.inference.vi","text":"<p>Variational inference algorithms and gradient estimators.</p>"},{"location":"reference/#advanced","title":"Advanced","text":""},{"location":"reference/#genjaxadev","title":"genjax.adev","text":"<p>Automatic differentiation of expected values for gradient estimation.</p>"},{"location":"reference/#genjaxstate","title":"genjax.state","text":"<p>State interpreter for inspecting and manipulating probabilistic computations.</p>"},{"location":"reference/#utilities","title":"Utilities","text":""},{"location":"reference/#genjaxsp","title":"genjax.sp","text":"<p>Structural primitives and combinators for building complex models.</p>"},{"location":"reference/#genjaxtiming","title":"genjax.timing","text":"<p>Utilities for benchmarking and performance analysis.</p>"},{"location":"reference/adev/","title":"genjax.adev","text":"<p>Automatic differentiation of expected values for gradient estimation.</p>"},{"location":"reference/adev/#genjax.adev","title":"adev","text":"<p>ADEV: Sound Automatic Differentiation of Expected Values</p> <p>This module implements ADEV (Automatic Differentiation of Expectation Values), a system for computing sound, unbiased gradient estimators of expectations involving stochastic functions. Based on the research presented in \"ADEV: Sound Automatic Differentiation of Expected Values of Probabilistic Programs\" (Lew et al., POPL 2023, arXiv:2212.06386).</p> <p>ADEV is a source-to-source transformation that extends forward-mode automatic differentiation to correctly handle probabilistic computations. The key insight is transforming a probabilistic program into a new program whose expected return value is the derivative of the original program's expectation.</p> <p>Theoetical Foundation: ADEV uses a continuation-passing style (CPS) transformation that reflects the law of iterated expectation: E[f(X)] = E[E[f(X) | Z]] where X depends on Z. This enables modular composition of different gradient estimation strategies while maintaining soundness guarantees proven via logical relations.</p> Key Concepts <ul> <li>ADEVPrimitive: Stochastic primitives with custom gradient estimation strategies</li> <li>Dual Numbers: Pairs (primal, tangent) for forward-mode automatic differentiation</li> <li>Continuations: Higher-order functions representing \"the rest of the computation\"<ul> <li>Pure continuation: Operates on primal values only (no differentiation)</li> <li>Dual continuation: Operates on dual numbers (differentiation applied)</li> </ul> </li> <li>CPS Transformation: Allows modular selection of gradient strategies per distribution</li> </ul> Gradient Estimation Strategies <ul> <li>REINFORCE: Score function estimator \u2207E[f(X)] = E[f(X) * \u2207log p(X)]</li> <li>Reparameterization: Pathwise estimator \u2207E[f(g(\u03b5))] = E[\u2207f(g(\u03b5)) * \u2207g(\u03b5)]</li> <li>Enumeration: Exact computation for finite discrete distributions</li> <li>Measure-Valued Derivatives: Advanced discrete gradient estimators</li> </ul> Example <pre><code>from genjax.adev import expectation, normal_reparam\n\n@expectation\ndef objective(theta):\n    x = normal_reparam(theta, 1.0)  # Reparameterizable distribution\n    return x**2\n\ngrad = objective.grad_estimate(0.5)  # Unbiased gradient estimate\n</code></pre> References <p>Lew, A. K., Huot, M., Staton, S., &amp; Mansinghka, V. K. (2023). ADEV: Sound Automatic Differentiation of Expected Values of Probabilistic Programs. Proceedings of the ACM on Programming Languages, 7(POPL), 121-148.</p>"},{"location":"reference/adev/#genjax.adev.DualTree","title":"DualTree  <code>module-attribute</code>","text":"<pre><code>DualTree = Annotated[Any, Is[lambda v: static_check_dual_tree(v)]]\n</code></pre> <p><code>DualTree</code> is the type of <code>Pytree</code> argument values with <code>Dual</code> leaves.</p>"},{"location":"reference/adev/#genjax.adev.ADEVPrimitive","title":"ADEVPrimitive","text":"<p>               Bases: <code>Pytree</code></p> <p>Base class for stochastic primitives with custom gradient estimation strategies.</p> <p>An ADEVPrimitive represents a stochastic operation (like sampling from a distribution) that can provide custom gradient estimates through the ADEV system. Each primitive implements both forward sampling and a strategy for computing Jacobian-Vector Product (JVP) estimates during automatic differentiation.</p> <p>The key insight is that different stochastic operations benefit from different gradient estimation strategies (REINFORCE, reparameterization, enumeration, etc.), and ADEVPrimitive allows each operation to specify its optimal strategy.</p>"},{"location":"reference/adev/#genjax.adev.ADEVPrimitive.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(*args) -&gt; Any\n</code></pre> <p>Forward sampling operation.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@abstractmethod\ndef sample(self, *args) -&gt; Any:\n    \"\"\"Forward sampling operation.\n\n    Args:\n        *args: Parameters for the stochastic operation\n\n    Returns:\n        Sample from the distribution/stochastic process\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/adev/#genjax.adev.ADEVPrimitive.prim_jvp_estimate","title":"prim_jvp_estimate  <code>abstractmethod</code>","text":"<pre><code>prim_jvp_estimate(dual_tree: tuple[DualTree, ...], konts: tuple[Callable[..., Any], Callable[..., Any]]) -&gt; Dual\n</code></pre> <p>Custom JVP gradient estimation strategy.</p> <p>This method implements the core gradient estimation logic for this primitive. It receives dual numbers (primal + tangent values) for the arguments and two continuations representing the rest of the computation.</p> Note <p>The choice of continuation reflects ADEV's CPS transformation approach: - Pure continuation: Evaluates the remaining computation on primal values - Dual continuation: Applies ADEV transformation to remaining computation</p> <p>Different gradient strategies utilize these continuations differently: - REINFORCE: Uses dual continuation to evaluate f(X), computes \u2207log p(X) - Reparameterization: Uses dual continuation with reparameterized samples - Enumeration: May use both to compute weighted exact expectations</p> <p>This design enables modular composition as described in the ADEV paper.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@abstractmethod\ndef prim_jvp_estimate(\n    self,\n    dual_tree: tuple[DualTree, ...],\n    konts: tuple[\n        Callable[..., Any],  # Pure continuation (kpure)\n        Callable[..., Any],  # Dual continuation (kdual)\n    ],\n) -&gt; \"Dual\":\n    \"\"\"Custom JVP gradient estimation strategy.\n\n    This method implements the core gradient estimation logic for this primitive.\n    It receives dual numbers (primal + tangent values) for the arguments and\n    two continuations representing the rest of the computation.\n\n    Args:\n        dual_tree: Arguments as dual numbers (primal, tangent) pairs\n        konts: Pair of continuations:\n            - konts[0] (kpure): Pure continuation - no ADEV transformation\n            - konts[1] (kdual): Dual continuation - ADEV transformation applied\n\n    Returns:\n        Dual number representing the gradient estimate for this operation\n\n    Note:\n        The choice of continuation reflects ADEV's CPS transformation approach:\n        - Pure continuation: Evaluates the remaining computation on primal values\n        - Dual continuation: Applies ADEV transformation to remaining computation\n\n        Different gradient strategies utilize these continuations differently:\n        - REINFORCE: Uses dual continuation to evaluate f(X), computes \u2207log p(X)\n        - Reparameterization: Uses dual continuation with reparameterized samples\n        - Enumeration: May use both to compute weighted exact expectations\n\n        This design enables modular composition as described in the ADEV paper.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Dual","title":"Dual","text":"<p>               Bases: <code>Pytree</code></p> <p>Dual number for forward-mode automatic differentiation.</p> <p>A Dual number represents both a value (primal) and its derivative (tangent) with respect to some input. This is the fundamental data structure for forward-mode AD in the ADEV system.</p> Example <p>x = Dual(3.0, 1.0)  # x = 3, dx/dx = 1 y = Dual(x.primal2, 2x.primalx.tangent)  # y = x^2, dy/dx = 2x</p>"},{"location":"reference/adev/#genjax.adev.Dual.tree_pure","title":"tree_pure  <code>staticmethod</code>","text":"<pre><code>tree_pure(v)\n</code></pre> <p>Convert a tree to have Dual leaves with zero tangents.</p> <p>This is used to \"lift\" regular values into the dual number system by pairing them with zero tangents, indicating no sensitivity.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@staticmethod\ndef tree_pure(v):\n    \"\"\"Convert a tree to have Dual leaves with zero tangents.\n\n    This is used to \"lift\" regular values into the dual number system\n    by pairing them with zero tangents, indicating no sensitivity.\n\n    Args:\n        v: Pytree that may contain mix of Dual and regular values\n\n    Returns:\n        Pytree where all leaves are Dual numbers\n    \"\"\"\n\n    def _inner(v):\n        if isinstance(v, Dual):\n            return v\n        else:\n            return Dual(v, jnp.zeros_like(v))\n\n    return jtu.tree_map(_inner, v, is_leaf=lambda v: isinstance(v, Dual))\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Dual.dual_tree","title":"dual_tree  <code>staticmethod</code>","text":"<pre><code>dual_tree(primals, tangents)\n</code></pre> <p>Combine primal and tangent trees into a tree of Dual numbers.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@staticmethod\ndef dual_tree(primals, tangents):\n    \"\"\"Combine primal and tangent trees into a tree of Dual numbers.\n\n    Args:\n        primals: Tree of primal values\n        tangents: Tree of tangent values (same structure as primals)\n\n    Returns:\n        Tree of Dual numbers combining corresponding primals and tangents\n    \"\"\"\n    return jtu.tree_map(lambda v1, v2: Dual(v1, v2), primals, tangents)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Dual.tree_primal","title":"tree_primal  <code>staticmethod</code>","text":"<pre><code>tree_primal(v)\n</code></pre> <p>Extract primal values from a tree of Dual numbers.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@staticmethod\ndef tree_primal(v):\n    \"\"\"Extract primal values from a tree of Dual numbers.\n\n    Args:\n        v: Tree that may contain Dual numbers\n\n    Returns:\n        Tree with Dual numbers replaced by their primal values\n    \"\"\"\n\n    def _inner(v):\n        if isinstance(v, Dual):\n            return v.primal\n        else:\n            return v\n\n    return jtu.tree_map(_inner, v, is_leaf=lambda v: isinstance(v, Dual))\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Dual.tree_tangent","title":"tree_tangent  <code>staticmethod</code>","text":"<pre><code>tree_tangent(v)\n</code></pre> <p>Extract tangent values from a tree of Dual numbers.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@staticmethod\ndef tree_tangent(v):\n    \"\"\"Extract tangent values from a tree of Dual numbers.\n\n    Args:\n        v: Tree that may contain Dual numbers\n\n    Returns:\n        Tree with Dual numbers replaced by their tangent values\n    \"\"\"\n\n    def _inner(v):\n        if isinstance(v, Dual):\n            return v.tangent\n        else:\n            return v\n\n    return jtu.tree_map(_inner, v, is_leaf=lambda v: isinstance(v, Dual))\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Dual.tree_leaves","title":"tree_leaves  <code>staticmethod</code>","text":"<pre><code>tree_leaves(v)\n</code></pre> <p>Get leaves of a tree, treating Dual numbers as atomic.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@staticmethod\ndef tree_leaves(v):\n    \"\"\"Get leaves of a tree, treating Dual numbers as atomic.\n\n    Args:\n        v: Tree structure\n\n    Returns:\n        List of Dual leaves\n    \"\"\"\n    v = Dual.tree_pure(v)\n    return jtu.tree_leaves(v, is_leaf=lambda v: isinstance(v, Dual))\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Dual.tree_unzip","title":"tree_unzip  <code>staticmethod</code>","text":"<pre><code>tree_unzip(v)\n</code></pre> <p>Separate a tree of Dual numbers into primal and tangent trees.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@staticmethod\ndef tree_unzip(v):\n    \"\"\"Separate a tree of Dual numbers into primal and tangent trees.\n\n    Args:\n        v: Tree containing Dual numbers\n\n    Returns:\n        Tuple of (primal_leaves, tangent_leaves) as flat lists\n    \"\"\"\n    primals = jtu.tree_leaves(Dual.tree_primal(v))\n    tangents = jtu.tree_leaves(Dual.tree_tangent(v))\n    return tuple(primals), tuple(tangents)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Dual.static_check_is_dual","title":"static_check_is_dual  <code>staticmethod</code>","text":"<pre><code>static_check_is_dual(v) -&gt; bool\n</code></pre> <p>Check if a value is a Dual number.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@staticmethod\ndef static_check_is_dual(v) -&gt; bool:\n    \"\"\"Check if a value is a Dual number.\"\"\"\n    return isinstance(v, Dual)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Dual.static_check_dual_tree","title":"static_check_dual_tree  <code>staticmethod</code>","text":"<pre><code>static_check_dual_tree(v) -&gt; bool\n</code></pre> <p>Check if all leaves in a tree are Dual numbers.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@staticmethod\ndef static_check_dual_tree(v) -&gt; bool:\n    \"\"\"Check if all leaves in a tree are Dual numbers.\"\"\"\n    return all(\n        map(\n            lambda v: isinstance(v, Dual),\n            jtu.tree_leaves(v, is_leaf=Dual.static_check_is_dual),\n        )\n    )\n</code></pre>"},{"location":"reference/adev/#genjax.adev.ADEV","title":"ADEV","text":"<p>               Bases: <code>Pytree</code></p> <p>Interpreter for ADEV's continuation-passing style automatic differentiation.</p> <p>The ADEV interpreter processes JAX computation graphs (Jaxpr) and transforms them to support stochastic automatic differentiation. It implements a continuation-passing style (CPS) transformation that reflects the law of iterated expectation, allowing different gradient estimation strategies for each stochastic operation.</p> <p>Key responsibilities: 1. Propagate dual numbers through deterministic JAX operations 2. Apply CPS transformation at stochastic operations (sample_p primitives) 3. Create continuation closures for gradient estimation strategies 4. Handle control flow (conditionals, loops) within the AD system</p> <p>The CPS transformation is crucial: when encountering a stochastic operation, the interpreter creates two continuations representing the rest of the computation: - Pure continuation: For sampling-based gradient estimates - Dual continuation: For the ADEV-transformed remainder</p> <p>This allows each ADEVPrimitive to choose its optimal gradient strategy while maintaining composability across the entire computation graph.</p>"},{"location":"reference/adev/#genjax.adev.ADEVProgram","title":"ADEVProgram","text":"<p>               Bases: <code>Pytree</code></p> <p>Internal representation of a stochastic program for ADEV gradient estimation.</p> <p>An ADEVProgram wraps a source function containing stochastic operations and provides the infrastructure for computing Jacobian-Vector Product (JVP) estimates through the ADEV system. This class serves as an intermediate representation between user-defined @expectation functions and the low-level ADEV interpreter.</p> <p>The ADEVProgram handles the integration between: 1. User source code containing ADEV primitives 2. The ADEV interpreter's CPS transformation 3. Continuation-based gradient estimation strategies</p> Note <p>This class is typically not used directly by users. It's created internally by the @expectation decorator and managed by the Expectation class.</p>"},{"location":"reference/adev/#genjax.adev.ADEVProgram.jvp_estimate","title":"jvp_estimate","text":"<pre><code>jvp_estimate(duals: tuple[DualTree, ...], dual_kont: Callable[..., Any]) -&gt; Dual\n</code></pre> <p>Compute JVP estimate for the stochastic program.</p> <p>This method applies the ADEV forward-mode transformation to compute an unbiased estimate of the Jacobian-Vector Product for expectations involving stochastic operations. It uses the continuation-passing style transformation to integrate different gradient estimation strategies.</p> Note <p>This method coordinates between the user's source function and the ADEV interpreter to apply the appropriate gradient estimation strategies for each stochastic primitive encountered during execution.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def jvp_estimate(\n    self,\n    duals: tuple[DualTree, ...],  # Pytree with Dual leaves.\n    dual_kont: Callable[..., Any],\n) -&gt; Dual:\n    \"\"\"Compute JVP estimate for the stochastic program.\n\n    This method applies the ADEV forward-mode transformation to compute\n    an unbiased estimate of the Jacobian-Vector Product for expectations\n    involving stochastic operations. It uses the continuation-passing style\n    transformation to integrate different gradient estimation strategies.\n\n    Args:\n        duals: Input arguments as dual numbers (primal, tangent) pairs\n        dual_kont: Continuation representing the computation after this program\n\n    Returns:\n        Dual number containing the JVP estimate (primal value + gradient estimate)\n\n    Note:\n        This method coordinates between the user's source function and the\n        ADEV interpreter to apply the appropriate gradient estimation strategies\n        for each stochastic primitive encountered during execution.\n    \"\"\"\n\n    def adev_jvp(f):\n        @wraps(f)\n        def wrapped(*duals: DualTree):\n            return ADEV.forward_mode(self.source.value, dual_kont)(*duals)\n\n        return wrapped\n\n    return adev_jvp(self.source.value)(*duals)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Expectation","title":"Expectation","text":"<p>               Bases: <code>Pytree</code></p> <p>Represents an expectation with automatic differentiation support.</p> <p>An Expectation object encapsulates a stochastic computation and provides methods to compute unbiased gradient estimates of expectation values. This is the primary user-facing interface for ADEV (Automatic Differentiation of Expectation Values).</p> <p>The key insight is that for expectations E[f(X)] where X is a random variable, we can compute unbiased gradient estimates \u2207E[f(X)] using various strategies: - REINFORCE: \u2207E[f(X)] = E[f(X) * \u2207log p(X)] - Reparameterization: \u2207E[f(X)] = E[\u2207f(g(\u03b5))] where X = g(\u03b5), \u03b5 ~ fixed distribution - Enumeration: Exact computation for discrete distributions with finite support</p> Example <p>from genjax.adev import expectation, normal_reparam import jax.numpy as jnp</p> <p>@expectation ... def loss_function(theta): ...     x = normal_reparam(theta, 1.0) ...     return x**2</p>"},{"location":"reference/adev/#genjax.adev.Expectation--compute-gradient-estimate","title":"Compute gradient estimate","text":"<p>grad = loss_function.grad_estimate(0.5) jnp.isfinite(grad)  # doctest: +ELLIPSIS Array(True, dtype=bool...)</p>"},{"location":"reference/adev/#genjax.adev.Expectation--compute-expectation-value","title":"Compute expectation value","text":"<p>value = loss_function.estimate(0.5) jnp.isfinite(value)  # doctest: +ELLIPSIS Array(True, dtype=bool...)</p>"},{"location":"reference/adev/#genjax.adev.Expectation.jvp_estimate","title":"jvp_estimate","text":"<pre><code>jvp_estimate(*duals: DualTree)\n</code></pre> <p>Compute Jacobian-Vector Product estimate for the expectation.</p> <p>This method provides the core JVP computation for ADEV. It applies the continuation-passing style transformation with an identity continuation, meaning this expectation represents the \"final\" computation in the chain.</p> Note <p>This is the foundational method that enables both grad_estimate and integration with JAX's automatic differentiation system.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def jvp_estimate(self, *duals: DualTree):\n    \"\"\"Compute Jacobian-Vector Product estimate for the expectation.\n\n    This method provides the core JVP computation for ADEV. It applies the\n    continuation-passing style transformation with an identity continuation,\n    meaning this expectation represents the \"final\" computation in the chain.\n\n    Args:\n        *duals: Input arguments as dual numbers (primal, tangent) pairs\n\n    Returns:\n        Dual number with primal value E[f(X)] and tangent containing \u2207E[f(X)]\n\n    Note:\n        This is the foundational method that enables both grad_estimate and\n        integration with JAX's automatic differentiation system.\n    \"\"\"\n\n    # Identity continuation - this expectation is the final computation\n    def _identity(v):\n        return v\n\n    return self.prog.jvp_estimate(duals, _identity)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Expectation.grad_estimate","title":"grad_estimate","text":"<pre><code>grad_estimate(*primals)\n</code></pre> <p>Compute unbiased gradient estimate of the expectation.</p> <p>This method provides the primary interface for computing gradients of expectation values. It leverages JAX's grad transformation combined with ADEV's custom JVP rules to produce unbiased gradient estimates.</p> Example <p>from genjax.adev import expectation, normal_reparam import jax.numpy as jnp</p> <p>@expectation ... def objective(mu, sigma): ...     x = normal_reparam(mu, sigma) ...     return x**2</p> Note <p>The gradient estimates are unbiased, meaning E[\u2207\u0302f] = \u2207E[f], but they may have variance. The choice of gradient estimation strategy (REINFORCE, reparameterization, etc.) affects this variance.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def grad_estimate(self, *primals):\n    \"\"\"Compute unbiased gradient estimate of the expectation.\n\n    This method provides the primary interface for computing gradients of\n    expectation values. It leverages JAX's grad transformation combined with\n    ADEV's custom JVP rules to produce unbiased gradient estimates.\n\n    Args:\n        *primals: Input values to compute gradients with respect to\n\n    Returns:\n        If single argument: Single gradient estimate array\n        If multiple arguments: Tuple of gradient estimates\n\n    Example:\n        &gt;&gt;&gt; from genjax.adev import expectation, normal_reparam\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @expectation\n        ... def objective(mu, sigma):\n        ...     x = normal_reparam(mu, sigma)\n        ...     return x**2\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute gradient with respect to both parameters\n        &gt;&gt;&gt; grad_mu, grad_sigma = objective.grad_estimate(1.0, 0.5)\n        &gt;&gt;&gt; jnp.isfinite(grad_mu)  # doctest: +ELLIPSIS\n        Array(True, dtype=bool...)\n        &gt;&gt;&gt; jnp.isfinite(grad_sigma)  # doctest: +ELLIPSIS\n        Array(True, dtype=bool...)\n\n    Note:\n        The gradient estimates are unbiased, meaning E[\u2207\u0302f] = \u2207E[f], but they\n        may have variance. The choice of gradient estimation strategy (REINFORCE,\n        reparameterization, etc.) affects this variance.\n    \"\"\"\n\n    def _invoke_closed_over(primals):\n        return invoke_closed_over(self, primals)\n\n    grad_result = jax.grad(_invoke_closed_over)(primals)\n\n    # Return single gradient for single argument, tuple for multiple arguments\n    if len(primals) == 1:\n        return grad_result[0]\n    else:\n        return grad_result\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Expectation.grad_estimate--compute-gradient-with-respect-to-both-parameters","title":"Compute gradient with respect to both parameters","text":"<p>grad_mu, grad_sigma = objective.grad_estimate(1.0, 0.5) jnp.isfinite(grad_mu)  # doctest: +ELLIPSIS Array(True, dtype=bool...) jnp.isfinite(grad_sigma)  # doctest: +ELLIPSIS Array(True, dtype=bool...)</p>"},{"location":"reference/adev/#genjax.adev.Expectation.estimate","title":"estimate","text":"<pre><code>estimate(*args)\n</code></pre> <p>Compute the expectation value (forward pass only).</p> <p>This method evaluates E[f(X)] without computing gradients. It's useful when you only need the expectation value itself, not its derivatives.</p> Example <p>from genjax.adev import expectation, normal_reparam import jax.numpy as jnp</p> <p>@expectation ... def mean_squared(mu): ...     x = normal_reparam(mu, 1.0) ...     return x**2</p> Note <p>This method uses zero tangents in the dual number computation, effectively performing only the forward pass through the stochastic computation graph.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def estimate(self, *args):\n    \"\"\"Compute the expectation value (forward pass only).\n\n    This method evaluates E[f(X)] without computing gradients. It's useful\n    when you only need the expectation value itself, not its derivatives.\n\n    Args:\n        *args: Arguments to the expectation function\n\n    Returns:\n        The expectation value E[f(X)] as computed by the stochastic program\n\n    Example:\n        &gt;&gt;&gt; from genjax.adev import expectation, normal_reparam\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @expectation\n        ... def mean_squared(mu):\n        ...     x = normal_reparam(mu, 1.0)\n        ...     return x**2\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Just compute E[X^2] where X ~ Normal(mu, 1)\n        &gt;&gt;&gt; expectation_value = mean_squared.estimate(2.0)\n        &gt;&gt;&gt; jnp.isfinite(expectation_value)  # doctest: +ELLIPSIS\n        Array(True, dtype=bool...)\n        &gt;&gt;&gt; expectation_value &gt; 0  # Should be positive for squared values  # doctest: +ELLIPSIS\n        Array(True, dtype=bool...)\n\n    Note:\n        This method uses zero tangents in the dual number computation,\n        effectively performing only the forward pass through the stochastic\n        computation graph.\n    \"\"\"\n    tangents = jtu.tree_map(lambda _: 0.0, args)\n    return self.jvp_estimate(*Dual.dual_tree(args, tangents)).primal\n</code></pre>"},{"location":"reference/adev/#genjax.adev.Expectation.estimate--just-compute-ex2-where-x-normalmu-1","title":"Just compute E[X^2] where X ~ Normal(mu, 1)","text":"<p>expectation_value = mean_squared.estimate(2.0) jnp.isfinite(expectation_value)  # doctest: +ELLIPSIS Array(True, dtype=bool...) expectation_value &gt; 0  # Should be positive for squared values  # doctest: +ELLIPSIS Array(True, dtype=bool...)</p>"},{"location":"reference/adev/#genjax.adev.REINFORCE","title":"REINFORCE","text":"<p>               Bases: <code>ADEVPrimitive</code></p> <p>REINFORCE (score function) gradient estimator primitive.</p> <p>Implements the REINFORCE gradient estimator (Williams, 1992), also known as the score function estimator or likelihood ratio method. This estimator is one of the key gradient estimation strategies supported by the ADEV framework.</p> <p>Theoretical Foundation: The REINFORCE estimator is based on the score function identity:     \u2207_\u03b8 E[f(X)] = E[f(X) * \u2207_\u03b8 log p(X; \u03b8)]</p> <p>where \u2207_\u03b8 log p(X; \u03b8) is the score function. This identity holds for any distribution p(X; \u03b8) with differentiable log-density, making REINFORCE universally applicable but potentially high-variance.</p> <p>ADEV Implementation: Within ADEV's CPS framework, REINFORCE: 1. Samples X ~ p(\u00b7; \u03b8) using the current parameters 2. Evaluates f(X) using the dual continuation (kdual) 3. Computes the score function \u2207_\u03b8 log p(X; \u03b8) via JAX's JVP 4. Returns f(X) + f(X) * \u2207_\u03b8 log p(X; \u03b8) as the gradient estimate</p> Note <p>While general-purpose, REINFORCE can exhibit high variance. Reparameterization is preferred when available, as proven more efficient in the ADEV paper.</p>"},{"location":"reference/adev/#genjax.adev.REINFORCE.sample","title":"sample","text":"<pre><code>sample(*args)\n</code></pre> <p>Forward sampling using the provided sample function.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def sample(self, *args):\n    \"\"\"Forward sampling using the provided sample function.\"\"\"\n    return self.sample_function.value(*args)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.REINFORCE.prim_jvp_estimate","title":"prim_jvp_estimate","text":"<pre><code>prim_jvp_estimate(dual_tree: DualTree, konts: tuple[Any, ...])\n</code></pre> <p>REINFORCE gradient estimation using the score function identity.</p> <p>Implements the score function estimator: \u2207_\u03b8 E[f(X)] = E[f(X) * \u2207_\u03b8 log p(X; \u03b8)]</p> <p>This method applies the ADEV CPS transformation for REINFORCE: 1. Sample X ~ p(\u00b7; \u03b8) from the distribution with current parameters 2. Evaluate f(X) using the dual continuation (kdual) to get the function value 3. Compute the score function \u2207_\u03b8 log p(X; \u03b8) using JAX's forward-mode AD 4. Combine via the REINFORCE identity: f(X) + f(X) * \u2207_\u03b8 log p(X; \u03b8)</p> <p>The dual continuation captures the ADEV-transformed \"rest of the computation\" after this stochastic choice, enabling modular composition with other gradient estimation strategies as described in the ADEV paper.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def prim_jvp_estimate(\n    self,\n    dual_tree: DualTree,\n    konts: tuple[Any, ...],\n):\n    \"\"\"REINFORCE gradient estimation using the score function identity.\n\n    Implements the score function estimator: \u2207_\u03b8 E[f(X)] = E[f(X) * \u2207_\u03b8 log p(X; \u03b8)]\n\n    This method applies the ADEV CPS transformation for REINFORCE:\n    1. Sample X ~ p(\u00b7; \u03b8) from the distribution with current parameters\n    2. Evaluate f(X) using the dual continuation (kdual) to get the function value\n    3. Compute the score function \u2207_\u03b8 log p(X; \u03b8) using JAX's forward-mode AD\n    4. Combine via the REINFORCE identity: f(X) + f(X) * \u2207_\u03b8 log p(X; \u03b8)\n\n    The dual continuation captures the ADEV-transformed \"rest of the computation\"\n    after this stochastic choice, enabling modular composition with other\n    gradient estimation strategies as described in the ADEV paper.\n    \"\"\"\n    (_, kdual) = konts\n    primals = Dual.tree_primal(dual_tree)\n    tangents = Dual.tree_tangent(dual_tree)\n\n    # Sample from the distribution\n    v = self.sample(*primals)\n\n    # Evaluate f(X) using dual continuation\n    dual_tree = Dual.tree_pure(v)\n    out_dual = kdual(dual_tree)\n    (out_primal,), (out_tangent,) = Dual.tree_unzip(out_dual)\n\n    # Compute score function: \u2207log p(X)\n    # For discrete values, use float0 tangent type as required by JAX\n    v_tangent = (\n        jnp.zeros(v.shape, dtype=jax.dtypes.float0)\n        if v.dtype in (jnp.bool_, jnp.int32, jnp.int64)\n        else jnp.zeros_like(v)\n    )\n    _, lp_tangent = jax.jvp(\n        self.differentiable_logpdf.value,\n        (v, *primals),\n        (v_tangent, *tangents),\n    )\n\n    # REINFORCE identity: \u2207E[f(X)] = f(X) + f(X) * \u2207log p(X)\n    # This gives an unbiased estimate of the gradient as proven in the ADEV paper\n    return Dual(out_primal, out_tangent + (out_primal * lp_tangent))\n</code></pre>"},{"location":"reference/adev/#genjax.adev.FlipEnum","title":"FlipEnum","text":"<p>               Bases: <code>ADEVPrimitive</code></p> <p>Exact enumeration gradient estimator for Bernoulli distributions.</p> <p>For discrete distributions with finite support, we can compute exact gradients by enumerating all possible outcomes and weighting by their probabilities. This gives zero-variance gradient estimates for the flip/Bernoulli case.</p> <p>The estimator computes: \u2207E[f(X)] = pf(True) + (1-p)f(False)</p>"},{"location":"reference/adev/#genjax.adev.FlipMVD","title":"FlipMVD","text":"<p>               Bases: <code>ADEVPrimitive</code></p> <p>Measure-Valued Derivative (MVD) gradient estimator for Bernoulli distributions.</p> <p>Implements the measure-valued derivative approach for gradient estimation with discrete distributions. MVD is a flexible gradient estimation technique that decomposes the derivative of a probability density into positive and negative components: \u2207_\u03b8 p(x; \u03b8) = c_\u03b8(p^+(x; \u03b8) - p^-(x; \u03b8)).</p> <p>Theoretical Foundation: For discrete distributions like Bernoulli, MVD enables gradient estimation without requiring differentiability assumptions. The estimator works by: 1. Sampling from the original distribution 2. Evaluating the function on both the sampled value and its complement 3. Using a signed difference to create an unbiased gradient estimate</p> <p>MVD Implementation for Bernoulli: The key insight is using the \"phantom estimator\" approach where: - The sampled outcome determines the sign via (-1)^v - Both the actual outcome and its complement are evaluated - The difference (other - b_primal) captures the discrete gradient</p> <p>Advantages: - Works with discrete distributions where REINFORCE may have issues - No differentiability requirements on the objective function - Provides unbiased gradient estimates for discrete parameters</p> <p>Disadvantages: - Computationally expensive (requires multiple evaluations) - Higher variance than reparameterization when applicable - Only applies to single parameters at a time</p> Note <p>This is a \"phantom estimator\" that evaluates the function on auxiliary samples (the complement outcome) to construct the gradient estimate. The (-1)^v term creates the appropriate sign for the discrete difference.</p>"},{"location":"reference/adev/#genjax.adev.FlipMVD.sample","title":"sample","text":"<pre><code>sample(*args)\n</code></pre> <p>Sample from Bernoulli distribution.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def sample(self, *args):\n    \"\"\"Sample from Bernoulli distribution.\"\"\"\n    p = (args,)\n    return 1 == bernoulli.sample(probs=p)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.FlipMVD.prim_jvp_estimate","title":"prim_jvp_estimate","text":"<pre><code>prim_jvp_estimate(dual_tree: DualTree, konts: tuple[Any, ...])\n</code></pre> <p>Measure-valued derivative gradient estimation for Bernoulli.</p> <p>Implements the MVD approach using phantom estimation: 1. Sample v ~ Bernoulli(p) to get the primary outcome 2. Evaluate f(v) using the dual continuation (kdual) 3. Evaluate f(\u00acv) using the pure continuation (kpure) as phantom estimate 4. Combine with signed difference: (-1)^v * (f(\u00acv) - f(v))</p> <p>The (-1)^v term ensures the correct sign for the discrete gradient: - When v=1: -1 * (f(0) - f(1)) = f(1) - f(0) - When v=0: +1 * (f(1) - f(0)) = f(1) - f(0)</p> <p>This creates an unbiased estimator of \u2207_p E[f(X)] for X ~ Bernoulli(p).</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def prim_jvp_estimate(\n    self,\n    dual_tree: DualTree,\n    konts: tuple[Any, ...],\n):\n    \"\"\"Measure-valued derivative gradient estimation for Bernoulli.\n\n    Implements the MVD approach using phantom estimation:\n    1. Sample v ~ Bernoulli(p) to get the primary outcome\n    2. Evaluate f(v) using the dual continuation (kdual)\n    3. Evaluate f(\u00acv) using the pure continuation (kpure) as phantom estimate\n    4. Combine with signed difference: (-1)^v * (f(\u00acv) - f(v))\n\n    The (-1)^v term ensures the correct sign for the discrete gradient:\n    - When v=1: -1 * (f(0) - f(1)) = f(1) - f(0)\n    - When v=0: +1 * (f(1) - f(0)) = f(1) - f(0)\n\n    This creates an unbiased estimator of \u2207_p E[f(X)] for X ~ Bernoulli(p).\n    \"\"\"\n    (kpure, kdual) = konts\n    (p_primal,) = Dual.tree_primal(dual_tree)\n    (p_tangent,) = Dual.tree_tangent(dual_tree)  # Fix: was tree_primal\n\n    # Sample from Bernoulli(p)\n    v = bernoulli.sample(probs=p_primal)\n    b = v == 1\n\n    # Evaluate f(v) using dual continuation\n    # For discrete values, use float0 tangent type as required by JAX\n    b_tangent_zero = (\n        jnp.zeros(b.shape, dtype=jax.dtypes.float0)\n        if b.dtype in (jnp.bool_, jnp.int32, jnp.int64)\n        else jnp.zeros_like(b)\n    )\n    b_dual = kdual(Dual(b, b_tangent_zero))\n    (b_primal,), (b_tangent,) = Dual.tree_unzip(b_dual)\n\n    # Evaluate f(\u00acv) using pure continuation (phantom estimate)\n    other_result = kpure(jnp.logical_not(b))\n\n    # Extract scalar value using JAX-compatible tree operations\n    # kpure may return a pytree structure, so we flatten and take the first element\n    other_flat, _ = jtu.tree_flatten(other_result)\n    other = other_flat[0]  # Assume there's always at least one element\n\n    # MVD estimator: (-1)^v * (f(\u00acv) - f(v))\n    # This creates the signed discrete difference for gradient estimation\n    est = ((-1) ** v) * (other - b_primal)\n\n    return Dual(b_primal, b_tangent + est * p_tangent)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.NormalREPARAM","title":"NormalREPARAM","text":"<p>               Bases: <code>ADEVPrimitive</code></p> <p>Reparameterization (pathwise) gradient estimator for normal distributions.</p> <p>Implements the reparameterization trick, also known as the pathwise estimator, which is one of the core gradient estimation strategies in the ADEV framework. This provides low-variance gradient estimates for reparameterizable distributions.</p> <p>Theoretical Foundation: For a reparameterizable distribution p(X; \u03b8) = p(g(\u03b5; \u03b8)) where \u03b5 ~ p(\u03b5) is parameter-free, the pathwise estimator is:     \u2207_\u03b8 E[f(X)] = E[\u2207_\u03b8 f(g(\u03b5; \u03b8))]</p> <p>For Normal(\u03bc, \u03c3): X = g(\u03b5; \u03bc, \u03c3) = \u03bc + \u03c3 * \u03b5, where \u03b5 ~ Normal(0, 1) This reparameterization allows gradients to flow directly through the parameters \u03bc and \u03c3 via standard automatic differentiation (chain rule).</p> <p>ADEV Implementation: Within ADEV's CPS framework, reparameterization: 1. Samples parameter-free noise \u03b5 ~ Normal(0, 1) 2. Applies the transformation X = \u03bc + \u03c3 * \u03b5 with JAX's JVP for gradients 3. Passes the reparameterized sample through the dual continuation (kdual)</p> <p>This strategy typically exhibits lower variance than REINFORCE, as noted in the ADEV paper and empirical studies (Kingma &amp; Welling, 2014).</p>"},{"location":"reference/adev/#genjax.adev.NormalREPARAM.prim_jvp_estimate","title":"prim_jvp_estimate","text":"<pre><code>prim_jvp_estimate(dual_tree: DualTree, konts: tuple[Any, ...])\n</code></pre> <p>Reparameterization gradient estimation using the pathwise estimator.</p> <p>Implements: \u2207_\u03b8 E[f(X)] = E[\u2207_\u03b8 f(g(\u03b5; \u03b8))] where X = g(\u03b5; \u03b8)</p> <p>This method applies the ADEV CPS transformation for reparameterization: 1. Sample parameter-free noise \u03b5 ~ Normal(0, 1) 2. Apply reparameterization X = \u03bc + \u03c3 * \u03b5 with gradients via JAX JVP 3. Pass the dual number (X, \u2207X) to the dual continuation (kdual)</p> <p>The dual continuation captures the ADEV-transformed remainder of the computation, enabling low-variance gradient flow as described in the ADEV paper.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def prim_jvp_estimate(\n    self,\n    dual_tree: DualTree,\n    konts: tuple[Any, ...],\n):\n    \"\"\"Reparameterization gradient estimation using the pathwise estimator.\n\n    Implements: \u2207_\u03b8 E[f(X)] = E[\u2207_\u03b8 f(g(\u03b5; \u03b8))] where X = g(\u03b5; \u03b8)\n\n    This method applies the ADEV CPS transformation for reparameterization:\n    1. Sample parameter-free noise \u03b5 ~ Normal(0, 1)\n    2. Apply reparameterization X = \u03bc + \u03c3 * \u03b5 with gradients via JAX JVP\n    3. Pass the dual number (X, \u2207X) to the dual continuation (kdual)\n\n    The dual continuation captures the ADEV-transformed remainder of the\n    computation, enabling low-variance gradient flow as described in the ADEV paper.\n    \"\"\"\n    _, kdual = konts\n    (mu_primal, sigma_primal) = Dual.tree_primal(dual_tree)\n    (mu_tangent, sigma_tangent) = Dual.tree_tangent(dual_tree)\n\n    # Sample parameter-free noise\n    eps = normal.sample(0.0, 1.0)\n\n    # Reparameterization: X = \u03bc + \u03c3 * \u03b5 with gradient flow\n    def _inner(mu, sigma):\n        return mu + sigma * eps\n\n    primal_out, tangent_out = jax.jvp(\n        _inner,\n        (mu_primal, sigma_primal),\n        (mu_tangent, sigma_tangent),\n    )\n    return kdual(Dual(primal_out, tangent_out))\n</code></pre>"},{"location":"reference/adev/#genjax.adev.MultivariateNormalREPARAM","title":"MultivariateNormalREPARAM","text":"<p>               Bases: <code>ADEVPrimitive</code></p> <p>Multivariate reparameterization (pathwise) gradient estimator.</p> <p>Extends the reparameterization trick to multivariate normal distributions, implementing the pathwise estimator for high-dimensional parameter spaces as supported by the ADEV framework.</p> <p>Theoretical Foundation: For MultivariateNormal(\u03bc, \u03a3), the reparameterization is:     X = g(\u03b5; \u03bc, \u03a3) = \u03bc + L @ \u03b5 where L = cholesky(\u03a3) and \u03b5 ~ Normal(0, I).</p> The pathwise estimator then gives <p>\u2207{\u03bc,\u03a3} E[f(X)] = E[\u2207 f(\u03bc + L @ \u03b5)]</p> <p>ADEV Implementation: This primitive enables efficient gradient flow with respect to both the mean vector \u03bc and covariance matrix \u03a3, crucial for scalable variational inference in high-dimensional spaces. The Cholesky decomposition ensures positive definiteness while enabling automatic differentiation through the covariance structure.</p> <p>This implementation follows the ADEV paper's approach to modular gradient estimation, allowing seamless integration with other stochastic primitives in complex probabilistic programs.</p>"},{"location":"reference/adev/#genjax.adev.MultivariateNormalREPARAM.prim_jvp_estimate","title":"prim_jvp_estimate","text":"<pre><code>prim_jvp_estimate(dual_tree: DualTree, konts: tuple[Any, ...])\n</code></pre> <p>Multivariate reparameterization using Cholesky decomposition.</p> <p>Implements: \u2207E[f(X)] = E[\u2207f(\u03bc + L @ \u03b5)] where L = cholesky(\u03a3)</p> <p>This method applies the pathwise estimator for multivariate normal distributions: 1. Sample standard multivariate normal noise \u03b5 ~ Normal(0, I) 2. Apply Cholesky reparameterization X = \u03bc + L @ \u03b5 with gradient flow 3. Pass the dual number (X, \u2207X) to the dual continuation (kdual)</p> <p>The Cholesky decomposition ensures efficient and numerically stable gradients with respect to the covariance matrix \u03a3, as described in the ADEV framework for modular gradient estimation strategies.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def prim_jvp_estimate(\n    self,\n    dual_tree: DualTree,\n    konts: tuple[Any, ...],\n):\n    \"\"\"Multivariate reparameterization using Cholesky decomposition.\n\n    Implements: \u2207E[f(X)] = E[\u2207f(\u03bc + L @ \u03b5)] where L = cholesky(\u03a3)\n\n    This method applies the pathwise estimator for multivariate normal distributions:\n    1. Sample standard multivariate normal noise \u03b5 ~ Normal(0, I)\n    2. Apply Cholesky reparameterization X = \u03bc + L @ \u03b5 with gradient flow\n    3. Pass the dual number (X, \u2207X) to the dual continuation (kdual)\n\n    The Cholesky decomposition ensures efficient and numerically stable\n    gradients with respect to the covariance matrix \u03a3, as described in\n    the ADEV framework for modular gradient estimation strategies.\n    \"\"\"\n    _, kdual = konts\n    (loc_primal, cov_primal) = Dual.tree_primal(dual_tree)\n    (loc_tangent, cov_tangent) = Dual.tree_tangent(dual_tree)\n\n    # Sample standard multivariate normal: \u03b5 ~ Normal(0, I)\n    eps = multivariate_normal.sample(\n        jnp.zeros_like(loc_primal), jnp.eye(loc_primal.shape[-1])\n    )\n\n    # Multivariate reparameterization: X = \u03bc + L @ \u03b5 where L = cholesky(\u03a3)\n    # This provides efficient gradients for both mean and covariance parameters\n    def _inner(loc, cov):\n        L = jnp.linalg.cholesky(cov)\n        return loc + L @ eps\n\n    primal_out, tangent_out = jax.jvp(\n        _inner,\n        (loc_primal, cov_primal),\n        (loc_tangent, cov_tangent),\n    )\n    return kdual(Dual(primal_out, tangent_out))\n</code></pre>"},{"location":"reference/adev/#genjax.adev.sample_primitive","title":"sample_primitive","text":"<pre><code>sample_primitive(adev_prim: ADEVPrimitive, *args)\n</code></pre> <p>Integrate an ADEV primitive with the PJAX infrastructure.</p> <p>This function wraps an ADEVPrimitive so it can be used within GenJAX's probabilistic programming system. It ensures the primitive works correctly with JAX transformations (jit, vmap, grad) and addressing (@) operators.</p> <p>The key insight is that ADEV primitives need to be integrated with PJAX's sample_binder to get proper parameter setup (like flat_keyful_sampler) that enables compatibility with the seed transformation and other GenJAX features.</p> Note <p>This function was crucial for fixing the flat_keyful_sampler error - previously ADEV primitives bypassed sample_binder and lacked proper parameter setup for JAX transformations.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def sample_primitive(adev_prim: ADEVPrimitive, *args):\n    \"\"\"Integrate an ADEV primitive with the PJAX infrastructure.\n\n    This function wraps an ADEVPrimitive so it can be used within GenJAX's\n    probabilistic programming system. It ensures the primitive works correctly\n    with JAX transformations (jit, vmap, grad) and addressing (@) operators.\n\n    The key insight is that ADEV primitives need to be integrated with PJAX's\n    sample_binder to get proper parameter setup (like flat_keyful_sampler) that\n    enables compatibility with the seed transformation and other GenJAX features.\n\n    Args:\n        adev_prim: The ADEV primitive to integrate\n        *args: Arguments to pass to the primitive's sample method\n\n    Returns:\n        Sample from the primitive, properly integrated with PJAX infrastructure\n\n    Note:\n        This function was crucial for fixing the flat_keyful_sampler error -\n        previously ADEV primitives bypassed sample_binder and lacked proper\n        parameter setup for JAX transformations.\n    \"\"\"\n\n    def _adev_prim_call(key, adev_prim, *args, **kwargs):\n        \"\"\"Wrapper function that conforms to sample_binder's expected signature.\"\"\"\n        return adev_prim.sample(*args)\n\n    return sample_binder(_adev_prim_call)(adev_prim, *args)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.expectation","title":"expectation","text":"<pre><code>expectation(source: Callable[..., Any]) -&gt; Expectation\n</code></pre> <p>Decorator to create an Expectation object from a stochastic function.</p> <p>This decorator transforms a function containing stochastic operations into an Expectation object that supports automatic differentiation of expectation values. The decorated function should use ADEV-compatible distributions (those with gradient estimation strategies like normal_reparam, normal_reinforce, etc.).</p> Example <p>from genjax.adev import expectation, normal_reparam</p> <p>More complex example with multiple variables: @expectation def complex_objective(mu, sigma):     x = normal_reparam(mu, sigma)     y = normal_reinforce(0.0, 1.0)  # REINFORCE strategy     return jnp.sin(x) * jnp.cos(y)</p> <p>grad_mu, grad_sigma = complex_objective.grad_estimate(0.5, 1.2) ```</p> Note <p>The function should only use ADEV-compatible distributions that have gradient estimation strategies. Regular distributions (normal, beta, etc.) won't provide gradient estimates - use their ADEV variants instead (normal_reparam, normal_reinforce, flip_enum, etc.).</p> <p>The resulting Expectation object's interfaces (grad_estimate, estimate, etc.) are compatible with JAX transformations like jit and modular_vmap. The Expectation object itself is also a Pytree, so it can be passed as an argument to JAX-transformed functions. Use modular_vmap instead of regular vmap for proper handling of probabilistic primitives within ADEV programs.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def expectation(source: Callable[..., Any]) -&gt; Expectation:\n    \"\"\"Decorator to create an Expectation object from a stochastic function.\n\n    This decorator transforms a function containing stochastic operations into an\n    Expectation object that supports automatic differentiation of expectation values.\n    The decorated function should use ADEV-compatible distributions (those with\n    gradient estimation strategies like normal_reparam, normal_reinforce, etc.).\n\n    Args:\n        source: Function containing stochastic operations using ADEV primitives\n\n    Returns:\n        Expectation object with grad_estimate, jvp_estimate, and estimate methods\n\n    Example:\n        &gt;&gt;&gt; from genjax.adev import expectation, normal_reparam\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Basic usage\n        &gt;&gt;&gt; @expectation\n        ... def quadratic_loss(theta):\n        ...     x = normal_reparam(theta, 1.0)  # Reparameterizable distribution\n        ...     return (x - 2.0)**2\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute gradient\n        &gt;&gt;&gt; gradient = quadratic_loss.grad_estimate(1.0)\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; jnp.isfinite(gradient)  # doctest: +ELLIPSIS\n        Array(True, dtype=bool...)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute expectation value\n        &gt;&gt;&gt; loss_value = quadratic_loss.estimate(1.0)\n        &gt;&gt;&gt; jnp.isfinite(loss_value)  # doctest: +ELLIPSIS\n        Array(True, dtype=bool...)\n\n        More complex example with multiple variables:\n        @expectation\n        def complex_objective(mu, sigma):\n            x = normal_reparam(mu, sigma)\n            y = normal_reinforce(0.0, 1.0)  # REINFORCE strategy\n            return jnp.sin(x) * jnp.cos(y)\n\n        grad_mu, grad_sigma = complex_objective.grad_estimate(0.5, 1.2)\n        ```\n\n    Note:\n        The function should only use ADEV-compatible distributions that have\n        gradient estimation strategies. Regular distributions (normal, beta, etc.)\n        won't provide gradient estimates - use their ADEV variants instead\n        (normal_reparam, normal_reinforce, flip_enum, etc.).\n\n        The resulting Expectation object's interfaces (grad_estimate, estimate, etc.)\n        are compatible with JAX transformations like jit and modular_vmap. The\n        Expectation object itself is also a Pytree, so it can be passed as an\n        argument to JAX-transformed functions. Use modular_vmap instead of regular\n        vmap for proper handling of probabilistic primitives within ADEV programs.\n    \"\"\"\n    prog = ADEVProgram(const(source))\n    return Expectation(prog)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.expectation--basic-usage","title":"Basic usage","text":"<p>@expectation ... def quadratic_loss(theta): ...     x = normal_reparam(theta, 1.0)  # Reparameterizable distribution ...     return (x - 2.0)**2</p>"},{"location":"reference/adev/#genjax.adev.expectation--compute-gradient","title":"Compute gradient","text":"<p>gradient = quadratic_loss.grad_estimate(1.0) import jax.numpy as jnp jnp.isfinite(gradient)  # doctest: +ELLIPSIS Array(True, dtype=bool...)</p>"},{"location":"reference/adev/#genjax.adev.expectation--compute-expectation-value","title":"Compute expectation value","text":"<p>loss_value = quadratic_loss.estimate(1.0) jnp.isfinite(loss_value)  # doctest: +ELLIPSIS Array(True, dtype=bool...)</p>"},{"location":"reference/adev/#genjax.adev.invoke_closed_over","title":"invoke_closed_over","text":"<pre><code>invoke_closed_over(instance, args)\n</code></pre> <p>Primal forward-mode function for Expectation objects with custom JVP rule.</p> <p>This function serves as the primal computation for JAX's custom JVP rule registration. It's defined externally to the Expectation class to avoid complications with defining custom JVP rules on Pytree classes.</p> Note <p>This function is decorated with @jax.custom_jvp to register ADEV's jvp_estimate as the custom JVP rule. This allows JAX to automatically synthesize grad implementations that use ADEV's unbiased gradient estimators.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>@jax.custom_jvp\ndef invoke_closed_over(instance, args):\n    \"\"\"Primal forward-mode function for Expectation objects with custom JVP rule.\n\n    This function serves as the primal computation for JAX's custom JVP rule\n    registration. It's defined externally to the Expectation class to avoid\n    complications with defining custom JVP rules on Pytree classes.\n\n    Args:\n        instance: Expectation object to evaluate\n        args: Arguments to pass to the expectation\n\n    Returns:\n        The expectation value computed by instance.estimate(*args)\n\n    Note:\n        This function is decorated with @jax.custom_jvp to register ADEV's\n        jvp_estimate as the custom JVP rule. This allows JAX to automatically\n        synthesize grad implementations that use ADEV's unbiased gradient estimators.\n    \"\"\"\n    return instance.estimate(*args)\n</code></pre>"},{"location":"reference/adev/#genjax.adev.invoke_closed_over_jvp","title":"invoke_closed_over_jvp","text":"<pre><code>invoke_closed_over_jvp(primals: tuple, tangents: tuple)\n</code></pre> <p>Custom JVP rule that delegates to ADEV's jvp_estimate method.</p> <p>This function registers ADEV's jvp_estimate as the JVP rule for Expectation objects, enabling JAX to automatically synthesize grad implementations. When JAX encounters invoke_closed_over in a computation that requires differentiation, it will use this rule instead of trying to differentiate through the stochastic computation.</p> Note <p>This converts between JAX's JVP representation (separate primals/tangents) and ADEV's dual number representation, then delegates to jvp_estimate for the actual gradient computation using ADEV's CPS transformation.</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def invoke_closed_over_jvp(primals: tuple, tangents: tuple):\n    \"\"\"Custom JVP rule that delegates to ADEV's jvp_estimate method.\n\n    This function registers ADEV's jvp_estimate as the JVP rule for Expectation\n    objects, enabling JAX to automatically synthesize grad implementations.\n    When JAX encounters invoke_closed_over in a computation that requires\n    differentiation, it will use this rule instead of trying to differentiate\n    through the stochastic computation.\n\n    Args:\n        primals: Tuple of (instance, args) representing the primal values\n        tangents: Tuple of (_, tangents) representing the tangent vectors\n\n    Returns:\n        Tuple of (primal_output, tangent_output) where:\n        - primal_output: The expectation value E[f(X)]\n        - tangent_output: ADEV's unbiased gradient estimate \u2207E[f(X)]\n\n    Note:\n        This converts between JAX's JVP representation (separate primals/tangents)\n        and ADEV's dual number representation, then delegates to jvp_estimate\n        for the actual gradient computation using ADEV's CPS transformation.\n    \"\"\"\n    (instance, primals) = primals\n    (_, tangents) = tangents\n    duals = Dual.dual_tree(primals, tangents)\n    out_dual = instance.jvp_estimate(*duals)\n    (v,), (tangent,) = Dual.tree_unzip(out_dual)\n    return v, tangent\n</code></pre>"},{"location":"reference/adev/#genjax.adev.reinforce","title":"reinforce","text":"<pre><code>reinforce(sample_func, logpdf_func)\n</code></pre> <p>Factory function for creating REINFORCE gradient estimators.</p> Example <p>normal_reinforce_prim = reinforce(normal.sample, normal.logpdf)</p> Source code in <code>src/genjax/adev/__init__.py</code> <pre><code>def reinforce(sample_func, logpdf_func):\n    \"\"\"Factory function for creating REINFORCE gradient estimators.\n\n    Args:\n        sample_func: Function to sample from distribution\n        logpdf_func: Function to compute log-probability density\n\n    Returns:\n        REINFORCE primitive for the given distribution\n\n    Example:\n        &gt;&gt;&gt; normal_reinforce_prim = reinforce(normal.sample, normal.logpdf)\n    \"\"\"\n    return REINFORCE(const(sample_func), const(logpdf_func))\n</code></pre>"},{"location":"reference/core/","title":"genjax.core","text":"<p>Core functionality for GenJAX including the Generative Function Interface, traces, and model construction.</p>"},{"location":"reference/core/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>The Generative Function Interface (GFI) is based on measure theory. A generative function \\(g\\) defines:</p> <ul> <li>A measure kernel \\(P(dx; \\text{args})\\) over measurable space \\(X\\)</li> <li>A return value function \\(f(x, \\text{args}) \\rightarrow R\\)</li> <li>Internal proposal family \\(Q(dx'; \\text{args}, x)\\)</li> </ul> <p>The importance weight from <code>generate</code> is:</p> \\[w = \\log \\frac{P(\\text{all\\_choices})}{Q(\\text{free\\_choices} | \\text{constrained\\_choices})}\\]"},{"location":"reference/core/#genjax.core","title":"core","text":""},{"location":"reference/core/#genjax.core.Pytree","title":"Pytree","text":"<p>               Bases: <code>Struct</code></p> <p><code>Pytree</code> is an abstract base class which registers a class with JAX's <code>Pytree</code> system. JAX's <code>Pytree</code> system tracks how data classes should behave across JAX-transformed function boundaries, like <code>jax.jit</code> or <code>jax.vmap</code>.</p> <p>Inheriting this class provides the implementor with the freedom to declare how the subfields of a class should behave:</p> <ul> <li><code>Pytree.static(...)</code>: the value of the field cannot be a JAX traced value, it must be a Python literal, or a constant). The values of static fields are embedded in the <code>PyTreeDef</code> of any instance of the class.</li> <li><code>Pytree.field(...)</code> or no annotation: the value may be a JAX traced value, and JAX will attempt to convert it to tracer values inside of its transformations.</li> </ul> <p>If a field points to another <code>Pytree</code>, it should not be declared as <code>Pytree.static()</code>, as the <code>Pytree</code> interface will automatically handle the <code>Pytree</code> fields as dynamic fields.</p>"},{"location":"reference/core/#genjax.core.Pytree.dataclass","title":"dataclass  <code>staticmethod</code>","text":"<pre><code>dataclass(incoming: None = None, /, **kwargs) -&gt; Callable[[type[R]], type[R]]\n</code></pre><pre><code>dataclass(incoming: type[R], /, **kwargs) -&gt; type[R]\n</code></pre> <pre><code>dataclass(incoming: type[R] | None = None, /, **kwargs) -&gt; type[R] | Callable[[type[R]], type[R]]\n</code></pre> <p>Denote that a class (which is inheriting <code>Pytree</code>) should be treated as a dataclass, meaning it can hold data in fields which are declared as part of the class.</p> <p>A dataclass is to be distinguished from a \"methods only\" <code>Pytree</code> class, which does not have fields, but may define methods. The latter cannot be instantiated, but can be inherited from, while the former can be instantiated: the <code>Pytree.dataclass</code> declaration informs the system how to instantiate the class as a dataclass, and how to automatically define JAX's <code>Pytree</code> interfaces (<code>tree_flatten</code>, <code>tree_unflatten</code>, etc.) for the dataclass, based on the fields declared in the class, and possibly <code>Pytree.static(...)</code> or <code>Pytree.field(...)</code> annotations (or lack thereof, the default is that all fields are <code>Pytree.field(...)</code>).</p> <p>All <code>Pytree</code> dataclasses support pretty printing, as well as rendering to HTML.</p>"},{"location":"reference/core/#genjax.core.Pytree.dataclass--examples","title":"Examples","text":"<p>from genjax import Pytree from jaxtyping import ArrayLike import jax.numpy as jnp</p> <p>@Pytree.dataclass ... class MyClass(Pytree): ...     my_static_field: int = Pytree.static() ...     my_dynamic_field: ArrayLike</p> <p>instance = MyClass(10, jnp.array(5.0)) instance.my_static_field 10 instance.my_dynamic_field  # doctest: +ELLIPSIS Array(5., dtype=float32...)</p> Source code in <code>src/genjax/core.py</code> <pre><code>@dataclass_transform(\n    frozen_default=True,\n)\n@staticmethod\ndef dataclass(\n    incoming: type[R] | None = None,\n    /,\n    **kwargs,\n) -&gt; type[R] | Callable[[type[R]], type[R]]:\n    \"\"\"\n    Denote that a class (which is inheriting `Pytree`) should be treated\n    as a dataclass, meaning it can hold data in fields which are\n    declared as part of the class.\n\n    A dataclass is to be distinguished from a \"methods only\"\n    `Pytree` class, which does not have fields, but may define methods.\n    The latter cannot be instantiated, but can be inherited from,\n    while the former can be instantiated:\n    the `Pytree.dataclass` declaration informs the system _how\n    to instantiate_ the class as a dataclass,\n    and how to automatically define JAX's `Pytree` interfaces\n    (`tree_flatten`, `tree_unflatten`, etc.) for the dataclass, based\n    on the fields declared in the class, and possibly `Pytree.static(...)`\n    or `Pytree.field(...)` annotations (or lack thereof, the default is\n    that all fields are `Pytree.field(...)`).\n\n    All `Pytree` dataclasses support pretty printing, as well as rendering\n    to HTML.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; from genjax import Pytree\n    &gt;&gt;&gt; from jaxtyping import ArrayLike\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; @Pytree.dataclass\n    ... class MyClass(Pytree):\n    ...     my_static_field: int = Pytree.static()\n    ...     my_dynamic_field: ArrayLike\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; instance = MyClass(10, jnp.array(5.0))\n    &gt;&gt;&gt; instance.my_static_field\n    10\n    &gt;&gt;&gt; instance.my_dynamic_field  # doctest: +ELLIPSIS\n    Array(5., dtype=float32...)\n    \"\"\"\n\n    return pz.pytree_dataclass(\n        incoming,\n        overwrite_parent_init=True,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/#genjax.core.Pytree.static","title":"static  <code>staticmethod</code>","text":"<pre><code>static(**kwargs)\n</code></pre> <p>Declare a field of a <code>Pytree</code> dataclass to be static. Users can provide additional keyword argument options, like <code>default</code> or <code>default_factory</code>, to customize how the field is instantiated when an instance of the dataclass is instantiated.` Fields which are provided with default values must come after required fields in the dataclass declaration.</p>"},{"location":"reference/core/#genjax.core.Pytree.static--examples","title":"Examples","text":"<p>from genjax import Pytree from jaxtyping import ArrayLike import jax.numpy as jnp</p> <p>@Pytree.dataclass ... class MyClass(Pytree): ...     my_dynamic_field: ArrayLike ...     my_static_field: int = Pytree.static(default=0)</p> <p>instance = MyClass(jnp.array(5.0)) instance.my_static_field 0 instance.my_dynamic_field  # doctest: +ELLIPSIS Array(5., dtype=float32...)</p> Source code in <code>src/genjax/core.py</code> <pre><code>@staticmethod\ndef static(**kwargs):\n    \"\"\"Declare a field of a `Pytree` dataclass to be static.\n    Users can provide additional keyword argument options,\n    like `default` or `default_factory`, to customize how the field is\n    instantiated when an instance of\n    the dataclass is instantiated.` Fields which are provided with default\n    values must come after required fields in the dataclass declaration.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; from genjax import Pytree\n    &gt;&gt;&gt; from jaxtyping import ArrayLike\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; @Pytree.dataclass\n    ... class MyClass(Pytree):\n    ...     my_dynamic_field: ArrayLike\n    ...     my_static_field: int = Pytree.static(default=0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; instance = MyClass(jnp.array(5.0))\n    &gt;&gt;&gt; instance.my_static_field\n    0\n    &gt;&gt;&gt; instance.my_dynamic_field  # doctest: +ELLIPSIS\n    Array(5., dtype=float32...)\n\n    \"\"\"\n    return field(metadata={\"pytree_node\": False}, **kwargs)\n</code></pre>"},{"location":"reference/core/#genjax.core.Pytree.field","title":"field  <code>staticmethod</code>","text":"<pre><code>field(**kwargs)\n</code></pre> <p>Declare a field of a <code>Pytree</code> dataclass to be dynamic. Alternatively, one can leave the annotation off in the declaration.</p> Source code in <code>src/genjax/core.py</code> <pre><code>@staticmethod\ndef field(**kwargs):\n    \"\"\"Declare a field of a `Pytree` dataclass to be dynamic.\n    Alternatively, one can leave the annotation off in the declaration.\"\"\"\n    return field(**kwargs)\n</code></pre>"},{"location":"reference/core/#genjax.core.Const","title":"Const","text":"<p>               Bases: <code>Generic[A]</code>, <code>Pytree</code></p> <p>A Pytree wrapper for Python literals that should remain static.</p> <p>This class wraps Python values that need to stay as literals (not become JAX tracers) when used inside JAX transformations. The wrapped value is marked as static, ensuring it's embedded in the PyTreeDef rather than becoming a traced value.</p> Example <pre><code># Instead of: n_steps: int (becomes tracer in JAX transforms)\n# Use: n_steps: Const[int] (stays as Python int)\n\ndef my_function(n_steps: Const[int]):\n    for i in range(n_steps.value):  # n_steps.value is Python int\n        ...\n</code></pre>"},{"location":"reference/core/#genjax.core.NotFixedException","title":"NotFixedException","text":"<pre><code>NotFixedException(choice_map_status: X)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when trace verification finds unfixed values.</p> <p>This exception provides a clear visualization of which parts of the choice map are properly fixed (True) vs. unfixed (False), helping users debug model structure issues during constrained inference.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def __init__(self, choice_map_status: X):\n    self.choice_map_status = choice_map_status\n    super().__init__(self._format_message())\n</code></pre>"},{"location":"reference/core/#genjax.core.Fixed","title":"Fixed","text":"<p>               Bases: <code>Generic[A]</code>, <code>Pytree</code></p> <p>A Pytree wrapper that denotes a random choice was provided (fixed), not proposed by a GFI's internal proposal family.</p> <p>This wrapper is used internally by Distribution implementations in <code>generate</code>, <code>update</code>, and <code>regenerate</code> methods to mark values that were constrained or provided externally rather than sampled from the distribution's internal proposal.</p> <p>The <code>Fixed</code> wrapper helps debug model structure issues during inference by tracking which random choices were externally constrained vs. internally proposed.</p>"},{"location":"reference/core/#genjax.core.Trace","title":"Trace","text":"<p>               Bases: <code>Generic[X, R]</code>, <code>Pytree</code></p>"},{"location":"reference/core/#genjax.core.Trace.get_fixed_choices","title":"get_fixed_choices  <code>abstractmethod</code>","text":"<pre><code>get_fixed_choices() -&gt; X\n</code></pre> <p>Get choices preserving Fixed wrappers.</p> <p>Returns the raw choice structure with Fixed wrappers intact, used for verification that values were constrained during inference.</p> Source code in <code>src/genjax/core.py</code> <pre><code>@abstractmethod\ndef get_fixed_choices(self) -&gt; X:\n    \"\"\"Get choices preserving Fixed wrappers.\n\n    Returns the raw choice structure with Fixed wrappers intact,\n    used for verification that values were constrained during inference.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/core/#genjax.core.Trace.verify","title":"verify","text":"<pre><code>verify() -&gt; None\n</code></pre> <p>Verify that all leaf values in the trace choices were fixed (constrained).</p> <p>Checks that all random choices in the trace are wrapped with Fixed, indicating they were provided externally rather than proposed by the GFI's internal proposal family. This helps debug model structure issues during inference.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def verify(self) -&gt; None:\n    \"\"\"Verify that all leaf values in the trace choices were fixed (constrained).\n\n    Checks that all random choices in the trace are wrapped with Fixed,\n    indicating they were provided externally rather than proposed by the\n    GFI's internal proposal family. This helps debug model structure issues\n    during inference.\n\n    Raises:\n        NotFixedException: If any leaf value is not wrapped with Fixed.\n                          The exception includes a detailed choice map showing\n                          which values are fixed vs. unfixed.\n    \"\"\"\n    # Get choices preserving Fixed wrappers\n    choice_values = get_fixed_choices(self)\n\n    # Check if value is Fixed\n    def check_instance_fixed(x):\n        return isinstance(x, Fixed)\n\n    # Flatten the tree to get all leaf choice values\n    leaf_values, tree_def = jtu.tree_flatten(\n        choice_values, is_leaf=check_instance_fixed\n    )\n\n    # Check if all leaves are Fixed\n    all_fixed = all(isinstance(leaf, Fixed) for leaf in leaf_values)\n\n    if not all_fixed:\n        # Create a boolean choice map showing which values are fixed\n        def make_bool_status(x):\n            if isinstance(x, Fixed):\n                return True\n            else:\n                return False\n\n        choice_map_status = jtu.tree_map(\n            make_bool_status, choice_values, is_leaf=check_instance_fixed\n        )\n\n        raise NotFixedException(choice_map_status)\n</code></pre>"},{"location":"reference/core/#genjax.core.Tr","title":"Tr","text":"<p>               Bases: <code>Trace[X, R]</code>, <code>Pytree</code></p> <p>Concrete implementation of the Trace interface.</p> <p>Stores all components of an execution trace including the generative function, arguments, random choices, return value, and score.</p>"},{"location":"reference/core/#genjax.core.Tr.get_fixed_choices","title":"get_fixed_choices","text":"<pre><code>get_fixed_choices() -&gt; X\n</code></pre> <p>Get choices preserving Fixed wrappers.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def get_fixed_choices(self) -&gt; X:\n    \"\"\"Get choices preserving Fixed wrappers.\"\"\"\n    return get_fixed_choices(self._choices)\n</code></pre>"},{"location":"reference/core/#genjax.core.AllSel","title":"AllSel","text":"<p>               Bases: <code>Pytree</code></p> <p>Selection that matches all addresses.</p>"},{"location":"reference/core/#genjax.core.NoneSel","title":"NoneSel","text":"<p>               Bases: <code>Pytree</code></p> <p>Selection that matches no addresses.</p>"},{"location":"reference/core/#genjax.core.StrSel","title":"StrSel","text":"<p>               Bases: <code>Pytree</code></p> <p>Selection that matches a specific string address.</p>"},{"location":"reference/core/#genjax.core.TupleSel","title":"TupleSel","text":"<p>               Bases: <code>Pytree</code></p> <p>Selection that matches a hierarchical tuple address.</p> <p>Tuple addresses represent hierarchical paths like (\"outer\", \"inner\", \"leaf\"). When matched against a single string address, it checks if that string matches the first element of the tuple, and returns a selection for the remaining path.</p>"},{"location":"reference/core/#genjax.core.DictSel","title":"DictSel","text":"<p>               Bases: <code>Pytree</code></p> <p>Selection that matches addresses using a dictionary mapping.</p>"},{"location":"reference/core/#genjax.core.ComplSel","title":"ComplSel","text":"<p>               Bases: <code>Pytree</code></p> <p>Selection that matches the complement of another selection.</p>"},{"location":"reference/core/#genjax.core.InSel","title":"InSel","text":"<p>               Bases: <code>Pytree</code></p> <p>Selection representing intersection of two selections.</p>"},{"location":"reference/core/#genjax.core.Selection","title":"Selection","text":"<p>               Bases: <code>Pytree</code></p> <p>A Selection acts as a filter to specify which random choices in a trace should be regenerated during the <code>regenerate</code> method call.</p> <p>Selections are used in inference algorithms like MCMC to specify which subset of random choices should be updated while keeping others fixed. The Selection determines which addresses (random choice names) match the selection criteria.</p> <p>A Selection wraps one of several concrete selection types: - StrSel: Matches a specific string address - DictSel: Matches addresses using a dictionary mapping - AllSel: Matches all addresses - NoneSel: Matches no addresses - ComplSel: Matches the complement of another selection - InSel: Matches the intersection of two selections - OrSel: Matches the union of two selections</p> Example <pre><code>from genjax.core import sel\n\n# Select a specific address\nselection = sel(\"x\")  # Matches address \"x\"\n\n# Select all addresses\nselection = sel(())   # Matches all addresses\n\n# Select nested addresses\nselection = sel({\"outer\": sel(\"inner\")})  # Matches \"outer\"/\"inner\"\n\n# Use in regenerate\nnew_trace, weight, discard = gen_fn.regenerate(args, trace, selection)\n</code></pre>"},{"location":"reference/core/#genjax.core.GFI","title":"GFI","text":"<p>               Bases: <code>Generic[X, R]</code>, <code>Pytree</code></p> <p>Generative Function Interface - the core abstraction for probabilistic programs.</p> <p>The GFI defines the standard interface that all generative functions must implement. It provides methods for simulation, assessment, generation, updating, and regeneration of probabilistic computations.</p> <p>Mathematical Foundation: A generative function bundles three mathematical objects: 1. Measure kernel P(dx; args) - the probability distribution over choices 2. Return value function f(x, args) -&gt; R - deterministic computation from choices 3. Internal proposal family Q(dx; args, context) - for efficient inference</p> <p>The GFI methods provide access to these mathematical objects and enable: - Forward sampling (simulate) - Density evaluation (assess) - Constrained generation (generate) - Edit moves (update, regenerate)</p> <p>All density computations are in log space for numerical stability. Weights from generate/update/regenerate enable importance sampling and MCMC.</p> Type Parameters <p>X: The type of the random choices (choice map). R: The type of the return value.</p> Core Methods <p>simulate: Sample (choices, retval) ~ P(\u00b7; args) assess: Compute log P(choices; args) generate: Sample with constraints, return importance weight update: Update trace arguments/choices, return incremental importance weight regenerate: Resample selected choices, return incremental importance weight</p> Additional Methods <p>merge: Combine choice maps (for compositional functions) log_density: Convenience method for assess that sums log densities vmap/repeat: Vectorization combinators cond: Conditional execution combinator</p>"},{"location":"reference/core/#genjax.core.GFI.simulate","title":"simulate  <code>abstractmethod</code>","text":"<pre><code>simulate(*args, **kwargs) -&gt; Trace[X, R]\n</code></pre> <p>Sample an execution trace from the generative function.</p> <p>Mathematical specification: - Samples (choices, retval) ~ P(\u00b7; args) where P is the generative function's measure kernel - Returns trace containing choices, return value, score, and arguments</p> <p>The score in the returned trace is log(1/P(choices; args)), i.e., the negative log probability density of the sampled choices.</p> Example Source code in <code>src/genjax/core.py</code> <pre><code>@abstractmethod\ndef simulate(\n    self,\n    *args,\n    **kwargs,\n) -&gt; Trace[X, R]:\n    \"\"\"Sample an execution trace from the generative function.\n\n    Mathematical specification:\n    - Samples (choices, retval) ~ P(\u00b7; args) where P is the generative function's measure kernel\n    - Returns trace containing choices, return value, score, and arguments\n\n    The score in the returned trace is log(1/P(choices; args)), i.e., the negative\n    log probability density of the sampled choices.\n\n    Args:\n        *args: Arguments to the generative function.\n        **kwargs: Keyword arguments to the generative function.\n\n    Returns:\n        A trace containing the sampled choices, return value, score, and arguments.\n\n    Example:\n        &gt;&gt;&gt; # model.simulate(mu, sigma)  # Example usage\n        &gt;&gt;&gt; # choices = trace.get_choices()\n        &gt;&gt;&gt; # score = trace.get_score()  # -log P(choices; mu, sigma)\n        &gt;&gt;&gt; pass  # doctest placeholder\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/core/#genjax.core.GFI.simulate--modelsimulatemu-sigma-example-usage","title":"model.simulate(mu, sigma)  # Example usage","text":""},{"location":"reference/core/#genjax.core.GFI.simulate--choices-traceget_choices","title":"choices = trace.get_choices()","text":""},{"location":"reference/core/#genjax.core.GFI.simulate--score-traceget_score-log-pchoices-mu-sigma","title":"score = trace.get_score()  # -log P(choices; mu, sigma)","text":"<p>pass  # doctest placeholder</p>"},{"location":"reference/core/#genjax.core.GFI.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(x: X | None, *args, **kwargs) -&gt; tuple[Trace[X, R], Weight]\n</code></pre> <p>Generate a trace with optional constraints on some choices.</p> <p>Mathematical specification: - Samples unconstrained choices ~ Q(\u00b7; constrained_choices, args) - Computes importance weight: log [P(all_choices; args) / Q(unconstrained_choices; constrained_choices, args)] - When x=None, equivalent to simulate() but returns weight=0</p> <p>The weight enables importance sampling and is crucial for inference algorithms. For fully constrained generation, the weight equals the log density.</p> Example Source code in <code>src/genjax/core.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    x: X | None,\n    *args,\n    **kwargs,\n) -&gt; tuple[Trace[X, R], Weight]:\n    \"\"\"Generate a trace with optional constraints on some choices.\n\n    Mathematical specification:\n    - Samples unconstrained choices ~ Q(\u00b7; constrained_choices, args)\n    - Computes importance weight: log [P(all_choices; args) / Q(unconstrained_choices; constrained_choices, args)]\n    - When x=None, equivalent to simulate() but returns weight=0\n\n    The weight enables importance sampling and is crucial for inference algorithms.\n    For fully constrained generation, the weight equals the log density.\n\n    Args:\n        x: Optional constraints on subset of choices. If None, equivalent to simulate.\n        *args: Arguments to the generative function.\n        **kwargs: Keyword arguments to the generative function.\n\n    Returns:\n        A tuple (trace, weight) where:\n        - trace: contains all choices (constrained + sampled) and return value\n        - weight: log [P(all_choices; args) / Q(unconstrained_choices; constrained_choices, args)]\n\n    Example:\n        &gt;&gt;&gt; # Constrain some choices\n        &gt;&gt;&gt; # constraints = {\"x\": 1.5, \"y\": 2.0}\n        &gt;&gt;&gt; # trace, weight = model.generate(constraints, mu, sigma)\n        &gt;&gt;&gt; # weight accounts for probability of constrained choices\n        &gt;&gt;&gt; pass  # doctest placeholder\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/core/#genjax.core.GFI.generate--constrain-some-choices","title":"Constrain some choices","text":""},{"location":"reference/core/#genjax.core.GFI.generate--constraints","title":"constraints =","text":""},{"location":"reference/core/#genjax.core.GFI.generate--trace-weight-modelgenerateconstraints-mu-sigma","title":"trace, weight = model.generate(constraints, mu, sigma)","text":""},{"location":"reference/core/#genjax.core.GFI.generate--weight-accounts-for-probability-of-constrained-choices","title":"weight accounts for probability of constrained choices","text":"<p>pass  # doctest placeholder</p>"},{"location":"reference/core/#genjax.core.GFI.assess","title":"assess  <code>abstractmethod</code>","text":"<pre><code>assess(x: X, *args, **kwargs) -&gt; tuple[Density, R]\n</code></pre> <p>Compute the log probability density of given choices.</p> <p>Mathematical specification: - Computes log P(choices; args) where P is the generative function's measure kernel - Also computes the return value for the given choices - Requires P(choices; args) &gt; 0 (choices must be valid)</p> Example Source code in <code>src/genjax/core.py</code> <pre><code>@abstractmethod\ndef assess(\n    self,\n    x: X,\n    *args,\n    **kwargs,\n) -&gt; tuple[Density, R]:\n    \"\"\"Compute the log probability density of given choices.\n\n    Mathematical specification:\n    - Computes log P(choices; args) where P is the generative function's measure kernel\n    - Also computes the return value for the given choices\n    - Requires P(choices; args) &gt; 0 (choices must be valid)\n\n    Args:\n        x: The choices to evaluate.\n        *args: Arguments to the generative function.\n        **kwargs: Keyword arguments to the generative function.\n\n    Returns:\n        A tuple (log_density, retval) where:\n        - log_density: log P(choices; args)\n        - retval: return value computed with the given choices\n\n    Example:\n        &gt;&gt;&gt; # log_density, retval = model.assess(choices, mu, sigma)\n        &gt;&gt;&gt; # log_density = log P(choices; mu, sigma)\n        &gt;&gt;&gt; pass  # doctest placeholder\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/core/#genjax.core.GFI.assess--log_density-retval-modelassesschoices-mu-sigma","title":"log_density, retval = model.assess(choices, mu, sigma)","text":""},{"location":"reference/core/#genjax.core.GFI.assess--log_density-log-pchoices-mu-sigma","title":"log_density = log P(choices; mu, sigma)","text":"<p>pass  # doctest placeholder</p>"},{"location":"reference/core/#genjax.core.GFI.update","title":"update  <code>abstractmethod</code>","text":"<pre><code>update(tr: Trace[X, R], x_: X | None, *args, **kwargs) -&gt; tuple[Trace[X, R], Weight, X | None]\n</code></pre> <p>Update a trace with new arguments and/or choice constraints.</p> <p>Mathematical specification: - Transforms trace from (old_args, old_choices) to (new_args, new_choices) - Computes incremental importance weight (edit move):</p> <p>weight = log [P(new_choices; new_args) / Q(new_choices; new_args, old_choices, constraints)]        - log [P(old_choices; old_args) / Q(old_choices; old_args)]</p> <p>where Q is the internal proposal distribution used for updating.</p> Example Source code in <code>src/genjax/core.py</code> <pre><code>@abstractmethod\ndef update(\n    self,\n    tr: Trace[X, R],\n    x_: X | None,\n    *args,\n    **kwargs,\n) -&gt; tuple[Trace[X, R], Weight, X | None]:\n    \"\"\"Update a trace with new arguments and/or choice constraints.\n\n    Mathematical specification:\n    - Transforms trace from (old_args, old_choices) to (new_args, new_choices)\n    - Computes incremental importance weight (edit move):\n\n    weight = log [P(new_choices; new_args) / Q(new_choices; new_args, old_choices, constraints)]\n           - log [P(old_choices; old_args) / Q(old_choices; old_args)]\n\n    where Q is the internal proposal distribution used for updating.\n\n    Args:\n        tr: Current trace to update.\n        x_: Optional constraints on choices to enforce during update.\n        *args: New arguments to the generative function.\n        **kwargs: New keyword arguments to the generative function.\n\n    Returns:\n        A tuple (new_trace, weight, discarded_choices) where:\n        - new_trace: updated trace with new arguments and choices\n        - weight: incremental importance weight for the update (enables MCMC, SMC)\n        - discarded_choices: old choice values that were changed\n\n    Example:\n        &gt;&gt;&gt; # Update trace with new arguments\n        &gt;&gt;&gt; # new_trace, weight, discarded = model.update(old_trace, None, new_mu, new_sigma)\n        &gt;&gt;&gt; # weight = log P(new_choices; new_args) - log P(old_choices; old_args)\n        &gt;&gt;&gt; pass  # doctest placeholder\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/core/#genjax.core.GFI.update--update-trace-with-new-arguments","title":"Update trace with new arguments","text":""},{"location":"reference/core/#genjax.core.GFI.update--new_trace-weight-discarded-modelupdateold_trace-none-new_mu-new_sigma","title":"new_trace, weight, discarded = model.update(old_trace, None, new_mu, new_sigma)","text":""},{"location":"reference/core/#genjax.core.GFI.update--weight-log-pnew_choices-new_args-log-pold_choices-old_args","title":"weight = log P(new_choices; new_args) - log P(old_choices; old_args)","text":"<p>pass  # doctest placeholder</p>"},{"location":"reference/core/#genjax.core.GFI.regenerate","title":"regenerate  <code>abstractmethod</code>","text":"<pre><code>regenerate(tr: Trace[X, R], sel: Selection, *args, **kwargs) -&gt; tuple[Trace[X, R], Weight, X | None]\n</code></pre> <p>Regenerate selected choices in a trace while keeping others fixed.</p> <p>Mathematical specification: - Resamples choices at addresses selected by 'sel' from their conditional distribution - Keeps non-selected choices unchanged - Computes incremental importance weight (edit move):</p> <p>weight = log P(new_selected_choices | non_selected_choices; args)        - log P(old_selected_choices | non_selected_choices; args)</p> <p>When sel selects all addresses, regenerate becomes equivalent to simulate. When sel selects no addresses, weight = 0 and trace unchanged.</p> Example Source code in <code>src/genjax/core.py</code> <pre><code>@abstractmethod\ndef regenerate(\n    self,\n    tr: Trace[X, R],\n    sel: Selection,\n    *args,\n    **kwargs,\n) -&gt; tuple[Trace[X, R], Weight, X | None]:\n    \"\"\"Regenerate selected choices in a trace while keeping others fixed.\n\n    Mathematical specification:\n    - Resamples choices at addresses selected by 'sel' from their conditional distribution\n    - Keeps non-selected choices unchanged\n    - Computes incremental importance weight (edit move):\n\n    weight = log P(new_selected_choices | non_selected_choices; args)\n           - log P(old_selected_choices | non_selected_choices; args)\n\n    When sel selects all addresses, regenerate becomes equivalent to simulate.\n    When sel selects no addresses, weight = 0 and trace unchanged.\n\n    Args:\n        tr: Current trace to regenerate from.\n        sel: Selection specifying which addresses to regenerate.\n        *args: Arguments to the generative function.\n        **kwargs: Keyword arguments to the generative function.\n\n    Returns:\n        A tuple (new_trace, weight, discarded_choices) where:\n        - new_trace: trace with selected choices resampled\n        - weight: incremental importance weight for the regeneration\n        - discarded_choices: old values of the regenerated choices\n\n    Example:\n        &gt;&gt;&gt; # Regenerate choices at addresses \"x\" and \"y\"\n        &gt;&gt;&gt; # selection = sel(\"x\") | sel(\"y\")\n        &gt;&gt;&gt; # new_trace, weight, discarded = model.regenerate(trace, selection, mu, sigma)\n        &gt;&gt;&gt; # weight accounts for probability change due to regeneration\n        &gt;&gt;&gt; pass  # doctest placeholder\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/core/#genjax.core.GFI.regenerate--regenerate-choices-at-addresses-x-and-y","title":"Regenerate choices at addresses \"x\" and \"y\"","text":""},{"location":"reference/core/#genjax.core.GFI.regenerate--selection-selx-sely","title":"selection = sel(\"x\") | sel(\"y\")","text":""},{"location":"reference/core/#genjax.core.GFI.regenerate--new_trace-weight-discarded-modelregeneratetrace-selection-mu-sigma","title":"new_trace, weight, discarded = model.regenerate(trace, selection, mu, sigma)","text":""},{"location":"reference/core/#genjax.core.GFI.regenerate--weight-accounts-for-probability-change-due-to-regeneration","title":"weight accounts for probability change due to regeneration","text":"<p>pass  # doctest placeholder</p>"},{"location":"reference/core/#genjax.core.GFI.merge","title":"merge  <code>abstractmethod</code>","text":"<pre><code>merge(x: X, x_: X, check: ndarray | None = None) -&gt; tuple[X, X | None]\n</code></pre> <p>Merge two choice maps, with the second taking precedence.</p> <p>Used internally for compositional generative functions where choice maps from different components need to be combined. The merge operation resolves conflicts by preferring choices from x_ over x.</p> Source code in <code>src/genjax/core.py</code> <pre><code>@abstractmethod\ndef merge(\n    self, x: X, x_: X, check: jnp.ndarray | None = None\n) -&gt; tuple[X, X | None]:\n    \"\"\"Merge two choice maps, with the second taking precedence.\n\n    Used internally for compositional generative functions where choice maps\n    from different components need to be combined. The merge operation resolves\n    conflicts by preferring choices from x_ over x.\n\n    Args:\n        x: First choice map.\n        x_: Second choice map (takes precedence in conflicts).\n        check: Optional boolean array for conditional selection.\n               If provided, selects x where True, x_ where False.\n\n    Returns:\n        Tuple of (merged choice map, discarded values).\n        - merged: Combined choices with x_ values overriding x values at conflicts\n        - discarded: Values from x that were overridden by x_ (None if no conflicts)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/core/#genjax.core.GFI.filter","title":"filter  <code>abstractmethod</code>","text":"<pre><code>filter(x: X, selection: Selection) -&gt; tuple[X | None, X | None]\n</code></pre> <p>Filter choice map into selected and unselected parts.</p> <p>Used to partition choices based on a selection, enabling fine-grained manipulation of subsets of choices in inference algorithms. Each GFI implementation specializes this method for its choice type X.</p> Example Source code in <code>src/genjax/core.py</code> <pre><code>@abstractmethod\ndef filter(self, x: X, selection: \"Selection\") -&gt; tuple[X | None, X | None]:\n    \"\"\"Filter choice map into selected and unselected parts.\n\n    Used to partition choices based on a selection, enabling fine-grained manipulation\n    of subsets of choices in inference algorithms. Each GFI implementation specializes\n    this method for its choice type X.\n\n    Args:\n        x: Choice map to filter.\n        selection: Selection specifying which addresses to include.\n\n    Returns:\n        Tuple of (selected_choices, unselected_choices) where:\n        - selected_choices: Choice map containing only selected addresses, or None if no matches\n        - unselected_choices: Choice map containing only unselected addresses, or None if no matches\n        Both have the same structure as X but contain disjoint subsets of addresses.\n\n    Example:\n        &gt;&gt;&gt; # choices = {\"mu\": 1.0, \"sigma\": 2.0, \"obs\": 3.0}\n        &gt;&gt;&gt; # selection = sel(\"mu\") | sel(\"sigma\")\n        &gt;&gt;&gt; # selected, unselected = model.filter(choices, selection)\n        &gt;&gt;&gt; # selected = {\"mu\": 1.0, \"sigma\": 2.0}, unselected = {\"obs\": 3.0}\n        &gt;&gt;&gt; pass  # doctest placeholder\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/core/#genjax.core.GFI.filter--choices","title":"choices =","text":""},{"location":"reference/core/#genjax.core.GFI.filter--selection-selmu-selsigma","title":"selection = sel(\"mu\") | sel(\"sigma\")","text":""},{"location":"reference/core/#genjax.core.GFI.filter--selected-unselected-modelfilterchoices-selection","title":"selected, unselected = model.filter(choices, selection)","text":""},{"location":"reference/core/#genjax.core.GFI.filter--selected-mu-10-sigma-20-unselected-obs-30","title":"selected = {\"mu\": 1.0, \"sigma\": 2.0}, unselected = {\"obs\": 3.0}","text":"<p>pass  # doctest placeholder</p>"},{"location":"reference/core/#genjax.core.Thunk","title":"Thunk","text":"<p>               Bases: <code>Generic[X, R]</code>, <code>Pytree</code></p> <p>Delayed evaluation wrapper for generative functions.</p> <p>A thunk represents a generative function call that has not yet been executed. It captures the function and its arguments for later evaluation.</p>"},{"location":"reference/core/#genjax.core.Vmap","title":"Vmap","text":"<p>               Bases: <code>Generic[X, R]</code>, <code>GFI[X, R]</code></p> <p>A <code>Vmap</code> is a generative function combinator that vectorizes another generative function.</p> <p><code>Vmap</code> applies a generative function across a batch dimension, similar to <code>jax.vmap</code>, but preserves probabilistic semantics. It uses GenJAX's <code>modular_vmap</code> to handle the vectorization of probabilistic computations correctly.</p> <p>Mathematical ingredients: - If callee has measure kernel P_callee(dx; args), then Vmap has kernel   P_vmap(dX; Args) = \u220f_i P_callee(dx_i; args_i) where X = [x_1, ..., x_n] - Return value function f_vmap(X, Args) = [f_callee(x_1, args_1), ..., f_callee(x_n, args_n)] - Internal proposal family inherits from callee's proposal family</p> Example <p>from genjax import normal</p>"},{"location":"reference/core/#genjax.core.Vmap--vectorize-a-normal-distribution","title":"Vectorize a normal distribution","text":"<p>vectorized_normal = normal.vmap(in_axes=(0, None))  # vectorize over first arg</p> <p>mus = jnp.array([0.0, 1.0, 2.0]) sigma = 1.0 trace = vectorized_normal.simulate(mus, sigma) samples = trace.get_choices()  # Array of 3 normal samples</p>"},{"location":"reference/core/#genjax.core.Vmap.filter","title":"filter","text":"<pre><code>filter(x: X, selection: Selection) -&gt; tuple[X | None, X | None]\n</code></pre> <p>Filter vectorized choices using the underlying generative function's filter.</p> <p>For Vmap, choices are vectorized across the batch dimension. We apply the underlying GF's filter to each vectorized choice.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def filter(self, x: X, selection: \"Selection\") -&gt; tuple[X | None, X | None]:\n    \"\"\"Filter vectorized choices using the underlying generative function's filter.\n\n    For Vmap, choices are vectorized across the batch dimension. We apply\n    the underlying GF's filter to each vectorized choice.\n\n    Args:\n        x: Vectorized choice to filter.\n        selection: Selection specifying which addresses to include.\n\n    Returns:\n        Tuple of (selected_choices, unselected_choices) where each is vectorized or None.\n    \"\"\"\n    # Use modular_vmap to apply filter across the batch dimension\n    selected, unselected = modular_vmap(\n        self.gen_fn.filter,\n        in_axes=(0, None),\n        axis_size=self.axis_size.value,\n    )(x, selection)\n\n    return selected, unselected\n</code></pre>"},{"location":"reference/core/#genjax.core.Distribution","title":"Distribution","text":"<p>               Bases: <code>Generic[X]</code>, <code>GFI[X, X]</code></p> <p>A <code>Distribution</code> is a generative function that implements a probability distribution.</p> <p>Distributions are the fundamental building blocks of probabilistic programs. They implement the Generative Function Interface (GFI) by wrapping a sampling function and a log probability density function (logpdf).</p> <p>Mathematical ingredients: - A measure kernel P(dx; args) over a measurable space X given arguments args - Return value function f(x, args) = x (identity function for distributions) - Internal proposal distribution family Q(dx; args, x') = P(dx; args) (prior)</p> Example <p>import jax import jax.numpy as jnp from genjax import Distribution, const</p>"},{"location":"reference/core/#genjax.core.Distribution--create-a-custom-normal-distribution","title":"Create a custom normal distribution","text":"<p>def sample_normal(mu, sigma): ...     key = jax.random.PRNGKey(0)  # In practice, use proper key management ...     return mu + sigma * jax.random.normal(key)</p> <p>def logpdf_normal(x, mu, sigma): ...     return -0.5 * ((x - mu) / sigma)**2 - jnp.log(sigma) - 0.5 * jnp.log(2 * jnp.pi)</p> <p>normal = Distribution(const(sample_normal), const(logpdf_normal), const(\"normal\")) trace = normal.simulate(0.0, 1.0)  # mu=0.0, sigma=1.0</p>"},{"location":"reference/core/#genjax.core.Distribution.sample","title":"sample","text":"<pre><code>sample(*args, **kwargs) -&gt; X\n</code></pre> <p>Sample from the distribution.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def sample(self, *args, **kwargs) -&gt; X:\n    \"\"\"Sample from the distribution.\"\"\"\n    return self._sample.value(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/#genjax.core.Distribution.logpdf","title":"logpdf","text":"<pre><code>logpdf(x: X, *args, **kwargs) -&gt; Weight\n</code></pre> <p>Compute log probability density.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def logpdf(self, x: X, *args, **kwargs) -&gt; Weight:\n    \"\"\"Compute log probability density.\"\"\"\n    return self._logpdf.value(x, *args, **kwargs)\n</code></pre>"},{"location":"reference/core/#genjax.core.Distribution.merge","title":"merge","text":"<pre><code>merge(x: X, x_: X, check: ndarray | None = None) -&gt; tuple[X, X | None]\n</code></pre> <p>Merge distribution choices with optional conditional selection.</p> <p>For distributions, choices are raw values from the sample space. When check is provided, we use jnp.where for conditional selection.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def merge(\n    self, x: X, x_: X, check: jnp.ndarray | None = None\n) -&gt; tuple[X, X | None]:\n    \"\"\"Merge distribution choices with optional conditional selection.\n\n    For distributions, choices are raw values from the sample space.\n    When check is provided, we use jnp.where for conditional selection.\n    \"\"\"\n    if check is not None:\n        # Conditional merge using jnp.where\n        merged = jtu.tree_map(lambda v1, v2: jnp.where(check, v1, v2), x, x_)\n        # No values are truly \"discarded\" in conditional selection\n        return merged, None\n    else:\n        # Without check, Distribution doesn't support merge\n        raise Exception(\n            \"Can't merge: the underlying sample space `X` for the type `Distribution` doesn't support merging without a check parameter.\"\n        )\n</code></pre>"},{"location":"reference/core/#genjax.core.Distribution.filter","title":"filter","text":"<pre><code>filter(x: X, selection: Selection) -&gt; tuple[X | None, X | None]\n</code></pre> <p>Filter choice into selected and unselected parts.</p> <p>For Distribution, the choice is a single value X. Selection either matches the empty address () or it doesn't.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def filter(self, x: X, selection: \"Selection\") -&gt; tuple[X | None, X | None]:\n    \"\"\"Filter choice into selected and unselected parts.\n\n    For Distribution, the choice is a single value X. Selection either\n    matches the empty address () or it doesn't.\n\n    Args:\n        x: Choice value to potentially filter.\n        selection: Selection specifying whether to include the choice.\n\n    Returns:\n        Tuple of (selected_choice, unselected_choice) where exactly one is x and the other is None.\n    \"\"\"\n    is_selected, _ = selection.match(())\n    if is_selected:\n        return x, None\n    else:\n        return None, x\n</code></pre>"},{"location":"reference/core/#genjax.core.Simulate","title":"Simulate  <code>dataclass</code>","text":"<pre><code>Simulate(score: Weight, trace_map: dict[str, Any], parent_fn: GFI = None)\n</code></pre> <p>Handler for simulating generative function executions.</p> <p>Tracks the accumulated score and trace map during simulation.</p>"},{"location":"reference/core/#genjax.core.Fn","title":"Fn","text":"<p>               Bases: <code>Generic[R]</code>, <code>GFI[dict[str, Any], R]</code></p> <p>A <code>Fn</code> is a generative function created from a JAX Python function using the <code>@gen</code> decorator.</p> <p><code>Fn</code> implements the GFI by executing the wrapped function in different execution contexts (handlers) that intercept calls to other generative functions via the <code>@</code> addressing syntax.</p> <p>Mathematical ingredients: - Measure kernel P(dx; args) defined by the composition of distributions in the function - Return value function f(x, args) defined by the function's logic and return statement - Internal proposal distribution family Q(dx; args, x') defined by ancestral sampling</p> <p>The choice space X is a dictionary mapping addresses (strings) to the choices made at those addresses during execution.</p> Example <p>import jax.numpy as jnp from genjax import gen, normal</p> <p>@gen def linear_regression(xs): ...     slope = normal(0.0, 1.0) @ \"slope\" ...     intercept = normal(0.0, 1.0) @ \"intercept\" ...     noise = normal(0.0, 0.1) @ \"noise\" ...     return normal(slope * xs + intercept, noise) @ \"y\"</p> <p>trace = linear_regression.simulate(jnp.array([1.0, 2.0, 3.0])) choices = trace.get_choices()  # dict with keys \"slope\", \"intercept\", \"noise\", \"y\"</p>"},{"location":"reference/core/#genjax.core.Fn.filter","title":"filter","text":"<pre><code>filter(x: dict[str, Any], selection: Selection) -&gt; tuple[dict[str, Any] | None, dict[str, Any] | None]\n</code></pre> <p>Filter choice map into selected and unselected parts.</p> <p>For Fn, choices are stored as dict[str, Any] with string addresses.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def filter(\n    self, x: dict[str, Any], selection: \"Selection\"\n) -&gt; tuple[dict[str, Any] | None, dict[str, Any] | None]:\n    \"\"\"Filter choice map into selected and unselected parts.\n\n    For Fn, choices are stored as dict[str, Any] with string addresses.\n\n    Args:\n        x: Choice dictionary to filter.\n        selection: Selection specifying which addresses to include.\n\n    Returns:\n        Tuple of (selected_choices, unselected_choices) where each is a dict or None.\n    \"\"\"\n    if not x:\n        return None, None\n\n    selected = {}\n    unselected = {}\n    found_selected = False\n    found_unselected = False\n\n    for addr, value in x.items():\n        is_selected, subselection = selection.match(addr)\n        if is_selected:\n            if isinstance(value, dict) and subselection is not None:\n                # Recursively filter nested choices\n                selected_sub, unselected_sub = self.filter(value, subselection)\n                if selected_sub is not None:\n                    selected[addr] = selected_sub\n                    found_selected = True\n                if unselected_sub is not None:\n                    unselected[addr] = unselected_sub\n                    found_unselected = True\n            else:\n                # Include the entire value in selected\n                selected[addr] = value\n                found_selected = True\n        else:\n            # Include the entire value in unselected\n            unselected[addr] = value\n            found_unselected = True\n\n    return (\n        selected if found_selected else None,\n        unselected if found_unselected else None,\n    )\n</code></pre>"},{"location":"reference/core/#genjax.core.ScanTr","title":"ScanTr","text":"<p>               Bases: <code>Generic[X, R]</code>, <code>Trace[X, R]</code></p>"},{"location":"reference/core/#genjax.core.ScanTr.get_fixed_choices","title":"get_fixed_choices","text":"<pre><code>get_fixed_choices() -&gt; X\n</code></pre> <p>Get choices preserving Fixed wrappers.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def get_fixed_choices(self) -&gt; X:\n    \"\"\"Get choices preserving Fixed wrappers.\"\"\"\n    return self.traces.get_fixed_choices()\n</code></pre>"},{"location":"reference/core/#genjax.core.Scan","title":"Scan","text":"<p>               Bases: <code>Generic[X, R]</code>, <code>GFI[X, R]</code></p> <p>A <code>Scan</code> is a generative function combinator that implements sequential iteration.</p> <p><code>Scan</code> repeatedly applies a generative function in a sequential loop, similar to <code>jax.lax.scan</code>, but preserves probabilistic semantics. The callee function should take (carry, x) as input and return (new_carry, output).</p> <p>Mathematical ingredients: - If callee has measure kernel P_callee(dx; carry, x), then Scan has kernel   P_scan(dX; init_carry, xs) = \u220fi P_callee(dx_i; carry_i, xs_i)   where carry = f_callee(x_i, carry_i, xs_i)[0] - Return value function returns (final_carry, [output_1, ..., output_n]) - Internal proposal family inherits from callee's proposal family</p> Example <p>from genjax import gen, normal, Scan, seed, const import jax.numpy as jnp import jax.random as jrand</p> <p>@gen def step(carry, x): ...     noise = normal(0.0, 0.1) @ \"noise\" ...     new_carry = carry + x + noise ...     return new_carry, new_carry  # output equals new carry</p> <p>scan_fn = Scan(step, length=const(3)) init_carry = 0.0 xs = jnp.array([1.0, 2.0, 3.0])</p>"},{"location":"reference/core/#genjax.core.Scan--use-seed-transformation-for-pjax-primitives","title":"Use seed transformation for PJAX primitives","text":"<p>key = jrand.key(0) trace = seed(scan_fn.simulate)(key, init_carry, xs) final_carry, outputs = trace.get_retval() assert len(outputs) == 3  # Should have 3 outputs</p>"},{"location":"reference/core/#genjax.core.Scan.filter","title":"filter","text":"<pre><code>filter(x: X, selection: Selection) -&gt; tuple[X | None, X | None]\n</code></pre> <p>Filter scan choices using the underlying generative function's filter.</p> <p>For Scan, choices are structured according to the scan iterations. We delegate to the underlying callee's filter method.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def filter(self, x: X, selection: \"Selection\") -&gt; tuple[X | None, X | None]:\n    \"\"\"Filter scan choices using the underlying generative function's filter.\n\n    For Scan, choices are structured according to the scan iterations.\n    We delegate to the underlying callee's filter method.\n\n    Args:\n        x: Scan choice structure to filter.\n        selection: Selection specifying which addresses to include.\n\n    Returns:\n        Tuple of (selected_choices, unselected_choices) from the underlying callee.\n    \"\"\"\n    return self.callee.filter(x, selection)\n</code></pre>"},{"location":"reference/core/#genjax.core.CondTr","title":"CondTr","text":"<p>               Bases: <code>Generic[X, R]</code>, <code>Trace[X, R]</code></p>"},{"location":"reference/core/#genjax.core.CondTr.get_fixed_choices","title":"get_fixed_choices","text":"<pre><code>get_fixed_choices() -&gt; X\n</code></pre> <p>Get choices preserving Fixed wrappers.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def get_fixed_choices(self) -&gt; X:\n    \"\"\"Get choices preserving Fixed wrappers.\"\"\"\n    chm, chm_ = map(lambda tr: tr.get_fixed_choices(), self.trs)\n\n    # Use merge with check parameter for conditional selection\n    merged, _ = self.gen_fn.merge(chm, chm_, self.check)\n    return merged\n</code></pre>"},{"location":"reference/core/#genjax.core.Cond","title":"Cond","text":"<p>               Bases: <code>Generic[X, R]</code>, <code>GFI[X, R]</code></p> <p>A <code>Cond</code> is a generative function combinator that implements conditional branching.</p> <p><code>Cond</code> takes a boolean condition and executes one of two generative functions based on the condition, similar to <code>jax.lax.cond</code>, but preserves probabilistic semantics by evaluating both branches and selecting the appropriate one.</p> <p>Mathematical ingredients: - If branches have measure kernels P_true(dx; args) and P_false(dx; args), then   Cond has kernel P_cond(dx; check, args) = P_true(dx; args) if check else P_false(dx; args) - Return value function f_cond(x, check, args) = f_true(x, args) if check else f_false(x, args) - Internal proposal family selects appropriate branch proposal based on condition</p> <p>Note: Both branches are always evaluated during simulation/generation to maintain JAX compatibility, but only the appropriate branch contributes to the final result.</p> Example <p>from genjax import gen, normal, exponential, Cond</p> <p>@gen def positive_branch(): ...     return exponential(1.0) @ \"value\"</p> <p>@gen def negative_branch(): ...     return exponential(2.0) @ \"value\"</p> <p>cond_fn = Cond(positive_branch, negative_branch)</p>"},{"location":"reference/core/#genjax.core.Cond--use-in-a-larger-model","title":"Use in a larger model","text":"<p>@gen def conditional_model(): ...     x = normal(0.0, 1.0) @ \"x\" ...     condition = x &gt; 0 ...     result = cond_fn((condition,)) @ \"conditional\" ...     return result</p>"},{"location":"reference/core/#genjax.core.Cond.filter","title":"filter","text":"<pre><code>filter(x: X, selection: Selection) -&gt; tuple[X | None, X | None]\n</code></pre> <p>Filter conditional choices using the underlying generative function's filter.</p> <p>For Cond, choices are determined by which branch was executed. We delegate to the first callee's filter method.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def filter(self, x: X, selection: \"Selection\") -&gt; tuple[X | None, X | None]:\n    \"\"\"Filter conditional choices using the underlying generative function's filter.\n\n    For Cond, choices are determined by which branch was executed.\n    We delegate to the first callee's filter method.\n\n    Args:\n        x: Conditional choice structure to filter.\n        selection: Selection specifying which addresses to include.\n\n    Returns:\n        Tuple of (selected_choices, unselected_choices) from the underlying callee.\n    \"\"\"\n    return self.callee.filter(x, selection)\n</code></pre>"},{"location":"reference/core/#genjax.core.const","title":"const","text":"<pre><code>const(a: A) -&gt; Const[A]\n</code></pre> <p>Create a Const wrapper for a static value.</p> Example <pre><code>from genjax import const, Const\n\n# Create a static value\nlength = const(10)\nprint(f\"Value: {length.value}\")\nprint(f\"Type: {type(length)}\")\n\n# Use in arithmetic\ndoubled = length * 2\nprint(f\"Doubled: {doubled.value}\")\n\n# Use as static parameter\n# @gen\n# def model(n: Const[int]):\n#     # n.value is guaranteed to be Python int, not JAX tracer\n#     for i in range(n.value):  # This works in JAX transforms!\n#         ...\n</code></pre> Source code in <code>src/genjax/core.py</code> <pre><code>def const(a: A) -&gt; Const[A]:\n    \"\"\"Create a Const wrapper for a static value.\n\n    Args:\n        a: The Python literal to wrap as static.\n\n    Returns:\n        A Const wrapper that keeps the value static in JAX transformations.\n\n    Example:\n        ```python\n        from genjax import const, Const\n\n        # Create a static value\n        length = const(10)\n        print(f\"Value: {length.value}\")\n        print(f\"Type: {type(length)}\")\n\n        # Use in arithmetic\n        doubled = length * 2\n        print(f\"Doubled: {doubled.value}\")\n\n        # Use as static parameter\n        # @gen\n        # def model(n: Const[int]):\n        #     # n.value is guaranteed to be Python int, not JAX tracer\n        #     for i in range(n.value):  # This works in JAX transforms!\n        #         ...\n        ```\n    \"\"\"\n    return Const(a)\n</code></pre>"},{"location":"reference/core/#genjax.core.fixed","title":"fixed","text":"<pre><code>fixed(a: A) -&gt; Fixed[A]\n</code></pre> <p>Create a Fixed wrapper for a constrained value.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def fixed(a: A) -&gt; Fixed[A]:\n    \"\"\"Create a Fixed wrapper for a constrained value.\n\n    Args:\n        a: The value that was provided/constrained externally.\n\n    Returns:\n        A Fixed wrapper indicating the value was not proposed internally.\n    \"\"\"\n    return Fixed(a)\n</code></pre>"},{"location":"reference/core/#genjax.core.get_choices","title":"get_choices","text":"<pre><code>get_choices(x: Trace[X, R] | X) -&gt; X\n</code></pre> <p>Extract choices from a trace or nested structure containing traces.</p> <p>Also strips Fixed wrappers from the choices, returning the unwrapped values. Fixed wrappers are used internally to track constrained vs. proposed values.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def get_choices(x: Trace[X, R] | X) -&gt; X:\n    \"\"\"Extract choices from a trace or nested structure containing traces.\n\n    Also strips Fixed wrappers from the choices, returning the unwrapped values.\n    Fixed wrappers are used internally to track constrained vs. proposed values.\n\n    Args:\n        x: A trace object or nested structure that may contain traces.\n\n    Returns:\n        The random choices, with any nested traces recursively unwrapped and\n        Fixed wrappers stripped.\n    \"\"\"\n    x = x.get_choices() if isinstance(x, Trace) else x\n\n    def _get_choices(x):\n        if isinstance(x, Trace):\n            return get_choices(x)\n        else:\n            return x\n\n    # First unwrap any nested traces\n    x = jtu.tree_map(\n        _get_choices,\n        x,\n        is_leaf=lambda x: isinstance(x, Trace),\n    )\n\n    # Then strip Fixed wrappers\n    def _strip_fixed(x):\n        if isinstance(x, Fixed):\n            return x.value  # Unwrap Fixed wrapper\n        else:\n            return x\n\n    return jtu.tree_map(\n        _strip_fixed,\n        x,\n        is_leaf=lambda x: isinstance(x, Fixed),\n    )\n</code></pre>"},{"location":"reference/core/#genjax.core.get_fixed_choices","title":"get_fixed_choices","text":"<pre><code>get_fixed_choices(x: Trace[X, R] | X) -&gt; X\n</code></pre> <p>Extract choices from a trace or nested structure containing traces, preserving Fixed wrappers.</p> <p>Similar to get_choices() but preserves Fixed wrappers around the choices, which is needed for verification that values were constrained during inference.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def get_fixed_choices(x: Trace[X, R] | X) -&gt; X:\n    \"\"\"Extract choices from a trace or nested structure containing traces, preserving Fixed wrappers.\n\n    Similar to get_choices() but preserves Fixed wrappers around the choices,\n    which is needed for verification that values were constrained during inference.\n\n    Args:\n        x: A trace object or nested structure that may contain traces.\n\n    Returns:\n        The random choices, with any nested traces recursively unwrapped but\n        Fixed wrappers preserved.\n    \"\"\"\n    x = x.get_fixed_choices() if isinstance(x, Trace) else x\n\n    def _get_fixed_choices(x):\n        if isinstance(x, Trace):\n            return get_fixed_choices(x)\n        else:\n            return x\n\n    # Unwrap any nested traces but preserve Fixed wrappers\n    # Note: Unlike get_choices(), we do NOT strip Fixed wrappers\n    return jtu.tree_map(\n        _get_fixed_choices,\n        x,\n        is_leaf=lambda x: isinstance(x, Trace),\n    )\n</code></pre>"},{"location":"reference/core/#genjax.core.get_score","title":"get_score","text":"<pre><code>get_score(x: Trace[X, R]) -&gt; Weight\n</code></pre> <p>Extract the log probability score from a trace.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def get_score(x: Trace[X, R]) -&gt; Weight:\n    \"\"\"Extract the log probability score from a trace.\n\n    Args:\n        x: Trace object to extract score from.\n\n    Returns:\n        The log probability score of the trace.\n    \"\"\"\n    return x.get_score()\n</code></pre>"},{"location":"reference/core/#genjax.core.get_retval","title":"get_retval","text":"<pre><code>get_retval(x: Trace[X, R]) -&gt; R\n</code></pre> <p>Extract the return value from a trace.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def get_retval(x: Trace[X, R]) -&gt; R:\n    \"\"\"Extract the return value from a trace.\n\n    Args:\n        x: Trace object to extract return value from.\n\n    Returns:\n        The return value of the trace.\n    \"\"\"\n    return x.get_retval()\n</code></pre>"},{"location":"reference/core/#genjax.core.sel","title":"sel","text":"<pre><code>sel(*v: tuple[] | str | tuple[str, ...] | dict[str, Any] | None) -&gt; Selection\n</code></pre> <p>Create a Selection from various input types.</p> <p>This is a convenience function to create Selection objects from common patterns. Selections specify which random choices in a trace should be regenerated during inference operations like MCMC.</p> <p>Examples:</p> <pre><code>from genjax import sel\n\n# Select specific address\ns1 = sel(\"x\")\nprint(f\"sel('x'): {s1}\")\n\n# Select hierarchical address\ns2 = sel((\"outer\", \"inner\"))\nprint(f\"sel(('outer', 'inner')): {s2}\")\n\n# Select all addresses\ns3 = sel(())\nprint(f\"sel(()): {s3}\")\n\n# Select no addresses\ns4 = sel()\nprint(f\"sel(): {s4}\")\n\n# Combine selections with OR\ns5 = sel(\"x\") | sel(\"y\")\nprint(f\"sel('x') | sel('y'): {s5}\")\n\n# Complement selection\ns6 = ~sel(\"x\")\nprint(f\"~sel('x'): {s6}\")\n</code></pre> Source code in <code>src/genjax/core.py</code> <pre><code>def sel(*v: tuple[()] | str | tuple[str, ...] | dict[str, Any] | None) -&gt; Selection:\n    \"\"\"Create a Selection from various input types.\n\n    This is a convenience function to create Selection objects from common patterns.\n    Selections specify which random choices in a trace should be regenerated during\n    inference operations like MCMC.\n\n    Args:\n        *v: Variable arguments specifying the selection pattern:\n            - str: Select a specific address (e.g., sel(\"x\"))\n            - tuple[str, ...]: Select hierarchical address (e.g., sel((\"outer\", \"inner\")))\n            - (): Select all addresses (e.g., sel(()))\n            - dict: Select nested addresses (e.g., sel({\"outer\": sel(\"inner\")}))\n            - None or no args: Select no addresses (e.g., sel() or sel(None))\n\n    Returns:\n        Selection object that can be used with regenerate methods\n\n    Examples:\n        ```python\n        from genjax import sel\n\n        # Select specific address\n        s1 = sel(\"x\")\n        print(f\"sel('x'): {s1}\")\n\n        # Select hierarchical address\n        s2 = sel((\"outer\", \"inner\"))\n        print(f\"sel(('outer', 'inner')): {s2}\")\n\n        # Select all addresses\n        s3 = sel(())\n        print(f\"sel(()): {s3}\")\n\n        # Select no addresses\n        s4 = sel()\n        print(f\"sel(): {s4}\")\n\n        # Combine selections with OR\n        s5 = sel(\"x\") | sel(\"y\")\n        print(f\"sel('x') | sel('y'): {s5}\")\n\n        # Complement selection\n        s6 = ~sel(\"x\")\n        print(f\"~sel('x'): {s6}\")\n        ```\n    \"\"\"\n    assert len(v) &lt;= 1\n    if len(v) == 1:\n        if v[0] is None:\n            return Selection(NoneSel())\n        if v[0] == ():\n            return Selection(AllSel())\n        elif isinstance(v[0], dict):\n            return Selection(DictSel(v[0]))\n        elif isinstance(v[0], tuple) and all(isinstance(s, str) for s in v[0]):\n            # Tuple of strings for hierarchical addresses\n            return Selection(TupleSel(const(v[0])))\n        else:\n            assert isinstance(v[0], str)\n            return Selection(StrSel(const(v[0])))\n    else:\n        return Selection(NoneSel())\n</code></pre>"},{"location":"reference/core/#genjax.core.distribution","title":"distribution","text":"<pre><code>distribution(sampler: Callable[..., Any], logpdf: Callable[..., Any], /, name: str | None = None) -&gt; Distribution[Any]\n</code></pre> <p>Create a Distribution from sampling and log probability functions.</p> Source code in <code>src/genjax/core.py</code> <pre><code>def distribution(\n    sampler: Callable[..., Any],\n    logpdf: Callable[..., Any],\n    /,\n    name: str | None = None,\n) -&gt; Distribution[Any]:\n    \"\"\"Create a Distribution from sampling and log probability functions.\n\n    Args:\n        sampler: Function that takes parameters and returns a sample.\n        logpdf: Function that takes (value, *parameters) and returns log probability.\n        name: Optional name for the distribution.\n\n    Returns:\n        A Distribution instance implementing the Generative Function Interface.\n    \"\"\"\n    return Distribution(\n        _sample=const(sampler),\n        _logpdf=const(logpdf),\n        name=const(name),\n    )\n</code></pre>"},{"location":"reference/core/#genjax.core.tfp_distribution","title":"tfp_distribution","text":"<pre><code>tfp_distribution(dist: Callable[..., Distribution], /, name: str | None = None) -&gt; Distribution[Any]\n</code></pre> <p>Create a Distribution from a TensorFlow Probability distribution.</p> <p>Wraps a TFP distribution constructor to create a GenJAX Distribution that properly handles PJAX's <code>sample_p</code> primitive.</p> Example <p>import tensorflow_probability.substrates.jax as tfp from genjax import tfp_distribution</p> Source code in <code>src/genjax/core.py</code> <pre><code>def tfp_distribution(\n    dist: Callable[..., \"tfd.Distribution\"],\n    /,\n    name: str | None = None,\n) -&gt; Distribution[Any]:\n    \"\"\"Create a Distribution from a TensorFlow Probability distribution.\n\n    Wraps a TFP distribution constructor to create a GenJAX Distribution\n    that properly handles PJAX's `sample_p` primitive.\n\n    Args:\n        dist: TFP distribution constructor function.\n        name: Optional name for the distribution.\n\n    Returns:\n        A Distribution that wraps the TFP distribution.\n\n    Example:\n        &gt;&gt;&gt; import tensorflow_probability.substrates.jax as tfp\n        &gt;&gt;&gt; from genjax import tfp_distribution\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create a normal distribution from TFP\n        &gt;&gt;&gt; normal = tfp_distribution(tfp.distributions.Normal, name=\"normal\")\n    \"\"\"\n\n    def keyful_sampler(key, *args, sample_shape=(), **kwargs):\n        d = dist(*args, **kwargs)\n        return d.sample(seed=key, sample_shape=sample_shape)\n\n    def logpdf(v, *args, **kwargs):\n        d = dist(*args, **kwargs)\n        return d.log_prob(v)\n\n    return distribution(\n        wrap_sampler(\n            keyful_sampler,\n            name=name,\n        ),\n        wrap_logpdf(logpdf),\n        name=name,\n    )\n</code></pre>"},{"location":"reference/core/#genjax.core.tfp_distribution--create-a-normal-distribution-from-tfp","title":"Create a normal distribution from TFP","text":"<p>normal = tfp_distribution(tfp.distributions.Normal, name=\"normal\")</p>"},{"location":"reference/core/#genjax.core.gen","title":"gen","text":"<pre><code>gen(fn: Callable[..., R]) -&gt; Fn[R]\n</code></pre> <p>Convert a function into a generative function.</p> <p>The decorated function can use the <code>@</code> operator to make addressed random choices from distributions and other generative functions.</p> Example <p>from genjax import gen, normal</p> <p>@gen ... def model(mu, sigma): ...     x = normal(mu, sigma) @ \"x\" ...     y = normal(x, 0.1) @ \"y\" ...     return x + y</p> <p>trace = model.simulate(0.0, 1.0) choices = trace.get_choices()</p> Source code in <code>src/genjax/core.py</code> <pre><code>def gen(fn: Callable[..., R]) -&gt; Fn[R]:\n    \"\"\"Convert a function into a generative function.\n\n    The decorated function can use the `@` operator to make addressed\n    random choices from distributions and other generative functions.\n\n    Args:\n        fn: Function to convert into a generative function.\n\n    Returns:\n        A Fn instance that implements the Generative Function Interface.\n\n    Example:\n        &gt;&gt;&gt; from genjax import gen, normal\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @gen\n        ... def model(mu, sigma):\n        ...     x = normal(mu, sigma) @ \"x\"\n        ...     y = normal(x, 0.1) @ \"y\"\n        ...     return x + y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; trace = model.simulate(0.0, 1.0)\n        &gt;&gt;&gt; choices = trace.get_choices()\n        &gt;&gt;&gt; # choices will contain {\"x\": &lt;value&gt;, \"y\": &lt;value&gt;}\n    \"\"\"\n    gf = Fn(source=const(fn))\n    # Copy function metadata to preserve name and module information\n    try:\n        gf.__name__ = fn.__name__\n        gf.__qualname__ = fn.__qualname__\n        gf.__module__ = fn.__module__\n        gf.__doc__ = fn.__doc__\n        gf.__annotations__ = getattr(fn, \"__annotations__\", {})\n    except (AttributeError, TypeError):\n        # If we can't set these attributes (e.g., on frozen dataclasses), continue anyway\n        pass\n    return gf\n</code></pre>"},{"location":"reference/core/#genjax.core.gen--choices-will-contain","title":"choices will contain","text":""},{"location":"reference/core/#live-examples","title":"Live Examples","text":""},{"location":"reference/core/#basic-model-definition","title":"Basic Model Definition","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom genjax import gen, distributions\n\n@gen\ndef coin_flip_model(n_flips):\n    \"\"\"A simple coin flipping model with unknown bias.\"\"\"\n    bias = distributions.beta(1.0, 1.0) @ \"bias\"\n\n    # For demonstration, we'll show manual unrolling\n    # In practice, use Scan combinator for loops\n    flip_0 = distributions.bernoulli(bias) @ \"flip_0\"\n    flip_1 = distributions.bernoulli(bias) @ \"flip_1\" \n    flip_2 = distributions.bernoulli(bias) @ \"flip_2\"\n\n    return jnp.array([flip_0, flip_1, flip_2])\n\nprint(\"Model defined successfully!\")\n</code></pre> <p>Model defined successfully!</p>"},{"location":"reference/core/#assessing-log-probability","title":"Assessing Log Probability","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom genjax import gen, distributions\n\n@gen\ndef coin_flip_model(n_flips):\n    \"\"\"A simple coin flipping model with unknown bias.\"\"\"\n    bias = distributions.beta(1.0, 1.0) @ \"bias\"\n\n    # For demonstration, we'll show manual unrolling\n    # In practice, use Scan combinator for loops\n    flip_0 = distributions.bernoulli(bias) @ \"flip_0\"\n    flip_1 = distributions.bernoulli(bias) @ \"flip_1\" \n    flip_2 = distributions.bernoulli(bias) @ \"flip_2\"\n\n    return jnp.array([flip_0, flip_1, flip_2])\n\n# Assess the log probability of specific choices\nchoices = {\"bias\": 0.7, \"flip_0\": 1, \"flip_1\": 1, \"flip_2\": 0}\nlog_prob, retval = coin_flip_model.assess(choices, 3)\n\nprint(f\"Given choices: {choices}\")\nprint(f\"Log probability: {log_prob:.3f}\")\nprint(f\"Return value (flips): {retval}\")\n</code></pre> <p>Given choices: {'bias': 0.7, 'flip_0': 1, 'flip_1': 1, 'flip_2': 0} Log probability: -1.910 Return value (flips): [1 1 0]</p>"},{"location":"reference/core/#using-selections","title":"Using Selections","text":"<pre><code>from genjax import sel, Selection\n\n# Create various selections\ns1 = sel(\"bias\")  # Select only bias\ns2 = sel(\"flip_0\") | sel(\"flip_1\")  # Select two flips with OR\ns3 = sel(\"bias\") | sel(\"flip_2\")  # Select bias OR flip_2\n\nprint(f\"Selection s1 targets: bias\")\nprint(f\"Selection s2 targets: flip_0 or flip_1\")\nprint(f\"Selection s3 targets: bias or flip_2\")\n</code></pre> <p>Selection s1 targets: bias Selection s2 targets: flip_0 or flip_1 Selection s3 targets: bias or flip_2</p>"},{"location":"reference/distributions/","title":"genjax.distributions","text":"<p>Built-in probability distributions that implement the Generative Function Interface.</p>"},{"location":"reference/distributions/#genjax.distributions","title":"distributions","text":"<p>Standard probability distributions for GenJAX.</p> <p>This module provides a collection of common probability distributions wrapped as GenJAX Distribution objects. All distributions are built using TensorFlow Probability as the backend.</p>"},{"location":"reference/distributions/#genjax.distributions.bernoulli","title":"bernoulli  <code>module-attribute</code>","text":"<pre><code>bernoulli = tfp_distribution(Bernoulli, name='Bernoulli')\n</code></pre> <p>Bernoulli distribution for binary outcomes.</p> Mathematical Formulation <p>PMF: P(X = k) = p^k \u00d7 (1-p)^(1-k) for k \u2208 {0, 1}</p> <p>Where p is the probability of success.</p> <p>Mean: \ud835\udd3c[X] = p Variance: Var[X] = p(1-p) Support: {0, 1}</p> Parameterization <p>Can be specified via: - probs: p \u2208 [0, 1] (probability of success) - logits: log(p/(1-p)) \u2208 \u211d (log-odds)</p> References <p>.. [1] Johnson, N. L., Kotz, S., &amp; Kemp, A. W. (1992). \"Univariate        Discrete Distributions\". Wiley, Chapter 3.</p>"},{"location":"reference/distributions/#genjax.distributions.flip","title":"flip  <code>module-attribute</code>","text":"<pre><code>flip = tfp_distribution(lambda p: Bernoulli(probs=p, dtype=bool_), name='Flip')\n</code></pre> <p>Flip distribution (Bernoulli with boolean output).</p>"},{"location":"reference/distributions/#genjax.distributions.beta","title":"beta  <code>module-attribute</code>","text":"<pre><code>beta = tfp_distribution(Beta, name='Beta')\n</code></pre> <p>Beta distribution on the interval [0, 1].</p> Mathematical Formulation <p>PDF: f(x; \u03b1, \u03b2) = \u0393(\u03b1+\u03b2)/(\u0393(\u03b1)\u0393(\u03b2)) \u00d7 x^(\u03b1-1) \u00d7 (1-x)^(\u03b2-1)</p> <p>Where \u0393 is the gamma function, \u03b1 &gt; 0, \u03b2 &gt; 0.</p> <p>Mean: \ud835\udd3c[X] = \u03b1/(\u03b1+\u03b2) Variance: Var[X] = \u03b1\u03b2/((\u03b1+\u03b2)\u00b2(\u03b1+\u03b2+1)) Mode: (\u03b1-1)/(\u03b1+\u03b2-2) for \u03b1,\u03b2 &gt; 1 Support: [0, 1]</p> Special Cases <ul> <li>Beta(1, 1) = Uniform(0, 1)</li> <li>Beta(\u03b1, \u03b1) is symmetric about 0.5</li> <li>As \u03b1,\u03b2 \u2192 \u221e with \u03b1/(\u03b1+\u03b2) fixed, approaches Normal</li> </ul> References <p>.. [1] Gupta, A. K., &amp; Nadarajah, S. (2004). \"Handbook of Beta        Distribution and Its Applications\". CRC Press.</p>"},{"location":"reference/distributions/#genjax.distributions.categorical","title":"categorical  <code>module-attribute</code>","text":"<pre><code>categorical = tfp_distribution(lambda logits: Categorical(logits), name='Categorical')\n</code></pre> <p>Categorical distribution over discrete outcomes.</p> Mathematical Formulation <p>PMF: P(X = k) = p_k for k \u2208 {0, 1, ..., K-1}</p> <p>Where \u2211_k p_k = 1 and p_k \u2265 0.</p> <p>Mean: \ud835\udd3c[X] = \u2211_k k \u00d7 p_k Variance: Var[X] = \u2211_k k\u00b2 \u00d7 p_k - (\ud835\udd3c[X])\u00b2 Entropy: H[X] = -\u2211_k p_k log(p_k) Support: {0, 1, ..., K-1}</p> Parameterization <ul> <li>logits: \u03b8_k \u2208 \u211d, where p_k = exp(\u03b8_k) / \u2211_j exp(\u03b8_j)</li> <li>Softmax transformation ensures valid probabilities</li> </ul> Connection to Other Distributions <ul> <li>K=2: Equivalent to Bernoulli</li> <li>Generalization of multinomial for single trial</li> </ul> References <p>.. [1] Bishop, C. M. (2006). \"Pattern Recognition and Machine Learning\".        Springer, Section 2.2.</p>"},{"location":"reference/distributions/#genjax.distributions.geometric","title":"geometric  <code>module-attribute</code>","text":"<pre><code>geometric = tfp_distribution(Geometric, name='Geometric')\n</code></pre> <p>Geometric distribution (number of trials until first success).</p> Mathematical Formulation <p>PMF: P(X = k) = (1-p)^(k-1) \u00d7 p for k \u2208 {1, 2, 3, ...}</p> <p>Where p \u2208 (0, 1] is the probability of success.</p> <p>Mean: \ud835\udd3c[X] = 1/p Variance: Var[X] = (1-p)/p\u00b2 CDF: F(k) = 1 - (1-p)^k Support: {1, 2, 3, ...}</p> Memoryless Property <p>P(X &gt; m + n | X &gt; m) = P(X &gt; n)</p> <p>The only discrete distribution with this property.</p> Alternative Parameterization <p>Some define X as failures before first success: P(X = k) = (1-p)^k \u00d7 p for k \u2208 {0, 1, 2, ...}</p> References <p>.. [1] Johnson, N. L., Kotz, S., &amp; Kemp, A. W. (1992). \"Univariate        Discrete Distributions\". Wiley, Chapter 5.</p>"},{"location":"reference/distributions/#genjax.distributions.normal","title":"normal  <code>module-attribute</code>","text":"<pre><code>normal = tfp_distribution(Normal, name='Normal')\n</code></pre> <p>Normal (Gaussian) distribution.</p> Mathematical Formulation <p>PDF: f(x; \u03bc, \u03c3) = (1/\u221a(2\u03c0\u03c3\u00b2)) \u00d7 exp(-(x-\u03bc)\u00b2/(2\u03c3\u00b2))</p> <p>Where \u03bc \u2208 \u211d is the mean, \u03c3 &gt; 0 is the standard deviation.</p> <p>Mean: \ud835\udd3c[X] = \u03bc Variance: Var[X] = \u03c3\u00b2 MGF: M(t) = exp(\u03bct + \u03c3\u00b2t\u00b2/2) Support: \u211d</p> Standard Normal <p>Z = (X - \u03bc)/\u03c3 ~ N(0, 1)</p> <p>\u03a6(z) = P(Z \u2264 z) = \u222b_{-\u221e}^z (1/\u221a(2\u03c0)) exp(-t\u00b2/2) dt</p> Properties <ul> <li>Maximum entropy distribution for fixed mean and variance</li> <li>Stable under convolution: X\u2081 + X\u2082 ~ N(\u03bc\u2081+\u03bc\u2082, \u03c3\u2081\u00b2+\u03c3\u2082\u00b2)</li> <li>Central Limit Theorem: Sample means converge to Normal</li> </ul> Example <pre><code>import jax\nimport jax.numpy as jnp\nfrom genjax import distributions\n\n# Sample from normal distribution  \ntrace = distributions.normal.simulate(0.0, 1.0)\nsample = trace.get_retval()\nprint(f\"Sample from Normal(0, 1): {sample:.3f}\")\n\n# Evaluate log probability\nlog_prob, _ = distributions.normal.assess(1.5, 0.0, 1.0)\nprint(f\"Log prob of 1.5 under Normal(0, 1): {log_prob:.3f}\")\n\n# Use in a generative function\nfrom genjax import gen\n\n@gen\ndef model():\n    x = distributions.normal(0.0, 1.0) @ \"x\"\n    y = distributions.normal(x, 0.1) @ \"y\"\n    return x + y\n\n# Simulate the model\ntrace = model.simulate()\nprint(f\"Model output: {trace.get_retval():.3f}\")\nprint(f\"Choices: x={trace.get_choices()['x']:.3f}, y={trace.get_choices()['y']:.3f}\")\n</code></pre> References <p>.. [1] Patel, J. K., &amp; Read, C. B. (1996). \"Handbook of the Normal        Distribution\". Marcel Dekker, 2nd edition.</p>"},{"location":"reference/distributions/#genjax.distributions.uniform","title":"uniform  <code>module-attribute</code>","text":"<pre><code>uniform = tfp_distribution(Uniform, name='Uniform')\n</code></pre> <p>Uniform distribution on an interval.</p> Mathematical Formulation <p>PDF: f(x; a, b) = 1/(b-a) for x \u2208 [a, b], 0 otherwise</p> <p>Where a &lt; b define the support interval.</p> <p>Mean: \ud835\udd3c[X] = (a + b)/2 Variance: Var[X] = (b - a)\u00b2/12 CDF: F(x) = (x - a)/(b - a) for x \u2208 [a, b] Support: [a, b]</p> Properties <ul> <li>Maximum entropy distribution on bounded interval</li> <li>All moments exist: \ud835\udd3c[X^n] = (b^(n+1) - a^(n+1))/((n+1)(b-a))</li> <li>Order statistics have Beta distributions</li> </ul> Connection to Other Distributions <ul> <li>Standard uniform U(0,1) generates other distributions</li> <li>-log(U) ~ Exponential(1)</li> <li>U^(1/\u03b1) ~ Power distribution</li> </ul> References <p>.. [1] Johnson, N. L., Kotz, S., &amp; Balakrishnan, N. (1995).        \"Continuous Univariate Distributions\". Wiley, Vol. 2, Chapter 26.</p>"},{"location":"reference/distributions/#genjax.distributions.exponential","title":"exponential  <code>module-attribute</code>","text":"<pre><code>exponential = tfp_distribution(Exponential, name='Exponential')\n</code></pre> <p>Exponential distribution for positive continuous values.</p> Mathematical Formulation <p>PDF: f(x; \u03bb) = \u03bb exp(-\u03bbx) for x \u2265 0</p> <p>Where \u03bb &gt; 0 is the rate parameter.</p> <p>Mean: \ud835\udd3c[X] = 1/\u03bb Variance: Var[X] = 1/\u03bb\u00b2 CDF: F(x) = 1 - exp(-\u03bbx) Support: [0, \u221e)</p> Memoryless Property <p>P(X &gt; s + t | X &gt; s) = P(X &gt; t)</p> <p>The only continuous distribution with this property.</p> Connection to Other Distributions <ul> <li>Special case of Gamma(1, \u03bb)</li> <li>-log(U) ~ Exponential(1) where U ~ Uniform(0,1)</li> <li>Minimum of n Exponential(\u03bb) ~ Exponential(n\u03bb)</li> <li>Sum of n Exponential(\u03bb) ~ Gamma(n, \u03bb)</li> </ul>"},{"location":"reference/distributions/#genjax.distributions.poisson","title":"poisson  <code>module-attribute</code>","text":"<pre><code>poisson = tfp_distribution(Poisson, name='Poisson')\n</code></pre> <p>Poisson distribution for count data.</p> Mathematical Formulation <p>PMF: P(X = k) = (\u03bb^k / k!) \u00d7 exp(-\u03bb) for k \u2208 {0, 1, 2, ...}</p> <p>Where \u03bb &gt; 0 is the rate parameter (expected count).</p> <p>Mean: \ud835\udd3c[X] = \u03bb Variance: Var[X] = \u03bb MGF: M(t) = exp(\u03bb(e^t - 1)) Support: {0, 1, 2, ...}</p> Properties <ul> <li>Mean equals variance (equidispersion)</li> <li>Sum of Poissons: X\u2081 ~ Pois(\u03bb\u2081), X\u2082 ~ Pois(\u03bb\u2082) \u21d2 X\u2081+X\u2082 ~ Pois(\u03bb\u2081+\u03bb\u2082)</li> <li>Limit of Binomial: Bin(n,p) \u2192 Pois(np) as n\u2192\u221e, p\u21920, np=\u03bb</li> </ul> Connection to Other Distributions <ul> <li>Poisson process: Inter-arrival times ~ Exponential(\u03bb)</li> <li>Large \u03bb: Approximately Normal(\u03bb, \u03bb)</li> <li>Conditional on rate: If \u03bb ~ Gamma(\u03b1,\u03b2), then X ~ NegBin(\u03b1, \u03b2/(1+\u03b2))</li> </ul> References <p>.. [1] Johnson, N. L., Kotz, S., &amp; Kemp, A. W. (1992). \"Univariate        Discrete Distributions\". Wiley, Chapter 4. .. [2] Haight, F. A. (1967). \"Handbook of the Poisson Distribution\".        Wiley.</p>"},{"location":"reference/distributions/#genjax.distributions.multivariate_normal","title":"multivariate_normal  <code>module-attribute</code>","text":"<pre><code>multivariate_normal = tfp_distribution(MultivariateNormalFullCovariance, name='MultivariateNormal')\n</code></pre> <p>Multivariate normal distribution.</p> Mathematical Formulation <p>PDF: f(x; \u03bc, \u03a3) = (2\u03c0)^(-k/2) |det(\u03a3)|^(-1/2) exp(-\u00bd(x-\u03bc)^T \u03a3^(-1) (x-\u03bc))</p> <p>Where \u03bc \u2208 \u211d^k is the mean vector, \u03a3 is k\u00d7k positive definite covariance.</p> <p>Mean: \ud835\udd3c[X] = \u03bc Covariance: Cov[X] = \u03a3 MGF: M(t) = exp(t^T\u03bc + \u00bdt^T\u03a3t) Support: \u211d^k</p> Properties <ul> <li>Linear transformations: If Y = AX + b, then Y ~ N(A\u03bc + b, A\u03a3A^T)</li> <li>Marginals are Normal: X_i ~ N(\u03bc_i, \u03a3_{ii})</li> <li>Conditional distributions are Normal with closed-form parameters</li> <li>Maximum entropy for fixed mean and covariance</li> </ul> Special Cases <ul> <li>\u03a3 = \u03c3\u00b2I: Spherical/isotropic Gaussian</li> <li>\u03a3 diagonal: Independent components</li> <li>k = 1: Univariate normal</li> </ul> References <p>.. [1] Mardia, K. V., Kent, J. T., &amp; Bibby, J. M. (1979). \"Multivariate        Analysis\". Academic Press, Chapter 3. .. [2] Tong, Y. L. (1990). \"The Multivariate Normal Distribution\".        Springer-Verlag.</p>"},{"location":"reference/distributions/#genjax.distributions.dirichlet","title":"dirichlet  <code>module-attribute</code>","text":"<pre><code>dirichlet = tfp_distribution(Dirichlet, name='Dirichlet')\n</code></pre> <p>Dirichlet distribution for probability vectors.</p> Mathematical Formulation <p>PDF: f(x; \u03b1) = [\u0393(\u2211\u1d62\u03b1\u1d62)/\u220f\u1d62\u0393(\u03b1\u1d62)] \u00d7 \u220f\u1d62 x\u1d62^(\u03b1\u1d62-1)</p> <p>Where x \u2208 \u03b4_{k-1} (probability simplex), \u03b1\u1d62 &gt; 0 are concentrations.</p> <p>Mean: \ud835\udd3c[X\u1d62] = \u03b1\u1d62 / \u2211\u2c7c\u03b1\u2c7c Variance: Var[X\u1d62] = [\u03b1\u1d62(\u03b1\u2080-\u03b1\u1d62)] / [\u03b1\u2080\u00b2(\u03b1\u2080+1)], where \u03b1\u2080 = \u2211\u2c7c\u03b1\u2c7c Support: \u03b4_{k-1} = {x \u2208 \u211d^k : x\u1d62 \u2265 0, \u2211\u1d62x\u1d62 = 1}</p> Properties <ul> <li>Conjugate prior for categorical/multinomial</li> <li>Marginals: X\u1d62 ~ Beta(\u03b1\u1d62, \u2211\u2c7c\u2260\u1d62\u03b1\u2c7c)</li> <li>Aggregation property: (X\u1d62 + X\u2c7c, X_rest) follows lower-dim Dirichlet</li> <li>Neutral element: Dir(1, 1, ..., 1) = Uniform on simplex</li> </ul> Connection to Other Distributions <ul> <li>k=2: Dir(\u03b1\u2081, \u03b1\u2082) equivalent to Beta(\u03b1\u2081, \u03b1\u2082)</li> <li>Gamma construction: If Y\u1d62 ~ Gamma(\u03b1\u1d62, 1), then Y/\u2211Y ~ Dir(\u03b1)</li> <li>Log-normal approximation for large \u03b1</li> </ul> References <p>.. [1] Kotz, S., Balakrishnan, N., &amp; Johnson, N. L. (2000). \"Continuous        Multivariate Distributions\". Wiley, Vol. 1, Chapter 49. .. [2] Ng, K. W., Tian, G. L., &amp; Tang, M. L. (2011). \"Dirichlet and        Related Distributions\". Wiley.</p>"},{"location":"reference/distributions/#genjax.distributions.binomial","title":"binomial  <code>module-attribute</code>","text":"<pre><code>binomial = tfp_distribution(Binomial, name='Binomial')\n</code></pre> <p>Binomial distribution for count data with fixed number of trials.</p> Mathematical Formulation <p>PMF: P(X = k) = C(n,k) \u00d7 p^k \u00d7 (1-p)^(n-k) for k \u2208 {0, 1, ..., n}</p> <p>Where n is the number of trials, p is success probability, and C(n,k) = n!/(k!(n-k)!) is the binomial coefficient.</p> <p>Mean: \ud835\udd3c[X] = np Variance: Var[X] = np(1-p) MGF: M(t) = (1 - p + pe<sup>t)</sup>n Support: {0, 1, 2, ..., n}</p> Properties <ul> <li>Sum of Bernoulli: X = \u2211\u1d62 Y\u1d62 where Y\u1d62 ~ Bernoulli(p)</li> <li>Additivity: Bin(n\u2081,p) + Bin(n\u2082,p) = Bin(n\u2081+n\u2082,p)</li> <li>Symmetry: If p = 0.5, then P(X = k) = P(X = n-k)</li> </ul> Approximations <ul> <li>Normal: For large n, np(1-p) &gt; 10, approximately N(np, np(1-p))</li> <li>Poisson: For large n, small p, np = \u03bb moderate, approximately Pois(\u03bb)</li> </ul> References <p>.. [1] Johnson, N. L., Kotz, S., &amp; Kemp, A. W. (1992). \"Univariate        Discrete Distributions\". Wiley, Chapter 3.</p>"},{"location":"reference/distributions/#genjax.distributions.gamma","title":"gamma  <code>module-attribute</code>","text":"<pre><code>gamma = tfp_distribution(Gamma, name='Gamma')\n</code></pre> <p>Gamma distribution for positive continuous values.</p> Mathematical Formulation <p>PDF: f(x; \u03b1, \u03b2) = (\u03b2^\u03b1 / \u0393(\u03b1)) \u00d7 x^(\u03b1-1) \u00d7 exp(-\u03b2x) for x &gt; 0</p> <p>Where \u03b1 &gt; 0 is the shape, \u03b2 &gt; 0 is the rate (or \u03b8 = 1/\u03b2 is scale).</p> <p>Mean: \ud835\udd3c[X] = \u03b1/\u03b2 = \u03b1\u03b8 Variance: Var[X] = \u03b1/\u03b2\u00b2 = \u03b1\u03b8\u00b2 Mode: (\u03b1-1)/\u03b2 for \u03b1 \u2265 1 Support: (0, \u221e)</p> Special Cases <ul> <li>\u03b1 = 1: Exponential(\u03b2)</li> <li>\u03b1 = k/2, \u03b2 = 1/2: Chi-squared(k)</li> <li>Integer \u03b1: Erlang distribution</li> </ul> Properties <ul> <li>Additivity: Gamma(\u03b1\u2081,\u03b2) + Gamma(\u03b1\u2082,\u03b2) = Gamma(\u03b1\u2081+\u03b1\u2082,\u03b2)</li> <li>Scaling: cX ~ Gamma(\u03b1, \u03b2/c) for c &gt; 0</li> <li>Conjugate prior for Poisson rate, exponential rate</li> </ul> Connection to Other Distributions <ul> <li>If X\u1d62 ~ Gamma(\u03b1\u1d62, 1), then X\u1d62/\u2211X\u2c7c ~ Dirichlet(\u03b1)</li> <li>Inverse: 1/X ~ InverseGamma(\u03b1, \u03b2)</li> </ul> References <p>.. [1] Johnson, N. L., Kotz, S., &amp; Balakrishnan, N. (1994). \"Continuous        Univariate Distributions\". Wiley, Vol. 1, Chapter 17.</p>"},{"location":"reference/distributions/#genjax.distributions.log_normal","title":"log_normal  <code>module-attribute</code>","text":"<pre><code>log_normal = tfp_distribution(LogNormal, name='LogNormal')\n</code></pre> <p>Log-normal distribution (exponential of normal random variable).</p> Mathematical Formulation <p>If Y ~ N(\u03bc, \u03c3\u00b2), then X = exp(Y) ~ LogNormal(\u03bc, \u03c3\u00b2)</p> <p>PDF: f(x; \u03bc, \u03c3) = (1/(x\u03c3\u221a(2\u03c0))) \u00d7 exp(-(ln(x)-\u03bc)\u00b2/(2\u03c3\u00b2)) for x &gt; 0</p> <p>Mean: \ud835\udd3c[X] = exp(\u03bc + \u03c3\u00b2/2) Variance: Var[X] = (exp(\u03c3\u00b2) - 1) \u00d7 exp(2\u03bc + \u03c3\u00b2) Mode: exp(\u03bc - \u03c3\u00b2) Support: (0, \u221e)</p> Properties <ul> <li>Multiplicative: If X\u1d62 ~ LogN(\u03bc\u1d62, \u03c3\u1d62\u00b2) independent, then \u220fX\u1d62 is log-normal</li> <li>Not closed under addition (sum of log-normals is not log-normal)</li> <li>Heavy right tail: all moments exist but grow rapidly</li> <li>Median: exp(\u03bc)</li> </ul> Applications <ul> <li>Income distributions</li> <li>Stock prices (geometric Brownian motion)</li> <li>Particle sizes</li> <li>Species abundance</li> </ul> References <p>.. [1] Crow, E. L., &amp; Shimizu, K. (Eds.). (1988). \"Lognormal Distributions:        Theory and Applications\". Marcel Dekker. .. [2] Limpert, E., Stahel, W. A., &amp; Abbt, M. (2001). \"Log-normal        distributions across the sciences\". BioScience, 51(5), 341-352.</p>"},{"location":"reference/distributions/#genjax.distributions.student_t","title":"student_t  <code>module-attribute</code>","text":"<pre><code>student_t = tfp_distribution(StudentT, name='StudentT')\n</code></pre> <p>Student's t-distribution with specified degrees of freedom.</p> Mathematical Formulation <p>PDF: f(x; \u03bd, \u03bc, \u03c3) = \u0393((\u03bd+1)/2)/(\u0393(\u03bd/2)\u221a(\u03bd\u03c0)\u03c3) \u00d7 [1 + ((x-\u03bc)/\u03c3)\u00b2/\u03bd]^(-(\u03bd+1)/2)</p> <p>Where \u03bd &gt; 0 is degrees of freedom, \u03bc is location, \u03c3 &gt; 0 is scale.</p> <p>Mean: \ud835\udd3c[X] = \u03bc for \u03bd &gt; 1 (undefined for \u03bd \u2264 1) Variance: Var[X] = \u03c3\u00b2\u03bd/(\u03bd-2) for \u03bd &gt; 2 (infinite for 1 &lt; \u03bd \u2264 2) Support: \u211d</p> Properties <ul> <li>Heavier tails than normal (polynomial vs exponential decay)</li> <li>\u03bd \u2192 \u221e: Converges to Normal(\u03bc, \u03c3\u00b2)</li> <li>\u03bd = 1: Cauchy distribution (no mean)</li> <li>\u03bd = 2: Finite mean but infinite variance</li> <li>Symmetric about \u03bc</li> </ul> Standardized Form <p>If T ~ t(\u03bd), then X = \u03bc + \u03c3T ~ t(\u03bd, \u03bc, \u03c3)</p> Connection to Other Distributions <ul> <li>Ratio of normal to chi: If Z ~ N(0,1), V ~ \u03c7\u00b2(\u03bd), then Z/\u221a(V/\u03bd) ~ t(\u03bd)</li> <li>F-distribution: T\u00b2 ~ F(1, \u03bd) if T ~ t(\u03bd)</li> </ul> References <p>.. [1] Lange, K. L., Little, R. J., &amp; Taylor, J. M. (1989). \"Robust        statistical modeling using the t distribution\". JASA, 84(408), 881-896. .. [2] Kotz, S., &amp; Nadarajah, S. (2004). \"Multivariate t-distributions        and their applications\". Cambridge University Press.</p>"},{"location":"reference/distributions/#genjax.distributions.laplace","title":"laplace  <code>module-attribute</code>","text":"<pre><code>laplace = tfp_distribution(Laplace, name='Laplace')\n</code></pre> <p>Laplace (double exponential) distribution.</p>"},{"location":"reference/distributions/#genjax.distributions.half_normal","title":"half_normal  <code>module-attribute</code>","text":"<pre><code>half_normal = tfp_distribution(HalfNormal, name='HalfNormal')\n</code></pre> <p>Half-normal distribution (positive half of normal distribution).</p>"},{"location":"reference/distributions/#genjax.distributions.inverse_gamma","title":"inverse_gamma  <code>module-attribute</code>","text":"<pre><code>inverse_gamma = tfp_distribution(InverseGamma, name='InverseGamma')\n</code></pre> <p>Inverse gamma distribution for positive continuous values.</p>"},{"location":"reference/distributions/#genjax.distributions.weibull","title":"weibull  <code>module-attribute</code>","text":"<pre><code>weibull = tfp_distribution(Weibull, name='Weibull')\n</code></pre> <p>Weibull distribution for modeling survival times and reliability.</p>"},{"location":"reference/distributions/#genjax.distributions.cauchy","title":"cauchy  <code>module-attribute</code>","text":"<pre><code>cauchy = tfp_distribution(Cauchy, name='Cauchy')\n</code></pre> <p>Cauchy distribution with heavy tails.</p>"},{"location":"reference/distributions/#genjax.distributions.chi2","title":"chi2  <code>module-attribute</code>","text":"<pre><code>chi2 = tfp_distribution(Chi2, name='Chi2')\n</code></pre> <p>Chi-squared distribution.</p>"},{"location":"reference/distributions/#genjax.distributions.multinomial","title":"multinomial  <code>module-attribute</code>","text":"<pre><code>multinomial = tfp_distribution(Multinomial, name='Multinomial')\n</code></pre> <p>Multinomial distribution over count vectors.</p>"},{"location":"reference/distributions/#genjax.distributions.negative_binomial","title":"negative_binomial  <code>module-attribute</code>","text":"<pre><code>negative_binomial = tfp_distribution(NegativeBinomial, name='NegativeBinomial')\n</code></pre> <p>Negative binomial distribution for overdispersed count data.</p>"},{"location":"reference/distributions/#genjax.distributions.zipf","title":"zipf  <code>module-attribute</code>","text":"<pre><code>zipf = tfp_distribution(Zipf, name='Zipf')\n</code></pre> <p>Zipf distribution for power-law distributed discrete data.</p>"},{"location":"reference/distributions/#live-examples","title":"Live Examples","text":""},{"location":"reference/distributions/#continuous-distributions","title":"Continuous Distributions","text":"<pre><code>import jax.numpy as jnp\nfrom genjax import distributions\n\n# Assess log probability under various distributions\nx = 1.5\n\n# Normal distribution\nlog_prob_normal, _ = distributions.normal.assess(x, 0.0, 1.0)\nprint(f\"Log prob of {x} under Normal(0, 1): {log_prob_normal:.3f}\")\n\n# Beta distribution (x must be in [0, 1])\nx_beta = 0.7\nlog_prob_beta, _ = distributions.beta.assess(x_beta, 2.0, 2.0)\nprint(f\"Log prob of {x_beta} under Beta(2, 2): {log_prob_beta:.3f}\")\n\n# Exponential distribution\nlog_prob_exp, _ = distributions.exponential.assess(x, 1.0)\nprint(f\"Log prob of {x} under Exponential(1): {log_prob_exp:.3f}\")\n</code></pre> <p>Log prob of 1.5 under Normal(0, 1): -2.044 Log prob of 0.7 under Beta(2, 2): 0.231 Log prob of 1.5 under Exponential(1): -1.500</p>"},{"location":"reference/distributions/#discrete-distributions","title":"Discrete Distributions","text":"<pre><code>import jax.numpy as jnp\nfrom genjax import distributions\n\n# Bernoulli distribution\nlog_prob_bern, _ = distributions.bernoulli.assess(1, 0.7)\nprint(f\"Log prob of 1 under Bernoulli(0.7): {log_prob_bern:.3f}\")\n\n# Categorical distribution (uses logits, not probs)\nprobs = jnp.array([0.2, 0.3, 0.5])\nlogits = jnp.log(probs)\nlog_prob_cat, _ = distributions.categorical.assess(2, logits)\nprint(f\"Log prob of category 2 under Categorical(probs={probs}): {log_prob_cat:.3f}\")\n\n# Poisson distribution\nlog_prob_pois, _ = distributions.poisson.assess(4, 3.0)\nprint(f\"Log prob of 4 under Poisson(3.0): {log_prob_pois:.3f}\")\n</code></pre> <p>Log prob of 1 under Bernoulli(0.7): -0.403 Log prob of category 2 under Categorical(probs=[0.2 0.3 0.5]): -0.693 Log prob of 4 under Poisson(3.0): -1.784</p>"},{"location":"reference/distributions/#distribution-parameters","title":"Distribution Parameters","text":"<pre><code># Distributions are parameterized by their standard parameters\nprint(\"Common distribution parameterizations:\")\nprint(\"- normal(mu, sigma)\")\nprint(\"- beta(alpha, beta)\")\nprint(\"- exponential(rate)\")\nprint(\"- bernoulli(p)\")\nprint(\"- categorical(logits)  # Note: uses logits, not probs\")\nprint(\"- poisson(rate)\")\nprint(\"- gamma(concentration, rate)\")\nprint(\"- uniform(low, high)\")\n</code></pre> <p>Common distribution parameterizations: - normal(mu, sigma) - beta(alpha, beta) - exponential(rate) - bernoulli(p) - categorical(logits)  # Note: uses logits, not probs - poisson(rate) - gamma(concentration, rate) - uniform(low, high)</p>"},{"location":"reference/mcmc/","title":"genjax.inference.mcmc","text":"<p>Markov Chain Monte Carlo algorithms for probabilistic inference.</p>"},{"location":"reference/mcmc/#genjax.inference.mcmc","title":"mcmc","text":"<p>MCMC (Markov Chain Monte Carlo) inference algorithms for GenJAX.</p> <p>This module provides implementations of standard MCMC algorithms including Metropolis-Hastings, MALA (Metropolis-Adjusted Langevin Algorithm), and HMC (Hamiltonian Monte Carlo). All algorithms use the GFI (Generative Function Interface) for efficient trace operations.</p>"},{"location":"reference/mcmc/#genjax.inference.mcmc--references","title":"References","text":"<p>Metropolis-Hastings Algorithm: - Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953).   \"Equation of state calculations by fast computing machines.\"   The Journal of Chemical Physics, 21(6), 1087-1092. - Hastings, W. K. (1970). \"Monte Carlo sampling methods using Markov chains and their applications.\"   Biometrika, 57(1), 97-109.</p> <p>MALA (Metropolis-Adjusted Langevin Algorithm): - Roberts, G. O., &amp; Tweedie, R. L. (1996). \"Exponential convergence of Langevin distributions   and their discrete approximations.\" Bernoulli, 2(4), 341-363. - Roberts, G. O., &amp; Rosenthal, J. S. (1998). \"Optimal scaling of discrete approximations to   Langevin diffusions.\" Journal of the Royal Statistical Society: Series B, 60(1), 255-268.</p> <p>HMC (Hamiltonian Monte Carlo): - Neal, R. M. (2011). \"MCMC Using Hamiltonian Dynamics\", Handbook of Markov Chain Monte Carlo,   pp. 113-162. URL: http://www.mcmchandbook.net/HandbookChapter5.pdf - Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987). \"Hybrid Monte Carlo.\"   Physics Letters B, 195(2), 216-222.</p> <p>Implementation Reference: - Gen.jl MALA implementation: https://github.com/probcomp/Gen.jl/blob/master/src/inference/mala.jl - Gen.jl HMC implementation: https://github.com/probcomp/Gen.jl/blob/master/src/inference/hmc.jl</p>"},{"location":"reference/mcmc/#genjax.inference.mcmc.MCMCResult","title":"MCMCResult","text":"<p>               Bases: <code>Pytree</code></p> <p>Result of MCMC chain sampling containing traces and diagnostics.</p>"},{"location":"reference/mcmc/#genjax.inference.mcmc.compute_rhat","title":"compute_rhat","text":"<pre><code>compute_rhat(samples: ndarray) -&gt; FloatArray\n</code></pre> <p>Compute potential scale reduction factor (R-hat) for MCMC convergence diagnostics.</p> <p>Implements the split-R-hat diagnostic from Vehtari et al. (2021), which improves upon the original formulation of Gelman &amp; Rubin (1992) by accounting for non-stationarity within chains.</p> Mathematical Formulation <p>Given M chains each of length N, compute:</p> <p>B = N/(M-1) * \u03a3\u1d62 (\u03b8\u0304\u1d62 - \u03b8\u0304)\u00b2  (between-chain variance) W = 1/M * \u03a3\u1d62 s\u1d62\u00b2             (within-chain variance)</p> <p>where \u03b8\u0304\u1d62 is the mean of chain i, \u03b8\u0304 is the grand mean, and s\u1d62\u00b2 is the sample variance of chain i.</p> <p>The potential scale reduction factor is: R\u0302 = \u221a[(N-1)/N * W + 1/N * B] / W</p> Convergence Criterion <p>R\u0302 &lt; 1.01 indicates good convergence (Vehtari et al., 2021) R\u0302 &lt; 1.1 was the classical threshold (Gelman &amp; Rubin, 1992)</p> References <p>.. [1] Gelman, A., &amp; Rubin, D. B. (1992). \"Inference from iterative        simulation using multiple sequences\". Statistical Science, 7(4), 457-472. .. [2] Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; B\u00fcrkner, P. C.        (2021). \"Rank-normalization, folding, and localization: An improved R\u0302        for assessing convergence of MCMC\". Bayesian Analysis, 16(2), 667-718.</p> Notes <ul> <li>This implementation uses the basic R-hat without rank-normalization</li> <li>For rank-normalized R-hat (more robust), see [2]</li> <li>Requires at least 2 chains for meaningful computation</li> </ul> Source code in <code>src/genjax/inference/mcmc.py</code> <pre><code>def compute_rhat(samples: jnp.ndarray) -&gt; FloatArray:\n    \"\"\"\n    Compute potential scale reduction factor (R-hat) for MCMC convergence diagnostics.\n\n    Implements the split-R-hat diagnostic from Vehtari et al. (2021), which improves\n    upon the original formulation of Gelman &amp; Rubin (1992) by accounting for\n    non-stationarity within chains.\n\n    Mathematical Formulation:\n        Given M chains each of length N, compute:\n\n        B = N/(M-1) * \u03a3\u1d62 (\u03b8\u0304\u1d62 - \u03b8\u0304)\u00b2  (between-chain variance)\n        W = 1/M * \u03a3\u1d62 s\u1d62\u00b2             (within-chain variance)\n\n        where \u03b8\u0304\u1d62 is the mean of chain i, \u03b8\u0304 is the grand mean, and s\u1d62\u00b2 is\n        the sample variance of chain i.\n\n        The potential scale reduction factor is:\n        R\u0302 = \u221a[(N-1)/N * W + 1/N * B] / W\n\n    Convergence Criterion:\n        R\u0302 &lt; 1.01 indicates good convergence (Vehtari et al., 2021)\n        R\u0302 &lt; 1.1 was the classical threshold (Gelman &amp; Rubin, 1992)\n\n    Args:\n        samples: Array of shape (n_chains, n_samples) containing MCMC samples\n                 from M chains each of length N\n\n    Returns:\n        R-hat statistic. Values close to 1.0 indicate convergence.\n        Returns NaN if n_chains &lt; 2.\n\n    References:\n        .. [1] Gelman, A., &amp; Rubin, D. B. (1992). \"Inference from iterative\n               simulation using multiple sequences\". Statistical Science, 7(4), 457-472.\n        .. [2] Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; B\u00fcrkner, P. C.\n               (2021). \"Rank-normalization, folding, and localization: An improved R\u0302\n               for assessing convergence of MCMC\". Bayesian Analysis, 16(2), 667-718.\n\n    Notes:\n        - This implementation uses the basic R-hat without rank-normalization\n        - For rank-normalized R-hat (more robust), see [2]\n        - Requires at least 2 chains for meaningful computation\n    \"\"\"\n    n_chains, n_samples = samples.shape\n\n    # For R-hat, we need at least 2 chains and enough samples\n    if n_chains &lt; 2:\n        return jnp.nan\n\n    # Use all samples for simpler computation\n    # Compute chain means\n    chain_means = jnp.mean(samples, axis=1)  # (n_chains,)\n\n    # Between-chain variance\n    B = n_samples * jnp.var(chain_means, ddof=1)\n\n    # Within-chain variance\n    chain_vars = jnp.var(samples, axis=1, ddof=1)  # (n_chains,)\n    W = jnp.mean(chain_vars)\n\n    # Pooled variance estimate\n    var_plus = ((n_samples - 1) * W + B) / n_samples\n\n    # R-hat statistic\n    rhat = jnp.sqrt(var_plus / W)\n\n    return rhat\n</code></pre>"},{"location":"reference/mcmc/#genjax.inference.mcmc.compute_ess","title":"compute_ess","text":"<pre><code>compute_ess(samples: ndarray, kind: str = 'bulk') -&gt; FloatArray\n</code></pre> <p>Compute effective sample size (ESS) for MCMC chains.</p> <p>Estimates the number of independent samples accounting for autocorrelation in Markov chains. Implements simplified versions of bulk and tail ESS from Vehtari et al. (2021).</p> Mathematical Formulation <p>The effective sample size is defined as:</p> <p>ESS = M \u00d7 N / \u03c4</p> <p>where M is the number of chains, N is the chain length, and \u03c4 is the integrated autocorrelation time:</p> <p>\u03c4 = 1 + 2 \u00d7 \u03a3\u2096 \u03c1\u2096</p> <p>where \u03c1\u2096 is the autocorrelation at lag k, summed over positive correlations.</p> Algorithm <ul> <li>Bulk ESS: Uses all samples to estimate central tendency efficiency</li> <li>Tail ESS: Uses quantile differences (0.05 and 0.95) to assess tail behavior</li> </ul> <p>This implementation uses a simplified approximation based on lag-1 autocorrelation: ESS \u2248 N / (1 + 2\u03c1\u2081)</p> <p>Time Complexity: O(M \u00d7 N) Space Complexity: O(1)</p> References <p>.. [1] Geyer, C. J. (1992). \"Practical Markov chain Monte Carlo\".        Statistical Science, 7(4), 473-483. .. [2] Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; B\u00fcrkner, P. C.        (2021). \"Rank-normalization, folding, and localization: An improved R\u0302        for assessing convergence of MCMC\". Bayesian Analysis, 16(2), 667-718. .. [3] Stan Development Team (2023). \"Stan Reference Manual: Effective        Sample Size\". Version 2.33. Section 15.4.</p> Notes <ul> <li>This is a simplified implementation using lag-1 autocorrelation</li> <li>Full implementation would compute autocorrelation function to first negative</li> <li>Tail ESS focuses on extreme quantiles, useful for credible intervals</li> <li>Bulk ESS focuses on center, useful for posterior expectations</li> </ul> Source code in <code>src/genjax/inference/mcmc.py</code> <pre><code>def compute_ess(samples: jnp.ndarray, kind: str = \"bulk\") -&gt; FloatArray:\n    \"\"\"\n    Compute effective sample size (ESS) for MCMC chains.\n\n    Estimates the number of independent samples accounting for autocorrelation\n    in Markov chains. Implements simplified versions of bulk and tail ESS from\n    Vehtari et al. (2021).\n\n    Mathematical Formulation:\n        The effective sample size is defined as:\n\n        ESS = M \u00d7 N / \u03c4\n\n        where M is the number of chains, N is the chain length, and \u03c4 is the\n        integrated autocorrelation time:\n\n        \u03c4 = 1 + 2 \u00d7 \u03a3\u2096 \u03c1\u2096\n\n        where \u03c1\u2096 is the autocorrelation at lag k, summed over positive correlations.\n\n    Algorithm:\n        - Bulk ESS: Uses all samples to estimate central tendency efficiency\n        - Tail ESS: Uses quantile differences (0.05 and 0.95) to assess tail behavior\n\n        This implementation uses a simplified approximation based on lag-1\n        autocorrelation: ESS \u2248 N / (1 + 2\u03c1\u2081)\n\n    Time Complexity: O(M \u00d7 N)\n    Space Complexity: O(1)\n\n    Args:\n        samples: Array of shape (n_chains, n_samples) containing MCMC samples\n        kind: Type of ESS to compute:\n              - \"bulk\": Efficiency for estimating posterior mean/median\n              - \"tail\": Efficiency for estimating posterior quantiles\n\n    Returns:\n        Effective sample size estimate. Range: [1, M \u00d7 N]\n        Lower values indicate higher autocorrelation.\n\n    References:\n        .. [1] Geyer, C. J. (1992). \"Practical Markov chain Monte Carlo\".\n               Statistical Science, 7(4), 473-483.\n        .. [2] Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; B\u00fcrkner, P. C.\n               (2021). \"Rank-normalization, folding, and localization: An improved R\u0302\n               for assessing convergence of MCMC\". Bayesian Analysis, 16(2), 667-718.\n        .. [3] Stan Development Team (2023). \"Stan Reference Manual: Effective\n               Sample Size\". Version 2.33. Section 15.4.\n\n    Notes:\n        - This is a simplified implementation using lag-1 autocorrelation\n        - Full implementation would compute autocorrelation function to first negative\n        - Tail ESS focuses on extreme quantiles, useful for credible intervals\n        - Bulk ESS focuses on center, useful for posterior expectations\n    \"\"\"\n    n_chains, n_samples = samples.shape\n\n    if kind == \"tail\":\n        # For tail ESS, use quantile-based approach\n        # Transform samples to focus on tails\n        quantiles = jnp.array([0.05, 0.95])\n        tail_samples = jnp.quantile(samples, quantiles, axis=1)\n        # Use difference between quantiles as the statistic\n        samples_for_ess = tail_samples[1] - tail_samples[0]\n        samples_for_ess = samples_for_ess.reshape(1, -1)\n    else:\n        # For bulk ESS, use all samples\n        samples_for_ess = samples.reshape(1, -1)\n\n    # Simple ESS approximation based on autocorrelation\n    # This is a simplified version - a full implementation would compute\n    # autocorrelation function and find cutoff\n\n    # Compute autocorrelation at lag 1 as rough approximation\n    flat_samples = samples_for_ess.flatten()\n\n    # Autocorrelation at lag 1\n    lag1_corr = jnp.corrcoef(flat_samples[:-1], flat_samples[1:])[0, 1]\n    lag1_corr = jnp.clip(lag1_corr, 0.0, 0.99)  # Avoid division issues\n\n    # Simple ESS approximation: N / (1 + 2*rho)\n    # where rho is the sum of positive autocorrelations\n    effective_chains = n_chains if kind == \"bulk\" else 1\n    total_samples = effective_chains * n_samples\n    ess = total_samples / (1 + 2 * lag1_corr)\n\n    return ess\n</code></pre>"},{"location":"reference/mcmc/#genjax.inference.mcmc.mh","title":"mh","text":"<pre><code>mh(current_trace: Trace[X, R], selection: Selection) -&gt; Trace[X, R]\n</code></pre> <p>Single Metropolis-Hastings step using GFI.regenerate.</p> <p>Uses the trace's generative function regenerate method to propose new values for selected addresses and computes MH accept/reject ratio.</p> State <p>accept: Boolean indicating whether the proposal was accepted</p> Source code in <code>src/genjax/inference/mcmc.py</code> <pre><code>def mh(\n    current_trace: Trace[X, R],\n    selection: Selection,\n) -&gt; Trace[X, R]:\n    \"\"\"\n    Single Metropolis-Hastings step using GFI.regenerate.\n\n    Uses the trace's generative function regenerate method to propose\n    new values for selected addresses and computes MH accept/reject ratio.\n\n    Args:\n        current_trace: Current trace state\n        selection: Addresses to regenerate (subset of choices)\n\n    Returns:\n        Updated trace after MH step\n\n    State:\n        accept: Boolean indicating whether the proposal was accepted\n    \"\"\"\n    target_gf = current_trace.get_gen_fn()\n    args = current_trace.get_args()\n\n    # Regenerate selected addresses - weight is log acceptance probability\n    new_trace, log_weight, _ = target_gf.regenerate(\n        current_trace, selection, *args[0], **args[1]\n    )\n\n    # MH acceptance step in log space\n    log_alpha = jnp.minimum(0.0, log_weight)  # log(min(1, exp(log_weight)))\n\n    # Accept or reject using GenJAX uniform distribution in log space\n    log_u = jnp.log(uniform.sample(0.0, 1.0))\n    accept = log_u &lt; log_alpha\n\n    # Use tree_map to apply select across all leaves of the traces\n    final_trace = jtu.tree_map(\n        lambda new_leaf, old_leaf: jax.lax.select(accept, new_leaf, old_leaf),\n        new_trace,\n        current_trace,\n    )\n\n    # Save acceptance as auxiliary state (can be accessed via state decorator)\n    save(accept=accept)\n\n    return final_trace\n</code></pre>"},{"location":"reference/mcmc/#genjax.inference.mcmc.mala","title":"mala","text":"<pre><code>mala(current_trace: Trace[X, R], selection: Selection, step_size: float) -&gt; Trace[X, R]\n</code></pre> <p>Single MALA (Metropolis-Adjusted Langevin Algorithm) step.</p> <p>MALA uses gradient information to make more efficient proposals than standard Metropolis-Hastings. The proposal distribution is:</p> <p>x_proposed = x_current + step_size^2/2 * \u2207log(p(x)) + step_size * \u03b5</p> <p>where \u03b5 ~ N(0, I) is standard Gaussian noise.</p> <p>This implementation follows the approach from Gen.jl, computing both forward and backward proposal probabilities to account for the asymmetric drift term in the MALA proposal.</p> State <p>accept: Boolean indicating whether the proposal was accepted</p> Source code in <code>src/genjax/inference/mcmc.py</code> <pre><code>def mala(\n    current_trace: Trace[X, R],\n    selection: Selection,\n    step_size: float,\n) -&gt; Trace[X, R]:\n    \"\"\"\n    Single MALA (Metropolis-Adjusted Langevin Algorithm) step.\n\n    MALA uses gradient information to make more efficient proposals than\n    standard Metropolis-Hastings. The proposal distribution is:\n\n    x_proposed = x_current + step_size^2/2 * \u2207log(p(x)) + step_size * \u03b5\n\n    where \u03b5 ~ N(0, I) is standard Gaussian noise.\n\n    This implementation follows the approach from Gen.jl, computing both\n    forward and backward proposal probabilities to account for the asymmetric\n    drift term in the MALA proposal.\n\n    Args:\n        current_trace: Current trace state\n        selection: Addresses to regenerate (subset of choices)\n        step_size: Step size parameter (\u03c4) controlling proposal variance\n\n    Returns:\n        Updated trace after MALA step\n\n    State:\n        accept: Boolean indicating whether the proposal was accepted\n    \"\"\"\n    target_gf = current_trace.get_gen_fn()\n    args = current_trace.get_args()\n    current_choices = current_trace.get_choices()\n\n    # Use the new GFI.filter method to extract selected choices\n    selected_choices, unselected_choices = target_gf.filter(current_choices, selection)\n\n    if selected_choices is None:\n        # No choices selected, return current trace unchanged\n        save(accept=True)\n        return current_trace\n\n    # Create closure to compute gradients with respect to only selected choices\n    log_density_wrt_selected = _create_log_density_wrt_selected(\n        target_gf, args, unselected_choices\n    )\n\n    # Get gradients with respect to selected choices only\n    selected_gradients = jax.grad(log_density_wrt_selected)(selected_choices)\n\n    # Generate MALA proposal for selected choices using tree operations\n    def mala_proposal_fn(current_val, grad_val):\n        # MALA drift term: step_size^2/2 * gradient\n        drift = (step_size**2 / 2.0) * grad_val\n\n        # Gaussian noise term: step_size * N(0,1)\n        noise = step_size * normal.sample(0.0, 1.0)\n\n        # Proposed value\n        return current_val + drift + noise\n\n    def mala_log_prob_fn(current_val, proposed_val, grad_val):\n        # MALA proposal log probability: N(current + drift, step_size)\n        drift = (step_size**2 / 2.0) * grad_val\n        mean = current_val + drift\n        log_probs = normal.logpdf(proposed_val, mean, step_size)\n        # Sum over all dimensions to get scalar log probability\n        return jnp.sum(log_probs)\n\n    # Apply MALA proposal to all selected choices\n    proposed_selected = jtu.tree_map(\n        mala_proposal_fn, selected_choices, selected_gradients\n    )\n\n    # Compute forward proposal log probabilities\n    forward_log_probs = jtu.tree_map(\n        mala_log_prob_fn, selected_choices, proposed_selected, selected_gradients\n    )\n\n    # Update trace with only the proposed selected choices\n    # This ensures discard only contains the keys that were actually changed\n    proposed_trace, model_weight, discard = target_gf.update(\n        current_trace, proposed_selected, *args[0], **args[1]\n    )\n\n    # Get gradients at proposed point with respect to selected choices\n    backward_gradients = jax.grad(log_density_wrt_selected)(proposed_selected)\n\n    # Filter discard to only the selected addresses (in case update includes extra keys)\n    discarded_selected, _ = target_gf.filter(discard, selection)\n\n    # Compute backward proposal log probabilities using the same function\n    backward_log_probs = jtu.tree_map(\n        mala_log_prob_fn,\n        proposed_selected,\n        discarded_selected,\n        backward_gradients,\n    )\n\n    # Sum up log probabilities using tree_reduce\n    forward_log_prob_total = jtu.tree_reduce(jnp.add, forward_log_probs)\n    backward_log_prob_total = jtu.tree_reduce(jnp.add, backward_log_probs)\n\n    # MALA acceptance probability\n    # Alpha = model_weight + log P(x_old | x_new) - log P(x_new | x_old)\n    log_alpha = model_weight + backward_log_prob_total - forward_log_prob_total\n    log_alpha = jnp.minimum(0.0, log_alpha)  # min(1, exp(log_alpha))\n\n    # Accept or reject using numerically stable log comparison\n    log_u = jnp.log(uniform.sample(0.0, 1.0))\n    accept = log_u &lt; log_alpha\n\n    # Select final trace\n    final_trace = jtu.tree_map(\n        lambda new_leaf, old_leaf: jax.lax.select(accept, new_leaf, old_leaf),\n        proposed_trace,\n        current_trace,\n    )\n\n    # Save acceptance for diagnostics\n    save(accept=accept)\n\n    return final_trace\n</code></pre>"},{"location":"reference/mcmc/#genjax.inference.mcmc.hmc","title":"hmc","text":"<pre><code>hmc(current_trace: Trace[X, R], selection: Selection, step_size: float, n_steps: int) -&gt; Trace[X, R]\n</code></pre> <p>Single HMC (Hamiltonian Monte Carlo) step using leapfrog integration.</p> <p>HMC uses gradient information and auxiliary momentum variables to propose distant moves that maintain detailed balance. The algorithm simulates Hamiltonian dynamics using leapfrog integration:</p> <ol> <li>Sample momentum p ~ N(0, I)</li> <li>Simulate Hamiltonian dynamics for n_steps using leapfrog integration:</li> <li>p' = p + (eps/2) * \u2207log(p(x))</li> <li>x' = x + eps * p'</li> <li>p' = p' + (eps/2) * \u2207log(p(x'))</li> <li>Accept/reject using Metropolis criterion with joint (x,p) density</li> </ol> <p>This implementation uses jax.lax.scan for leapfrog integration, making it fully JAX-compatible and JIT-compilable. It follows Neal (2011) equations (5.18)-(5.20) and the Gen.jl HMC implementation structure.</p> State <p>accept: Boolean indicating whether the proposal was accepted</p> Source code in <code>src/genjax/inference/mcmc.py</code> <pre><code>def hmc(\n    current_trace: Trace[X, R],\n    selection: Selection,\n    step_size: float,\n    n_steps: int,\n) -&gt; Trace[X, R]:\n    \"\"\"\n    Single HMC (Hamiltonian Monte Carlo) step using leapfrog integration.\n\n    HMC uses gradient information and auxiliary momentum variables to propose\n    distant moves that maintain detailed balance. The algorithm simulates\n    Hamiltonian dynamics using leapfrog integration:\n\n    1. Sample momentum p ~ N(0, I)\n    2. Simulate Hamiltonian dynamics for n_steps using leapfrog integration:\n       - p' = p + (eps/2) * \u2207log(p(x))\n       - x' = x + eps * p'\n       - p' = p' + (eps/2) * \u2207log(p(x'))\n    3. Accept/reject using Metropolis criterion with joint (x,p) density\n\n    This implementation uses jax.lax.scan for leapfrog integration, making it\n    fully JAX-compatible and JIT-compilable. It follows Neal (2011) equations\n    (5.18)-(5.20) and the Gen.jl HMC implementation structure.\n\n    Args:\n        current_trace: Current trace state\n        selection: Addresses to regenerate (subset of choices)\n        step_size: Leapfrog integration step size (eps)\n        n_steps: Number of leapfrog steps (L)\n\n    Returns:\n        Updated trace after HMC step\n\n    State:\n        accept: Boolean indicating whether the proposal was accepted\n    \"\"\"\n    target_gf = current_trace.get_gen_fn()\n    args = current_trace.get_args()\n    current_choices = current_trace.get_choices()\n\n    # Use the new GFI.filter method to extract selected choices\n    selected_choices, unselected_choices = target_gf.filter(current_choices, selection)\n\n    if selected_choices is None:\n        # No choices selected, return current trace unchanged\n        save(accept=True)\n        return current_trace\n\n    # Create closure to compute gradients with respect to only selected choices\n    log_density_wrt_selected = _create_log_density_wrt_selected(\n        target_gf, args, unselected_choices\n    )\n\n    # Helper functions for momentum\n    def sample_momentum(_):\n        \"\"\"Sample momentum with same structure as reference value.\"\"\"\n        return normal.sample(0.0, 1.0)\n\n    def assess_momentum(momentum_val):\n        \"\"\"Compute log probability of momentum (standard normal).\"\"\"\n        return normal.logpdf(momentum_val, 0.0, 1.0)\n\n    # Initial model score (negative potential energy)\n    prev_model_score = log_density_wrt_selected(selected_choices)\n\n    # Sample initial momentum and compute its score (negative kinetic energy)\n    initial_momentum = jtu.tree_map(sample_momentum, selected_choices)\n    prev_momentum_score = jtu.tree_reduce(\n        jnp.add, jtu.tree_map(assess_momentum, initial_momentum)\n    )\n\n    # Initialize leapfrog variables\n    current_position = selected_choices\n    current_momentum = initial_momentum\n\n    # Leapfrog integration for n_steps using jax.lax.scan\n    # Initial gradient\n    current_gradient = jax.grad(log_density_wrt_selected)(current_position)\n\n    def leapfrog_step(carry, _):\n        \"\"\"Single leapfrog integration step.\"\"\"\n        position, momentum, gradient = carry\n\n        # Half step on momentum\n        momentum = jtu.tree_map(\n            lambda p, g: p + (step_size / 2.0) * g, momentum, gradient\n        )\n\n        # Full step on position\n        position = jtu.tree_map(lambda x, p: x + step_size * p, position, momentum)\n\n        # Get new gradient at new position\n        gradient = jax.grad(log_density_wrt_selected)(position)\n\n        # Half step on momentum (completing the leapfrog step)\n        momentum = jtu.tree_map(\n            lambda p, g: p + (step_size / 2.0) * g, momentum, gradient\n        )\n\n        new_carry = (position, momentum, gradient)\n        return new_carry, None  # No output needed, just carry\n\n    # Run leapfrog integration\n    initial_carry = (current_position, current_momentum, current_gradient)\n    final_carry, _ = jax.lax.scan(leapfrog_step, initial_carry, jnp.arange(n_steps))\n\n    # Extract final position and momentum\n    final_position, final_momentum, _ = final_carry\n\n    # Update trace with proposed final position\n    proposed_trace, model_weight, discard = target_gf.update(\n        current_trace, final_position, *args[0], **args[1]\n    )\n\n    # Compute final model score (negative potential energy)\n    new_model_score = log_density_wrt_selected(final_position)\n\n    # Compute final momentum score (negative kinetic energy)\n    # Note: In HMC, we evaluate momentum at negated final momentum to account for\n    # the reversibility requirement of Hamiltonian dynamics\n    final_momentum_negated = jtu.tree_map(lambda p: -p, final_momentum)\n    new_momentum_score = jtu.tree_reduce(\n        jnp.add, jtu.tree_map(assess_momentum, final_momentum_negated)\n    )\n\n    # HMC acceptance probability\n    # alpha = (new_model_score + new_momentum_score) - (prev_model_score + prev_momentum_score)\n    # This is equivalent to the energy difference: -\u0394H = -(\u0394U + \u0394K)\n    log_alpha = (new_model_score + new_momentum_score) - (\n        prev_model_score + prev_momentum_score\n    )\n    log_alpha = jnp.minimum(0.0, log_alpha)  # min(1, exp(log_alpha))\n\n    # Accept or reject using numerically stable log comparison\n    log_u = jnp.log(uniform.sample(0.0, 1.0))\n    accept = log_u &lt; log_alpha\n\n    # Select final trace\n    final_trace = jtu.tree_map(\n        lambda new_leaf, old_leaf: jax.lax.select(accept, new_leaf, old_leaf),\n        proposed_trace,\n        current_trace,\n    )\n\n    # Save acceptance for diagnostics\n    save(accept=accept)\n\n    return final_trace\n</code></pre>"},{"location":"reference/mcmc/#genjax.inference.mcmc.chain","title":"chain","text":"<pre><code>chain(mcmc_kernel: MCMCKernel)\n</code></pre> <p>Higher-order function that creates MCMC chain algorithms from simple kernels.</p> <p>This function transforms simple MCMC moves (like metropolis_hastings_step) into full-fledged MCMC algorithms with burn-in, thinning, and parallel chains. The kernel should save acceptances via state for diagnostics.</p> Note <p>The mcmc_kernel should use save(accept=...) to record acceptances for proper diagnostics collection.</p> Source code in <code>src/genjax/inference/mcmc.py</code> <pre><code>def chain(mcmc_kernel: MCMCKernel):\n    \"\"\"\n    Higher-order function that creates MCMC chain algorithms from simple kernels.\n\n    This function transforms simple MCMC moves (like metropolis_hastings_step)\n    into full-fledged MCMC algorithms with burn-in, thinning, and parallel chains.\n    The kernel should save acceptances via state for diagnostics.\n\n    Args:\n        mcmc_kernel: MCMC kernel function that takes and returns a trace\n\n    Returns:\n        Function that runs MCMC chains with burn-in, thinning, and diagnostics\n\n    Note:\n        The mcmc_kernel should use save(accept=...) to record acceptances\n        for proper diagnostics collection.\n    \"\"\"\n\n    def run_chain(\n        initial_trace: Trace[X, R],\n        n_steps: Const[int],\n        *,\n        burn_in: Const[int] = const(0),\n        autocorrelation_resampling: Const[int] = const(1),\n        n_chains: Const[int] = const(1),\n    ) -&gt; MCMCResult:\n        \"\"\"\n        Run MCMC chain with the configured kernel.\n\n        Args:\n            initial_trace: Starting trace\n            n_steps: Total number of steps to run (before burn-in/thinning)\n            burn_in: Number of initial steps to discard as burn-in\n            autocorrelation_resampling: Keep every N-th sample (thinning)\n            n_chains: Number of parallel chains to run\n\n        Returns:\n            MCMCResult with traces, acceptances, and diagnostics\n        \"\"\"\n\n        def scan_fn(trace, _):\n            new_trace = mcmc_kernel(trace)\n            return new_trace, new_trace\n\n        if n_chains.value == 1:\n            # Single chain case\n            @state  # Use state decorator to collect acceptances\n            def run_scan():\n                final_trace, all_traces = jax.lax.scan(\n                    scan_fn, initial_trace, jnp.arange(n_steps.value)\n                )\n                return all_traces\n\n            # Run chain and collect state (including accepts)\n            all_traces, chain_state = run_scan()\n\n            # Extract accepts from state\n            accepts = chain_state.get(\"accept\", jnp.zeros(n_steps.value))\n\n            # Apply burn-in and thinning\n            start_idx = burn_in.value\n            end_idx = n_steps.value\n            indices = jnp.arange(start_idx, end_idx, autocorrelation_resampling.value)\n\n            # Apply selection to traces and accepts\n            final_traces = jax.tree_util.tree_map(\n                lambda x: x[indices] if hasattr(x, \"shape\") and len(x.shape) &gt; 0 else x,\n                all_traces,\n            )\n            final_accepts = accepts[indices]\n\n            # Compute final acceptance rate\n            acceptance_rate = jnp.mean(final_accepts)\n            final_n_steps = len(indices)\n\n            return MCMCResult(\n                traces=final_traces,\n                accepts=final_accepts,\n                acceptance_rate=acceptance_rate,\n                n_steps=const(final_n_steps),\n                n_chains=n_chains,\n            )\n\n        else:\n            # Multiple chains case - use vmap to run parallel chains\n            # Vectorize the scan function over chains\n            vectorized_run = modular_vmap(\n                lambda trace: run_chain(\n                    trace,\n                    n_steps,\n                    burn_in=burn_in,\n                    autocorrelation_resampling=autocorrelation_resampling,\n                    n_chains=const(1),  # Each vectorized call runs 1 chain\n                ),\n                in_axes=0,\n            )\n\n            # Create multiple initial traces by repeating the single trace\n            # This creates independent starting points\n            initial_traces = jax.tree_util.tree_map(\n                lambda x: jnp.repeat(x[None, ...], n_chains.value, axis=0),\n                initial_trace,\n            )\n\n            # Run multiple chains in parallel\n            multi_chain_results = vectorized_run(initial_traces)\n\n            # Combine results from multiple chains\n            # Traces shape: (n_chains, n_steps, ...)\n            combined_traces = multi_chain_results.traces\n            combined_accepts = multi_chain_results.accepts  # (n_chains, n_steps)\n\n            # Per-chain acceptance rates\n            acceptance_rates = jnp.mean(combined_accepts, axis=1)  # (n_chains,)\n            overall_acceptance_rate = jnp.mean(acceptance_rates)\n\n            final_n_steps = multi_chain_results.n_steps.value\n\n            # Compute between-chain diagnostics using Pytree utilities\n            rhat_values = None\n            ess_bulk_values = None\n            ess_tail_values = None\n\n            if n_chains.value &gt; 1:\n                # Extract choices for diagnostics computation\n                choices = combined_traces.get_choices()\n\n                # Helper function to compute all diagnostics for scalar arrays\n                def compute_all_diagnostics(samples):\n                    \"\"\"Compute all diagnostics if samples are scalar over (chains, steps).\"\"\"\n                    if samples.ndim == 2:  # (n_chains, n_steps) - scalar samples\n                        rhat_val = compute_rhat(samples)\n                        ess_bulk_val = compute_ess(samples, kind=\"bulk\")\n                        ess_tail_val = compute_ess(samples, kind=\"tail\")\n                        # Return as JAX array so we can index into it\n                        return jnp.array([rhat_val, ess_bulk_val, ess_tail_val])\n                    else:\n                        # For non-scalar arrays, return NaN for all diagnostics\n                        return jnp.array([jnp.nan, jnp.nan, jnp.nan])\n\n                # Compute all diagnostics in one tree_map pass\n                all_diagnostics = jax.tree_util.tree_map(\n                    compute_all_diagnostics, choices\n                )\n\n                # Extract individual diagnostics using indexing\n                rhat_values = jax.tree_util.tree_map(lambda x: x[0], all_diagnostics)\n                ess_bulk_values = jax.tree_util.tree_map(\n                    lambda x: x[1], all_diagnostics\n                )\n                ess_tail_values = jax.tree_util.tree_map(\n                    lambda x: x[2], all_diagnostics\n                )\n\n            return MCMCResult(\n                traces=combined_traces,\n                accepts=combined_accepts,\n                acceptance_rate=overall_acceptance_rate,\n                n_steps=const(final_n_steps),\n                n_chains=n_chains,\n                rhat=rhat_values,\n                ess_bulk=ess_bulk_values,\n                ess_tail=ess_tail_values,\n            )\n\n    return run_chain\n</code></pre>"},{"location":"reference/mcmc/#available-algorithms","title":"Available Algorithms","text":""},{"location":"reference/mcmc/#metropolis_hastings","title":"metropolis_hastings","text":"<p>Basic Metropolis-Hastings algorithm with custom proposals.</p>"},{"location":"reference/mcmc/#hmc","title":"hmc","text":"<p>Hamiltonian Monte Carlo for efficient exploration of continuous spaces.</p>"},{"location":"reference/mcmc/#mala","title":"mala","text":"<p>Metropolis-Adjusted Langevin Algorithm for gradient-informed proposals.</p>"},{"location":"reference/mcmc/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/mcmc/#metropolis-hastings","title":"Metropolis-Hastings","text":"<pre><code>from genjax.inference.mcmc import metropolis_hastings\nfrom genjax import select\n\n# Define selection of variables to update\nselection = select(\"mu\", \"sigma\")\n\n# Single MH step\nnew_trace = metropolis_hastings(trace, selection, key)\n\n# Run MCMC chain\ndef mcmc_step(carry, key):\n    trace = carry\n    new_trace = metropolis_hastings(trace, selection, key)\n    return new_trace, new_trace[\"mu\"]\n\nkeys = jax.random.split(key, 1000)\nfinal_trace, samples = jax.lax.scan(mcmc_step, initial_trace, keys)\n</code></pre>"},{"location":"reference/mcmc/#hamiltonian-monte-carlo","title":"Hamiltonian Monte Carlo","text":"<pre><code>from genjax.inference.mcmc import hmc\n\n# HMC with custom parameters\nnew_trace = hmc(\n    trace, \n    selection,\n    key,\n    step_size=0.01,\n    num_leapfrog_steps=10\n)\n</code></pre>"},{"location":"reference/mcmc/#best-practices","title":"Best Practices","text":"<ol> <li>Warm-up Period: Discard initial samples during burn-in</li> <li>Thinning: Keep every nth sample to reduce autocorrelation</li> <li>Multiple Chains: Run parallel chains for convergence diagnostics</li> <li>Adaptive Step Size: Tune step sizes during warm-up for HMC</li> </ol>"},{"location":"reference/pjax/","title":"genjax.pjax","text":"<p>Probabilistic JAX (PJAX) - foundational probabilistic programming primitives.</p>"},{"location":"reference/pjax/#genjax.pjax","title":"pjax","text":"<p>PJAX: Probabilistic JAX</p> <p>This module implements PJAX (Probabilistic JAX), which extends JAX with probabilistic primitives and specialized interpreters for handling probabilistic computations.</p> <p>PJAX provides the foundational infrastructure for GenJAX's probabilistic programming capabilities by introducing:</p> <ol> <li> <p>Probabilistic Primitives: Custom JAX primitives (<code>sample_p</code>, <code>log_density_p</code>)    that represent random sampling and density evaluation operations.</p> </li> <li> <p>JAX-aware Interpreters: Specialized interpreters that handle probabilistic    primitives while preserving JAX's transformation semantics:</p> </li> <li><code>Seed</code>: Eliminates PJAX's sampling primitive for JAX PRNG implementations</li> <li> <p><code>ModularVmap</code>: Vectorizes probabilistic computations</p> </li> <li> <p>Staging Infrastructure: Tools for converting Python functions to JAX's    intermediate representation (Jaxpr) while preserving probabilistic semantics.</p> </li> </ol> Key Concepts <ul> <li>Assume Primitive: Represents random sampling operations in Jaxpr</li> <li>Seed Transformation: Converts probabilistic functions to accept explicit keys</li> <li>Modular Vmap: Vectorizes probabilistic functions while preserving semantics</li> <li>Elaborated Primitives: Enhanced primitives with metadata for pretty printing</li> </ul> Keyful Sampler Contract <p>All samplers used with PJAX must follow this signature contract:</p> <pre><code>def keyful_sampler(key: PRNGKey, *args, sample_shape: tuple[int, ...], **kwargs) -&gt; Array:\n    '''Sample from a distribution.\n\n    Args:\n        key: JAX PRNGKey for randomness\n        *args: Distribution parameters (positional)\n        sample_shape: REQUIRED keyword argument specifying sample shape\n        **kwargs: Additional distribution parameters (keyword)\n\n    Returns:\n        Array with shape sample_shape + distribution.event_shape\n    '''\n</code></pre> <p>The sample_shape parameter is REQUIRED and must be accepted as a keyword argument. This ensures compatibility with JAX/TensorFlow Probability conventions.</p> Usage <pre><code>from genjax.pjax import seed, modular_vmap, sample_binder\nimport tensorflow_probability.substrates.jax as tfp\n\n# Create a keyful sampler following the contract\ndef normal_sampler(key, loc, scale, sample_shape=(), **kwargs):\n    dist = tfp.distributions.Normal(loc, scale)\n    return dist.sample(seed=key, sample_shape=sample_shape)\n\n# Bind to PJAX primitive\nnormal = sample_binder(normal_sampler, name=\"normal\")\n\n# Transform probabilistic function to accept explicit keys\nseeded_fn = seed(probabilistic_function)\nresult = seeded_fn(key, args)\n\n# Vectorize probabilistic computations\nvmap_fn = modular_vmap(probabilistic_function, in_axes=(0,))\nresults = vmap_fn(batched_args)\n</code></pre> Technical Details <p>PJAX works by representing sampling and density evaluation as JAX primitives that can be interpreted differently depending on the transformation applied. The <code>seed</code> transformation eliminates the sampling primitive by providing explicit randomness, while <code>modular_vmap</code> preserves both primitives for probability-aware vectorization.</p> References <ul> <li>JAX Primitives: https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html</li> <li>GenJAX Documentation: See src/genjax/CLAUDE.md for PJAX usage patterns</li> </ul>"},{"location":"reference/pjax/#genjax.pjax.sample_p","title":"sample_p  <code>module-attribute</code>","text":"<pre><code>sample_p = InitialStylePrimitive(f'{BOLD}{CYAN}pjax.sample{RESET}')\n</code></pre> <p>Core primitive representing random sampling operations.</p> <p><code>sample_p</code> is the fundamental primitive in PJAX that represents the act of drawing a random sample from a probability distribution. It appears in Jaxpr when probabilistic functions are staged, and different interpreters handle it in different ways:</p> <ul> <li><code>Seed</code>: Replaces with actual sampling using provided PRNG key</li> <li><code>ModularVmap</code>: Vectorizes the sampling operation</li> <li>Standard JAX: Raises warning/exception (requires transformation)</li> </ul> <p>The primitive carries metadata about the sampler function, distribution parameters, and optional support constraints.</p>"},{"location":"reference/pjax/#genjax.pjax.log_density_p","title":"log_density_p  <code>module-attribute</code>","text":"<pre><code>log_density_p = InitialStylePrimitive(f'{BOLD}{CYAN}pjax.log_density{RESET}')\n</code></pre> <p>Core primitive representing log-density evaluation operations.</p> <p><code>log_density_p</code> represents the evaluation of log probability density at a given value. This is dual to <code>sample_p</code> - while <code>sample_p</code> generates samples, <code>log_density_p</code> evaluates how likely those samples are under the distribution.</p> <p>Used primarily in: - Density evaluation for inference algorithms - Gradient computation in variational methods - Importance weight calculations in SMC</p>"},{"location":"reference/pjax/#genjax.pjax.enforce_lowering_exception","title":"enforce_lowering_exception  <code>module-attribute</code>","text":"<pre><code>enforce_lowering_exception = True\n</code></pre> <p>Whether to raise exceptions when sample_p primitives reach MLIR lowering.</p> <p>When True, attempting to compile probabilistic functions (e.g., with jax.jit) without first applying <code>seed()</code> will raise a LoweringSamplePrimitiveToMLIRException. This prevents silent errors where PRNG keys get baked into compiled code.</p> <p>Set to False for debugging or if you want warnings instead of exceptions.</p>"},{"location":"reference/pjax/#genjax.pjax.lowering_warning","title":"lowering_warning  <code>module-attribute</code>","text":"<pre><code>lowering_warning = False\n</code></pre> <p>Whether to show warnings when sample_p primitives reach MLIR lowering.</p> <p>When True, shows warning messages instead of raising exceptions when probabilistic primitives reach compilation without proper transformation. Generally, exceptions (enforce_lowering_exception=True) are preferred as they prevent subtle bugs.</p>"},{"location":"reference/pjax/#genjax.pjax.InitialStylePrimitive","title":"InitialStylePrimitive","text":"<pre><code>InitialStylePrimitive(name)\n</code></pre> <p>               Bases: <code>Primitive</code></p> <p>JAX primitive with configurable transformation implementations.</p> <p>This class extends JAX's <code>Primitive</code> to provide a convenient way to define custom primitives where transformation semantics are provided at the binding site rather than registration time. This is essential for PJAX's dynamic primitive creation where the same primitive can have different behaviors depending on the probabilistic context.</p> <p>The primitive expects implementations for JAX transformations to be provided as parameters during <code>initial_style_bind(...)</code> calls using these keys:</p> Transformation Keys <ul> <li><code>impl</code>: Evaluation semantics - the concrete implementation that executes          when the primitive is evaluated with concrete values</li> <li><code>abstract</code>: Abstract semantics - used by JAX when tracing a Python program              to a Jaxpr; determines output shapes and dtypes from input abstract values</li> <li><code>jvp</code>: Forward-mode automatic differentiation - defines how to compute         Jacobian-vector products for this primitive</li> <li><code>batch</code>: Vectorization semantics for <code>vmap</code> - defines how the primitive           behaves when vectorized over a batch dimension</li> <li><code>lowering</code>: Compilation semantics for <code>jit</code> - defines how to lower the              primitive to MLIR for XLA compilation</li> </ul> Technical Details <p>Unlike standard JAX primitives where transformation rules are registered once, InitialStylePrimitive defers all rule definitions to binding time. The primitive acts as a parameterizable template where transformation semantics are injected dynamically, enabling PJAX's context-dependent reinterpretation of probabilistic operations.</p> Example <pre><code>my_primitive = InitialStylePrimitive(\"my_op\")\n\n# Transformation semantics provided at binding time\nresult = my_primitive.bind(\n    inputs,\n    impl=lambda x: x + 1,                    # Evaluation: add 1\n    abstract=lambda aval: aval,              # Same shape/dtype\n    jvp=lambda primals, tangents: (primals[0] + 1, tangents[0]),\n    batch=lambda args, dim: (args[0] + 1,),  # Vectorized add\n    lowering=my_lowering_rule\n)\n</code></pre> Source code in <code>src/genjax/pjax.py</code> <pre><code>def __init__(self, name):\n    super(InitialStylePrimitive, self).__init__(name)\n    self.multiple_results = True\n\n    def impl(*flat_args, **params):\n        return params[\"impl\"](*flat_args, **params)\n\n    def abstract(*flat_avals, **params):\n        return params[\"abstract\"](*flat_avals, **params)\n\n    def jvp(\n        flat_primals: tuple[Any, ...] | list[Any],\n        flat_tangents: tuple[Any, ...] | list[Any],\n        **params,\n    ) -&gt; tuple[list[Any], list[Any]]:\n        return params[\"jvp\"](flat_primals, flat_tangents, **params)\n\n    def batch(flat_vector_args: tuple[Any, ...] | list[Any], dim, **params):\n        return params[\"batch\"](flat_vector_args, dim, **params)\n\n    def lowering(ctx: mlir.LoweringRuleContext, *mlir_args, **params):\n        if \"lowering_warning\" in params and lowering_warning:\n            warnings.warn(params[\"lowering_warning\"])\n        elif \"lowering_exception\" in params and enforce_lowering_exception:\n            raise params[\"lowering_exception\"]\n        lowering = mlir.lower_fun(self.impl, multiple_results=True)\n        return lowering(ctx, *mlir_args, **params)\n\n    # Store for elaboration.\n    self.impl = impl\n    self.jvp = jvp\n    self.abstract = abstract\n    self.batch = batch\n    self.lowering = lowering\n\n    self.def_impl(impl)\n    ad.primitive_jvps[self] = jvp\n    self.def_abstract_eval(abstract)\n    batching.primitive_batchers[self] = batch\n    mlir.register_lowering(self, lowering)\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.PPPrimitive","title":"PPPrimitive","text":"<pre><code>PPPrimitive(prim: InitialStylePrimitive, **params)\n</code></pre> <p>               Bases: <code>Primitive</code></p> <p>A primitive wrapper that hides metadata from JAX's Jaxpr pretty printer.</p> <p><code>PPPrimitive</code> (Pretty Print Primitive) wraps an underlying InitialStylePrimitive and stores metadata parameters in a hidden field to prevent them from cluttering JAX's Jaxpr pretty printer output. PJAX's probabilistic primitives carry complex metadata (samplers, distributions, transformation rules, etc.) that would make Jaxpr representations unreadable if displayed.</p> <p>The wrapper: - Stores the underlying primitive and its parameters in a private field - Hides metadata from JAX's Jaxpr pretty printer - Acts as a transparent proxy for all JAX transformations - Preserves all transformation behavior of the wrapped primitive</p> Technical Details <p>When JAX creates a Jaxpr representation, it only shows the primitive name and visible parameters. By storing metadata in the PPPrimitive's internal state rather than as binding parameters, we get clean Jaxpr output while preserving all the functionality and metadata needed for transformations.</p> Example <pre><code>base_prim = InitialStylePrimitive(\"sample\")\n\n# Without PPPrimitive: cluttered Jaxpr with all metadata visible\n# sample[impl=&lt;function&gt;, abstract=&lt;function&gt;, distribution=\"normal\", ...]\n\n# With PPPrimitive: clean Jaxpr output\npretty_prim = PPPrimitive(base_prim, distribution=\"normal\", name=\"x\")\n# Jaxpr shows: sample\n\nresult = pretty_prim.bind(args, mu=0.0, sigma=1.0)\n</code></pre> Source code in <code>src/genjax/pjax.py</code> <pre><code>def __init__(self, prim: InitialStylePrimitive, **params):\n    super(PPPrimitive, self).__init__(prim.name)\n    self.prim = prim\n    self.multiple_results = self.prim.multiple_results\n    self.params = params\n\n    def impl(*args, **params):\n        return self.prim.impl(*args, **self.params, **params)\n\n    def abstract(*args, **params):\n        return self.prim.abstract(*args, **self.params, **params)\n\n    def jvp(*args, **params):\n        return self.prim.jvp(*args, **self.params, **params)\n\n    def batch(*args, **params):\n        return self.prim.batch(*args, **self.params, **params)\n\n    def lowering(*args, **params):\n        return self.prim.lowering(*args, **self.params, **params)\n\n    self.def_impl(impl)\n    ad.primitive_jvps[self] = jvp\n    self.def_abstract_eval(abstract)\n    batching.primitive_batchers[self] = batch\n    mlir.register_lowering(self, lowering)\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.TerminalStyle","title":"TerminalStyle","text":"<p>ANSI terminal styling for pretty-printed primitives.</p>"},{"location":"reference/pjax/#genjax.pjax.LoweringSamplePrimitiveToMLIRException","title":"LoweringSamplePrimitiveToMLIRException","text":"<pre><code>LoweringSamplePrimitiveToMLIRException(lowering_msg: str, binding_context: dict | None = None)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when PJAX sample_p primitives reach MLIR lowering.</p> <p>This exception occurs when probabilistic functions containing sample_p primitives are passed to JAX transformations (like jit, grad, vmap) without first applying the <code>seed()</code> transformation. This prevents silent errors where PRNG keys get baked into compiled code, leading to deterministic behavior.</p> <p>The exception includes execution context to help identify where the problematic binding occurred in the user's code.</p> <p>Initialize the exception with lowering message and binding context.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def __init__(self, lowering_msg: str, binding_context: dict | None = None):\n    \"\"\"Initialize the exception with lowering message and binding context.\n\n    Args:\n        lowering_msg: The core error message about why this is problematic\n        binding_context: Dictionary containing execution context information\n    \"\"\"\n    self.lowering_msg = lowering_msg\n    self.binding_context = binding_context or {}\n\n    # Create a comprehensive error message\n    full_message = self._format_full_message()\n    super().__init__(full_message)\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.SamplerConfig","title":"SamplerConfig  <code>dataclass</code>","text":"<pre><code>SamplerConfig(keyful_sampler: Callable[..., Any], name: str | None = None, sample_shape: tuple[int, ...] = (), support: Callable[..., Any] | None = None)\n</code></pre> <p>Configuration for a probabilistic sampler.</p> <p>Encapsulates all the information needed to create and execute a sampler, making the relationships between different components explicit.</p> Keyful Sampler Contract <p>The keyful_sampler must follow this exact signature:</p> <p>def keyful_sampler(key: PRNGKey, args, sample_shape: tuple[int, ...], *kwargs) -&gt; Array:     '''Sample from a distribution.</p> <pre><code>Args:\n    key: JAX PRNGKey for randomness\n    *args: Distribution parameters (positional)\n    sample_shape: REQUIRED keyword argument specifying sample shape\n    **kwargs: Additional distribution parameters (keyword)\n\nReturns:\n    Array with shape sample_shape + distribution.event_shape\n'''\n</code></pre> <p>The sample_shape parameter is REQUIRED and must be accepted as a keyword argument. This ensures compatibility with JAX/TensorFlow Probability conventions and enables proper vectorization under PJAX's modular_vmap.</p>"},{"location":"reference/pjax/#genjax.pjax.SamplerConfig.with_sample_shape","title":"with_sample_shape","text":"<pre><code>with_sample_shape(new_sample_shape: tuple[int, ...]) -&gt; SamplerConfig\n</code></pre> <p>Create a new config with updated sample shape.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def with_sample_shape(self, new_sample_shape: tuple[int, ...]) -&gt; \"SamplerConfig\":\n    \"\"\"Create a new config with updated sample shape.\"\"\"\n    return SamplerConfig(\n        keyful_sampler=self.keyful_sampler,\n        name=self.name,\n        sample_shape=new_sample_shape,\n        support=self.support,\n    )\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.SamplerConfig.get_keyful_sampler_with_shape","title":"get_keyful_sampler_with_shape","text":"<pre><code>get_keyful_sampler_with_shape() -&gt; Callable[..., Any]\n</code></pre> <p>Get the keyful sampler with sample_shape pre-applied.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def get_keyful_sampler_with_shape(self) -&gt; Callable[..., Any]:\n    \"\"\"Get the keyful sampler with sample_shape pre-applied.\"\"\"\n    return partial(self.keyful_sampler, sample_shape=self.sample_shape)\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.KeylessWrapper","title":"KeylessWrapper","text":"<pre><code>KeylessWrapper(config: SamplerConfig)\n</code></pre> <p>Wrapper that provides keyless sampling interface using global counter.</p> <p>This encapsulates the \"cheeky\" global counter pattern and makes it explicit that this is a convenience wrapper with known limitations.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def __init__(self, config: SamplerConfig):\n    self.config = config\n    self._keyful_with_shape = config.get_keyful_sampler_with_shape()\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.FlatSamplerCache","title":"FlatSamplerCache","text":"<pre><code>FlatSamplerCache(config: SamplerConfig)\n</code></pre> <p>Manages the flattened version of samplers for JAX interpretation.</p> <p>JAX interpreters work with flattened argument lists, so we need to pre-compute a flattened version of the sampler for efficiency.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def __init__(self, config: SamplerConfig):\n    self.config = config\n    self._flat_sampler = None\n    self._cached_args_signature = None\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.FlatSamplerCache.get_flat_sampler","title":"get_flat_sampler","text":"<pre><code>get_flat_sampler(*args, **kwargs)\n</code></pre> <p>Get or create the flattened sampler for these arguments.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def get_flat_sampler(self, *args, **kwargs):\n    \"\"\"Get or create the flattened sampler for these arguments.\"\"\"\n    # Simple caching based on argument signature\n    args_sig = (len(args), tuple(kwargs.keys()))\n    if self._cached_args_signature != args_sig:\n        keyful_with_shape = self.config.get_keyful_sampler_with_shape()\n        flat_sampler, _ = self._make_flat(keyful_with_shape)(\n            _fake_key, *args, **kwargs\n        )\n        self._flat_sampler = flat_sampler\n        self._cached_args_signature = args_sig\n    return self._flat_sampler\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.VmapBatchHandler","title":"VmapBatchHandler","text":"<pre><code>VmapBatchHandler(config: SamplerConfig)\n</code></pre> <p>Handles the complex vmap batching logic for probabilistic primitives.</p> <p>This encapsulates the logic for how sample shapes change under vmap and how primitives get rebound with new sample shapes.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def __init__(self, config: SamplerConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.VmapBatchHandler.create_batch_rule","title":"create_batch_rule","text":"<pre><code>create_batch_rule()\n</code></pre> <p>Create the batch rule function for this sampler.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def create_batch_rule(self):\n    \"\"\"Create the batch rule function for this sampler.\"\"\"\n\n    def batch_rule(vector_args, batch_axes, **params):\n        if \"ctx\" in params and params[\"ctx\"] == \"modular_vmap\":\n            return self._handle_modular_vmap(vector_args, batch_axes, **params)\n        else:\n            raise NotImplementedError(\"Only modular_vmap context supported\")\n\n    return batch_rule\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.LogDensityConfig","title":"LogDensityConfig  <code>dataclass</code>","text":"<pre><code>LogDensityConfig(log_density_impl: Callable[..., Any], name: str | None = None)\n</code></pre> <p>Configuration for a log density function.</p> <p>Encapsulates all the information needed to create and execute a log density primitive, following the same component-based architecture as SamplerConfig.</p> Log Density Function Contract <p>The log_density_impl must follow this signature:</p> <p>def log_density_impl(value, args, *kwargs) -&gt; float:     '''Compute log probability density.</p> <pre><code>Args:\n    value: The value to evaluate density at\n    *args: Distribution parameters (positional)\n    **kwargs: Additional distribution parameters (keyword)\n\nReturns:\n    Scalar log probability density\n'''\n</code></pre> <p>Log density functions always return scalars - there is no sample_shape concept.</p>"},{"location":"reference/pjax/#genjax.pjax.LogDensityVmapHandler","title":"LogDensityVmapHandler","text":"<pre><code>LogDensityVmapHandler(config: LogDensityConfig)\n</code></pre> <p>Handles the complex vmap batching logic for log density primitives.</p> <p>This encapsulates the logic for how log density functions get vectorized under vmap, handling both args-only and args+kwargs cases.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def __init__(self, config: LogDensityConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.LogDensityVmapHandler.create_batch_rule","title":"create_batch_rule","text":"<pre><code>create_batch_rule()\n</code></pre> <p>Create the batch rule function for this log density function.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def create_batch_rule(self):\n    \"\"\"Create the batch rule function for this log density function.\"\"\"\n\n    def batch_rule(vector_args, batch_axes, **params):\n        n = static_dim_length(batch_axes, tuple(vector_args))\n        num_consts = params[\"num_consts\"]\n        in_tree = jtu.tree_unflatten(params[\"in_tree\"], vector_args[num_consts:])\n        batch_tree = jtu.tree_unflatten(params[\"in_tree\"], batch_axes[num_consts:])\n\n        if params[\"yes_kwargs\"]:\n            args = in_tree[0]\n            kwargs = in_tree[1]\n            v = create_log_density_primitive(\n                LogDensityConfig(\n                    log_density_impl=jax.vmap(\n                        lambda args, kwargs: self.config.log_density_impl(\n                            *args, **kwargs\n                        ),\n                        in_axes=batch_tree,\n                    ),\n                    name=self.config.name,\n                )\n            )(args, kwargs)\n        else:\n            v = create_log_density_primitive(\n                LogDensityConfig(\n                    log_density_impl=jax.vmap(\n                        self.config.log_density_impl,\n                        in_axes=batch_tree,\n                    ),\n                    name=self.config.name,\n                )\n            )(*in_tree)\n\n        outvals = (v,)\n        out_axes = (0 if n else None,)\n        return outvals, out_axes\n\n    return batch_rule\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.Environment","title":"Environment  <code>dataclass</code>","text":"<pre><code>Environment(env: dict[int, Any] = dict())\n</code></pre> <p>Variable environment for Jaxpr interpretation.</p> <p>Manages the mapping between JAX variables (from Jaxpr) and their concrete values during interpretation. This is essential for interpreters that need to execute Jaxpr equations step-by-step while maintaining state.</p> <p>The environment handles both: - Var objects: Variables with unique identifiers - Literal objects: Constant values embedded in the Jaxpr</p> <p>This design enables efficient interpretation of probabilistic Jaxpr by PJAX's specialized interpreters.</p>"},{"location":"reference/pjax/#genjax.pjax.Environment.read","title":"read","text":"<pre><code>read(var: VarOrLiteral) -&gt; Any\n</code></pre> <p>Read a value from a variable in the environment.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def read(self, var: VarOrLiteral) -&gt; Any:\n    \"\"\"\n    Read a value from a variable in the environment.\n    \"\"\"\n    v = self.get(var)\n    if v is None:\n        assert isinstance(var, Var)\n        raise ValueError(\n            f\"Unbound variable in interpreter environment at count {var.count}:\\nEnvironment keys (count): {list(self.env.keys())}\"\n        )\n    return v\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.Environment.write","title":"write","text":"<pre><code>write(var: VarOrLiteral, cell: Any) -&gt; Any\n</code></pre> <p>Write a value to a variable in the environment.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def write(self, var: VarOrLiteral, cell: Any) -&gt; Any:\n    \"\"\"\n    Write a value to a variable in the environment.\n    \"\"\"\n    if isinstance(var, Literal):\n        return cell\n    cur_cell = self.get(var)\n    if isinstance(var, jc.DropVar):\n        return cur_cell\n    self.env[var.count] = cell\n    return self.env[var.count]\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.Environment.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p><code>Environment.copy</code> is sometimes used to create a new environment with the same variables and values as the original, especially in CPS interpreters (where a continuation closes over the application of an interpreter to a <code>Jaxpr</code>).</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def copy(self):\n    \"\"\"\n    `Environment.copy` is sometimes used to create a new environment with\n    the same variables and values as the original, especially in CPS\n    interpreters (where a continuation closes over the application of an\n    interpreter to a `Jaxpr`).\n    \"\"\"\n    keys = list(self.env.keys())\n    return Environment({k: self.env[k] for k in keys})\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.Seed","title":"Seed  <code>dataclass</code>","text":"<pre><code>Seed(key: PRNGKey)\n</code></pre> <p>Interpreter that eliminates probabilistic primitives with explicit randomness.</p> <p>The <code>Seed</code> interpreter is PJAX's core mechanism for making probabilistic computations compatible with standard JAX transformations. It works by traversing a Jaxpr and replacing <code>sample_p</code> primitives with actual sampling operations using explicit PRNG keys.</p> <p>Key Features: - Eliminates PJAX primitives: Converts sample_p to concrete sampling - Explicit randomness: Uses provided PRNG key for all random operations - JAX compatibility: Output can be jit'd, vmap'd, grad'd normally - Deterministic: Same key produces same results (good for debugging) - Hierarchical key splitting: Automatically manages keys for nested operations</p> <p>The interpreter handles JAX control flow primitives (cond, scan) by recursively applying the seed transformation to their sub-computations.</p> Usage Pattern <p>This interpreter is primarily used via the <code>seed()</code> transformation:</p> <pre><code># Instead of using the interpreter directly:\n# interpreter = Seed(key)\n# result = interpreter.eval(fn, args)\n\n# Use the seed transformation:\nseeded_fn = seed(fn)\nresult = seeded_fn(key, args)\n</code></pre> Technical Details <p>The interpreter maintains a PRNG key that is split at each random operation, ensuring proper randomness while maintaining determinism. For control flow, it passes seeded versions of sub-computations to JAX's control primitives.</p>"},{"location":"reference/pjax/#genjax.pjax.ModularVmap","title":"ModularVmap  <code>dataclass</code>","text":"<pre><code>ModularVmap()\n</code></pre> <p>Vectorization interpreter that preserves probabilistic primitives.</p> <p>The <code>ModularVmap</code> interpreter extends JAX's <code>vmap</code> to handle PJAX's probabilistic primitives correctly. Unlike standard <code>vmap</code>, which isn't aware of PJAX primitives, this interpreter knows how to vectorize probabilistic operations while preserving their semantic meaning.</p> <p>Key Capabilities: - Probabilistic vectorization: Correctly handles <code>sample_p</code> under vmap - Sample shape inference: Automatically adjusts distribution sample shapes - Control flow support: Handles cond/scan within vectorized computations - Semantic preservation: Maintains probabilistic meaning across batches</p> How It Works <p>The interpreter uses a \"dummy argument\" technique to track the vectorization axis size and injects this information into probabilistic primitives so they can adjust their behavior appropriately (e.g., sampling multiple independent values vs. broadcasting parameters).</p> Usage <p>Primarily used via the <code>modular_vmap()</code> function:</p> <pre><code># Vectorize a probabilistic function\nbatch_fn = modular_vmap(prob_function, in_axes=(0,))\nbatch_results = batch_fn(batch_args)\n\n# Each element gets independent randomness\n# Distribution parameters are correctly broadcast\n</code></pre> Technical Details <p>The interpreter maintains PJAX primitives in the Jaxpr rather than eliminating them (unlike Seed). This allows proper vectorization semantics for probabilistic operations.</p>"},{"location":"reference/pjax/#genjax.pjax.get_shaped_aval","title":"get_shaped_aval","text":"<pre><code>get_shaped_aval(x)\n</code></pre> <p>Get the shaped abstract value of a JAX array.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def get_shaped_aval(x):\n    \"\"\"Get the shaped abstract value of a JAX array.\"\"\"\n    return jc.get_aval(x)\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.cached_stage_dynamic","title":"cached_stage_dynamic","text":"<pre><code>cached_stage_dynamic(flat_fun, in_avals)\n</code></pre> <p>Cache-enabled function to stage a flattened function to Jaxpr.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>@lu.cache\ndef cached_stage_dynamic(flat_fun, in_avals):\n    \"\"\"Cache-enabled function to stage a flattened function to Jaxpr.\n\n    Args:\n        flat_fun: Flattened function to stage.\n        in_avals: Input abstract values.\n\n    Returns:\n        ClosedJaxpr representing the function.\n    \"\"\"\n    jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(flat_fun, in_avals)\n    typed_jaxpr = ClosedJaxpr(jaxpr, consts)\n    return typed_jaxpr\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.stage","title":"stage","text":"<pre><code>stage(f, **params)\n</code></pre> <p>Stage a function to JAX's intermediate representation (Jaxpr).</p> <p>Converts a Python function into JAX's Jaxpr format, which enables interpretation and transformation of the function's computation graph. This is essential for PJAX's ability to inspect and transform probabilistic computations.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\nstaged_fn = stage(my_function)\njaxpr, metadata = staged_fn(1.0, 2.0)\n# jaxpr contains the computational graph\n# metadata contains argument information for reconstruction\n</code></pre> Source code in <code>src/genjax/pjax.py</code> <pre><code>def stage(f, **params):\n    \"\"\"Stage a function to JAX's intermediate representation (Jaxpr).\n\n    Converts a Python function into JAX's Jaxpr format, which enables\n    interpretation and transformation of the function's computation graph.\n    This is essential for PJAX's ability to inspect and transform\n    probabilistic computations.\n\n    Args:\n        f: Function to stage to JAX representation.\n        **params: Additional parameters to pass to the wrapped function.\n\n    Returns:\n        Callable that returns a tuple of (ClosedJaxpr, execution_metadata).\n        The execution metadata contains flattened arguments, input/output trees,\n        and tree reconstruction functions.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        staged_fn = stage(my_function)\n        jaxpr, metadata = staged_fn(1.0, 2.0)\n        # jaxpr contains the computational graph\n        # metadata contains argument information for reconstruction\n        ```\n    \"\"\"\n\n    @wraps(f)\n    def wrapped(\n        *args, **kwargs\n    ) -&gt; tuple[ClosedJaxpr, tuple[list[Any], Any, Callable[..., Any]]]:\n        debug_info = api_util.debug_info(\"genjax.stage\", f, args, kwargs)\n        fun = lu.wrap_init(f, params, debug_info=debug_info)\n        if kwargs:\n            flat_args, in_tree = jtu.tree_flatten((args, kwargs))\n            flat_fun, out_tree = api_util.flatten_fun(fun, in_tree)\n        else:\n            flat_args, in_tree = jtu.tree_flatten(args)\n            flat_fun, out_tree = api_util.flatten_fun_nokwargs(fun, in_tree)\n        flat_avals = safe_map(get_shaped_aval, flat_args)\n        closed_jaxpr = cached_stage_dynamic(flat_fun, tuple(flat_avals))\n        return closed_jaxpr, (flat_args, in_tree, out_tree)\n\n    return wrapped\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.initial_style_bind","title":"initial_style_bind","text":"<pre><code>initial_style_bind(prim, modular_vmap_aware=True, **params)\n</code></pre> <p>Binds a primitive to a function call.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def initial_style_bind(\n    prim,\n    modular_vmap_aware=True,\n    **params,\n):\n    \"\"\"Binds a primitive to a function call.\"\"\"\n\n    def bind(f, **elaboration_kwargs):\n        \"\"\"Wraps a function to be bound to a primitive, keeping track of Pytree\n        information.\"\"\"\n\n        def wrapped(*args, **kwargs):\n            \"\"\"Runs a function and binds it to a `PPPrimitive`\n            primitive, hiding the implementation details of the eval\n            (impl) rule, abstract rule, batch rule, and jvp rule.\"\"\"\n            jaxpr, (flat_args, in_tree, out_tree) = stage(f)(*args, **kwargs)\n            debug_info = jaxpr.jaxpr.debug_info\n\n            def impl(*flat_args, **params) -&gt; list[Any]:\n                consts, flat_args = split_list(flat_args, [params[\"num_consts\"]])\n                return jc.eval_jaxpr(jaxpr.jaxpr, consts, *flat_args)\n\n            def abstract(*flat_avals, **params):\n                if modular_vmap_aware:\n                    if \"ctx\" in params and params[\"ctx\"] == \"modular_vmap\":\n                        flat_avals = flat_avals[1:]  # ignore dummy\n                return pe.abstract_eval_fun(\n                    impl,\n                    *flat_avals,\n                    debug_info=debug_info,\n                    **params,\n                )\n\n            def batch(flat_vector_args: tuple[Any, ...] | list[Any], dims, **params):\n                axis_data = AxisData(None, None, None, None)\n                batched, out_dims = batch_fun(\n                    lu.wrap_init(impl, params, debug_info=debug_info),\n                    axis_data,\n                    dims,\n                )\n                return batched.call_wrapped(*flat_vector_args), out_dims()\n\n            def jvp(\n                flat_primals: tuple[Any, ...] | list[Any],\n                flat_tangents: tuple[Any, ...] | list[Any],\n                **params,\n            ) -&gt; tuple[list[Any], list[Any]]:\n                primals_out, tangents_out = ad.jvp(\n                    lu.wrap_init(impl, params, debug_info=debug_info)\n                ).call_wrapped(flat_primals, flat_tangents)\n\n                # We always normalize back to list.\n                return list(primals_out), list(tangents_out)\n\n            if \"impl\" in params:\n                impl = params[\"impl\"]\n                params.pop(\"impl\")\n\n            if \"abstract\" in params:\n                abstract = params[\"abstract\"]\n                params.pop(\"abstract\")\n\n            if \"batch\" in params:\n                batch = params[\"batch\"]\n                params.pop(\"batch\")\n\n            if \"jvp\" in params:\n                jvp = params[\"jvp\"]\n                params.pop(\"jvp\")\n\n            elaborated_prim = PPPrimitive(\n                prim,\n                impl=impl,\n                abstract=abstract,\n                batch=batch,\n                jvp=jvp,\n                in_tree=in_tree,\n                out_tree=out_tree,\n                num_consts=len(jaxpr.literals),\n                yes_kwargs=bool(kwargs),\n                **params,\n            )\n            outs = elaborated_prim.bind(\n                *it.chain(jaxpr.literals, flat_args),\n                **elaboration_kwargs,\n            )\n            return tree_util.tree_unflatten(out_tree(), outs)\n\n        return wrapped\n\n    return bind\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.create_log_density_primitive","title":"create_log_density_primitive","text":"<pre><code>create_log_density_primitive(config: LogDensityConfig)\n</code></pre> <p>Create a log density primitive from a log density configuration.</p> <p>This is the main entry point that orchestrates all the log density components.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def create_log_density_primitive(config: LogDensityConfig):\n    \"\"\"Create a log density primitive from a log density configuration.\n\n    This is the main entry point that orchestrates all the log density components.\n    \"\"\"\n    # Create the vmap batch handler\n    batch_handler = LogDensityVmapHandler(config)\n\n    def log_density(*args, **kwargs):\n        # Create batch rule\n        batch_rule = batch_handler.create_batch_rule()\n\n        return initial_style_bind(log_density_p, batch=batch_rule)(\n            config.log_density_impl, name=config.name\n        )(*args, **kwargs)\n\n    return log_density\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.log_density_binder","title":"log_density_binder","text":"<pre><code>log_density_binder(log_density_impl: Callable[..., Any], name: str | None = None)\n</code></pre> <p>Create a log density primitive using component-based architecture.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def log_density_binder(\n    log_density_impl: Callable[..., Any],\n    name: str | None = None,\n):\n    \"\"\"Create a log density primitive using component-based architecture.\n\n    Args:\n        log_density_impl: A function that computes log probability density:\n\n            def log_density_impl(value, *args, **kwargs) -&gt; float:\n                # Returns scalar log probability density\n\n            Log density functions always return scalars (no sample_shape concept).\n\n        name: Optional name for the primitive (for debugging)\n\n    Returns:\n        A callable that implements log density interface compatible with PJAX primitives.\n    \"\"\"\n    config = LogDensityConfig(\n        log_density_impl=log_density_impl,\n        name=name,\n    )\n    return create_log_density_primitive(config)\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.create_sample_primitive","title":"create_sample_primitive","text":"<pre><code>create_sample_primitive(config: SamplerConfig)\n</code></pre> <p>Create a sample primitive from a sampler configuration.</p> <p>This is the main entry point that orchestrates all the components. Replaces the current sample_binder function with clearer separation of concerns.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def create_sample_primitive(config: SamplerConfig):\n    \"\"\"Create a sample primitive from a sampler configuration.\n\n    This is the main entry point that orchestrates all the components.\n    Replaces the current sample_binder function with clearer separation of concerns.\n    \"\"\"\n    # Create the keyless wrapper for user convenience\n    keyless_sampler = KeylessWrapper(config)\n\n    # Create the flat sampler cache for JAX interpretation\n    flat_cache = FlatSamplerCache(config)\n\n    # Create the vmap batch handler\n    batch_handler = VmapBatchHandler(config)\n\n    def sample(*args, **kwargs):\n        # Get flat sampler for these arguments\n        flat_keyful_sampler = flat_cache.get_flat_sampler(*args, **kwargs)\n\n        # Create batch rule\n        batch_rule = batch_handler.create_batch_rule()\n\n        # Create lowering warning/exception\n        lowering_msg = (\n            \"JAX is attempting to lower the `pjax.sample_p` primitive to MLIR. \"\n            \"This will bake a PRNG key into the MLIR code, resulting in deterministic behavior. \"\n            \"Instead, use `seed` to transform your function into one which allows keys to be passed in. \"\n            \"Try and do this as high in the computation graph as you can.\"\n        )\n\n        # Capture execution context to help identify where the binding occurred\n        binding_context = _capture_binding_context(sampler_name=config.name)\n        lowering_exception = LoweringSamplePrimitiveToMLIRException(\n            lowering_msg, binding_context\n        )\n\n        # Bind to the primitive\n        return initial_style_bind(\n            sample_p,\n            keyful_sampler=config.keyful_sampler,\n            flat_keyful_sampler=flat_keyful_sampler,\n            batch=batch_rule,\n            support=config.support,\n            lowering_warning=lowering_msg,\n            lowering_exception=lowering_exception,\n        )(keyless_sampler, name=config.name)(*args, **kwargs)\n\n    return sample\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.sample_binder","title":"sample_binder","text":"<pre><code>sample_binder(keyful_sampler: Callable[..., Any], name: str | None = None, sample_shape: tuple[int, ...] = (), support: Callable[..., Any] | None = None)\n</code></pre> <p>Create a sample primitive that binds a keyful sampler to PJAX's sample_p primitive.</p> <p>Uses a component-based architecture to handle the complex interactions between keyless sampling, sample shapes, JAX flattening, and vmap transformations.</p> Note <p>The keyful_sampler MUST accept sample_shape as a keyword argument. This is required for compatibility with JAX transformations and proper vectorization.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def sample_binder(\n    keyful_sampler: Callable[..., Any],\n    name: str | None = None,\n    sample_shape: tuple[int, ...] = (),\n    support: Callable[..., Any] | None = None,\n):\n    \"\"\"Create a sample primitive that binds a keyful sampler to PJAX's sample_p primitive.\n\n    Uses a component-based architecture to handle the complex interactions between\n    keyless sampling, sample shapes, JAX flattening, and vmap transformations.\n\n    Args:\n        keyful_sampler: A function that follows the keyful sampler contract:\n\n            def keyful_sampler(key: PRNGKey, *args, sample_shape: tuple[int, ...], **kwargs) -&gt; Array:\n                # Must accept sample_shape as a REQUIRED keyword argument\n                # Returns array with shape: sample_shape + distribution.event_shape\n\n            Examples of valid keyful samplers:\n            - TFP distribution: lambda key, *args, sample_shape=(), **kw: dist(*args, **kw).sample(seed=key, sample_shape=sample_shape)\n            - JAX function: jax.random.normal with appropriate signature adaptation\n\n        name: Optional name for the primitive (for debugging)\n        sample_shape: Default sample shape to use if not overridden\n        support: Optional support function for the distribution\n\n    Returns:\n        A callable that implements keyless sampling interface compatible with PJAX primitives.\n        The returned function can be called as: sampler(*args, **kwargs) and will use\n        the global key counter for randomness.\n\n    Note:\n        The keyful_sampler MUST accept sample_shape as a keyword argument. This is\n        required for compatibility with JAX transformations and proper vectorization.\n    \"\"\"\n    config = SamplerConfig(\n        keyful_sampler=keyful_sampler,\n        name=name,\n        sample_shape=sample_shape,\n        support=support,\n    )\n    return create_sample_primitive(config)\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.wrap_logpdf","title":"wrap_logpdf","text":"<pre><code>wrap_logpdf(logpdf, name: str | None = None)\n</code></pre> <p>Wrap a log-density function to work with PJAX primitives.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def wrap_logpdf(\n    logpdf,\n    name: str | None = None,\n):\n    \"\"\"Wrap a log-density function to work with PJAX primitives.\n\n    Args:\n        logpdf: Function that computes log probability density.\n        name: Optional name for the operation (used in Jaxpr pretty-printing).\n\n    Returns:\n        Function that binds the logpdf to the log_density_p primitive.\n    \"\"\"\n\n    def _(v, *args, **kwargs):\n        return log_density_binder(\n            logpdf,\n            name=name,\n        )(v, *args, **kwargs)\n\n    return _\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.seed","title":"seed","text":"<pre><code>seed(f: Callable[..., Any])\n</code></pre> <p>Transform a function to accept an explicit PRNG key.</p> <p>This transformation eliminates probabilistic primitives by providing explicit randomness through a PRNG key, enabling the use of standard JAX transformations like jit and vmap.</p> Example <p>import jax.random as jrand from genjax import gen, normal, seed</p> <p>@gen ... def model(): ...     return normal(0.0, 1.0) @ \"x\"</p> <p>seeded_model = seed(model.simulate) key = jrand.key(0) trace = seeded_model(key)</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def seed(\n    f: Callable[..., Any],\n):\n    \"\"\"Transform a function to accept an explicit PRNG key.\n\n    This transformation eliminates probabilistic primitives by providing\n    explicit randomness through a PRNG key, enabling the use of standard\n    JAX transformations like jit and vmap.\n\n    Args:\n        f: Function containing probabilistic computations to transform.\n\n    Returns:\n        Function that takes a PRNGKey as first argument followed by\n        the original function arguments.\n\n    Example:\n        &gt;&gt;&gt; import jax.random as jrand\n        &gt;&gt;&gt; from genjax import gen, normal, seed\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @gen\n        ... def model():\n        ...     return normal(0.0, 1.0) @ \"x\"\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; seeded_model = seed(model.simulate)\n        &gt;&gt;&gt; key = jrand.key(0)\n        &gt;&gt;&gt; trace = seeded_model(key)\n    \"\"\"\n\n    @wraps(f)\n    def wrapped(key: PRNGKey, *args, **kwargs):\n        interpreter = Seed(key)\n        return interpreter.eval(\n            f,\n            *args,\n            **kwargs,\n        )\n\n    return wrapped\n</code></pre>"},{"location":"reference/pjax/#genjax.pjax.modular_vmap","title":"modular_vmap","text":"<pre><code>modular_vmap(f: Callable[..., R], in_axes: int | tuple[int | None, ...] | Sequence[Any] | None = 0, axis_size: int | None = None, axis_name: str | None = None, spmd_axis_name: str | None = None) -&gt; Callable[..., R]\n</code></pre> <p>Vectorize a function while preserving probabilistic semantics.</p> <p>This is PJAX's probabilistic-aware version of <code>jax.vmap</code>. Unlike standard <code>vmap</code>, which fails on probabilistic primitives, <code>modular_vmap</code> correctly handles probabilistic computations by preserving their semantic meaning across the vectorized dimension.</p> <p>Key Differences from <code>jax.vmap</code>: - Probabilistic awareness: Handles <code>sample_p</code> and <code>log_density_p</code> primitives - Sample shape handling: Automatically adjusts distribution sample shapes - Independent sampling: Each vectorized element gets independent randomness - Semantic correctness: Maintains probabilistic meaning across batches</p> Example <pre><code>import jax.random as jrand\nfrom genjax import normal, modular_vmap\n\ndef sample_normal(mu):\n    return normal.sample(mu, 1.0)  # Contains sample_p primitive\n\n# Vectorize over different means\nbatch_sample = modular_vmap(sample_normal, in_axes=(0,))\nmus = jnp.array([0.0, 1.0, 2.0])\nsamples = batch_sample(mus)  # Shape: (3,), independent samples\n\n# Compare with seed for JAX compatibility\nseeded_fn = seed(batch_sample)\nsamples = seeded_fn(key, mus)  # Can be jit'd, vmap'd, etc.\n</code></pre> Note <p>For JAX transformations (jit, grad, etc.), use <code>seed()</code> first: <code>jax.jit(seed(modular_vmap(f)))</code> rather than trying to jit the modular_vmap directly.</p> Source code in <code>src/genjax/pjax.py</code> <pre><code>def modular_vmap(\n    f: Callable[..., R],\n    in_axes: int | tuple[int | None, ...] | Sequence[Any] | None = 0,\n    axis_size: int | None = None,\n    axis_name: str | None = None,\n    spmd_axis_name: str | None = None,\n) -&gt; Callable[..., R]:\n    \"\"\"Vectorize a function while preserving probabilistic semantics.\n\n    This is PJAX's probabilistic-aware version of `jax.vmap`. Unlike standard\n    `vmap`, which fails on probabilistic primitives, `modular_vmap` correctly\n    handles probabilistic computations by preserving their semantic meaning\n    across the vectorized dimension.\n\n    Key Differences from `jax.vmap`:\n    - **Probabilistic awareness**: Handles `sample_p` and `log_density_p` primitives\n    - **Sample shape handling**: Automatically adjusts distribution sample shapes\n    - **Independent sampling**: Each vectorized element gets independent randomness\n    - **Semantic correctness**: Maintains probabilistic meaning across batches\n\n    Args:\n        f: Function to vectorize (may contain probabilistic operations).\n        in_axes: Axis specification for input arguments (same as jax.vmap).\n        axis_size: Size of the mapped axis (inferred if None).\n        axis_name: Name for the mapped axis (for debugging).\n        spmd_axis_name: SPMD axis name for parallel computation.\n\n    Returns:\n        Vectorized function that correctly handles probabilistic computations.\n\n    Example:\n        ```python\n        import jax.random as jrand\n        from genjax import normal, modular_vmap\n\n        def sample_normal(mu):\n            return normal.sample(mu, 1.0)  # Contains sample_p primitive\n\n        # Vectorize over different means\n        batch_sample = modular_vmap(sample_normal, in_axes=(0,))\n        mus = jnp.array([0.0, 1.0, 2.0])\n        samples = batch_sample(mus)  # Shape: (3,), independent samples\n\n        # Compare with seed for JAX compatibility\n        seeded_fn = seed(batch_sample)\n        samples = seeded_fn(key, mus)  # Can be jit'd, vmap'd, etc.\n        ```\n\n    Note:\n        For JAX transformations (jit, grad, etc.), use `seed()` first:\n        `jax.jit(seed(modular_vmap(f)))` rather than trying to jit\n        the modular_vmap directly.\n    \"\"\"\n\n    @wraps(f)\n    def wrapped(*args):\n        # Quickly throw if \"normal\" vmap would fail.\n        jax.vmap(\n            lambda *_: None,\n            in_axes=in_axes,\n            axis_size=axis_size,\n            axis_name=axis_name,\n            spmd_axis_name=spmd_axis_name,\n        )(*args)\n\n        interpreter = ModularVmap()\n        return interpreter.eval(\n            in_axes,\n            axis_size,\n            axis_name,\n            spmd_axis_name,\n            f,\n            *args,\n        )\n\n    return wrapped\n</code></pre>"},{"location":"reference/smc/","title":"genjax.inference.smc","text":"<p>Sequential Monte Carlo methods for particle-based inference.</p>"},{"location":"reference/smc/#genjax.inference.smc","title":"smc","text":"<p>Standard library of programmable inference algorithms for GenJAX.</p> <p>This module provides implementations of common inference algorithms that can be composed with generative functions through the GFI (Generative Function Interface). Uses GenJAX distributions and modular_vmap for efficient vectorized computation.</p> References <p>[1] P. D. Moral, A. Doucet, and A. Jasra, \"Sequential Monte Carlo samplers,\"     Journal of the Royal Statistical Society: Series B (Statistical Methodology),     vol. 68, no. 3, pp. 411\u2013436, 2006.</p>"},{"location":"reference/smc/#genjax.inference.smc.ParticleCollection","title":"ParticleCollection","text":"<p>               Bases: <code>Pytree</code></p> <p>Result of importance sampling containing traces, weights, and statistics.</p>"},{"location":"reference/smc/#genjax.inference.smc.ParticleCollection.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood() -&gt; jnp.ndarray\n</code></pre> <p>Estimate log marginal likelihood using importance sampling.</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def log_marginal_likelihood(self) -&gt; jnp.ndarray:\n    \"\"\"\n    Estimate log marginal likelihood using importance sampling.\n\n    Returns:\n        Log marginal likelihood estimate using log-sum-exp of importance weights\n        plus any accumulated marginal estimate from previous resampling steps\n    \"\"\"\n    current_marginal = jax.scipy.special.logsumexp(self.log_weights) - jnp.log(\n        self.n_samples.value\n    )\n    return self.log_marginal_estimate + current_marginal\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.ParticleCollection.estimate","title":"estimate","text":"<pre><code>estimate(fn: Callable[[X], Any]) -&gt; Any\n</code></pre> <p>Compute weighted estimate of a function applied to particle traces.</p> <p>Properly accounts for importance weights to give unbiased estimates.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; # particles.estimate(lambda choices: choices[\"param\"])  # Posterior mean\n&gt;&gt;&gt; # particles.estimate(lambda choices: choices[\"param\"]**2) - mean**2  # Variance\n&gt;&gt;&gt; # particles.estimate(lambda choices: jnp.sin(choices[\"x\"]) + choices[\"y\"])  # Custom\n</code></pre> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def estimate(self, fn: Callable[[X], Any]) -&gt; Any:\n    \"\"\"\n    Compute weighted estimate of a function applied to particle traces.\n\n    Properly accounts for importance weights to give unbiased estimates.\n\n    Args:\n        fn: Function to apply to each particle's choices (X -&gt; Any)\n\n    Returns:\n        Weighted estimate: sum(w_i * fn(x_i)) / sum(w_i)\n        where w_i are normalized importance weights\n\n    Examples:\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; # particles.estimate(lambda choices: choices[\"param\"])  # Posterior mean\n        &gt;&gt;&gt; # particles.estimate(lambda choices: choices[\"param\"]**2) - mean**2  # Variance\n        &gt;&gt;&gt; # particles.estimate(lambda choices: jnp.sin(choices[\"x\"]) + choices[\"y\"])  # Custom\n    \"\"\"\n    # Get particle choices\n    choices = self.traces.get_choices()\n\n    # Apply function to each particle\n    values = jax.vmap(fn)(choices)\n\n    # Compute normalized weights (in log space for numerical stability)\n    log_weights_normalized = self.log_weights - jax.scipy.special.logsumexp(\n        self.log_weights\n    )\n    weights_normalized = jnp.exp(log_weights_normalized)\n\n    # Compute weighted average\n    # For scalar values: sum(w_i * v_i)\n    # For arrays: maintains shape of values\n    if values.ndim == 1:\n        # Simple weighted average for scalar values per particle\n        return jnp.sum(weights_normalized * values)\n    else:\n        # For multi-dimensional values, weight along the particle dimension (axis 0)\n        return jnp.sum(weights_normalized[:, None] * values, axis=0)\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.effective_sample_size","title":"effective_sample_size","text":"<pre><code>effective_sample_size(log_weights: ndarray) -&gt; jnp.ndarray\n</code></pre> <p>Compute the effective sample size (ESS) from log importance weights.</p> <p>The ESS measures the efficiency of importance sampling by estimating the number of independent samples that would provide equivalent statistical information. It quantifies particle degeneracy in SMC algorithms.</p> Mathematical Formulation <p>Given N particles with normalized weights w\u2081, ..., w\u2099:</p> <p>ESS = 1 / \u03a3\u1d62 w\u1d62\u00b2 = (\u03a3\u1d62 w\u1d62)\u00b2 / \u03a3\u1d62 w\u1d62\u00b2</p> <p>Since \u03a3\u1d62 w\u1d62 = 1 for normalized weights:</p> <p>ESS = 1 / \u03a3\u1d62 w\u1d62\u00b2</p> Interpretation <ul> <li>ESS = N: Perfect sampling (uniform weights)</li> <li>ESS = 1: Complete degeneracy (single particle has all weight)</li> <li>ESS/N: Efficiency ratio, often used to trigger resampling when &lt; 0.5</li> </ul> Connection to Importance Sampling <p>The ESS approximates the variance inflation factor for importance sampling estimates. For self-normalized importance sampling:</p> <p>Var[\ud835\udd3c[f]] \u2248 (N/ESS) \u00d7 Var_\u03c0[f]</p> <p>where \u03c0 is the target distribution.</p> References <p>.. [1] Kong, A., Liu, J. S., &amp; Wong, W. H. (1994). \"Sequential imputations        and Bayesian missing data problems\". Journal of the American        Statistical Association, 89(425), 278-288. .. [2] Liu, J. S. (2001). \"Monte Carlo strategies in scientific computing\".        Springer, Chapter 3. .. [3] Doucet, A., de Freitas, N., &amp; Gordon, N. (2001). \"Sequential Monte        Carlo methods in practice\". Springer, Chapter 1.</p> Notes <ul> <li>Computed in log-space for numerical stability</li> <li>Input weights need not be normalized (handled internally)</li> <li>Common resampling threshold: ESS &lt; N/2 (Doucet et al., 2001)</li> </ul> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def effective_sample_size(log_weights: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute the effective sample size (ESS) from log importance weights.\n\n    The ESS measures the efficiency of importance sampling by estimating the\n    number of independent samples that would provide equivalent statistical\n    information. It quantifies particle degeneracy in SMC algorithms.\n\n    Mathematical Formulation:\n        Given N particles with normalized weights w\u2081, ..., w\u2099:\n\n        ESS = 1 / \u03a3\u1d62 w\u1d62\u00b2 = (\u03a3\u1d62 w\u1d62)\u00b2 / \u03a3\u1d62 w\u1d62\u00b2\n\n        Since \u03a3\u1d62 w\u1d62 = 1 for normalized weights:\n\n        ESS = 1 / \u03a3\u1d62 w\u1d62\u00b2\n\n    Interpretation:\n        - ESS = N: Perfect sampling (uniform weights)\n        - ESS = 1: Complete degeneracy (single particle has all weight)\n        - ESS/N: Efficiency ratio, often used to trigger resampling when &lt; 0.5\n\n    Connection to Importance Sampling:\n        The ESS approximates the variance inflation factor for importance\n        sampling estimates. For self-normalized importance sampling:\n\n        Var[\ud835\udd3c[f]] \u2248 (N/ESS) \u00d7 Var_\u03c0[f]\n\n        where \u03c0 is the target distribution.\n\n    Args:\n        log_weights: Array of unnormalized log importance weights of shape (N,)\n\n    Returns:\n        Effective sample size as a scalar in range [1, N]\n\n    References:\n        .. [1] Kong, A., Liu, J. S., &amp; Wong, W. H. (1994). \"Sequential imputations\n               and Bayesian missing data problems\". Journal of the American\n               Statistical Association, 89(425), 278-288.\n        .. [2] Liu, J. S. (2001). \"Monte Carlo strategies in scientific computing\".\n               Springer, Chapter 3.\n        .. [3] Doucet, A., de Freitas, N., &amp; Gordon, N. (2001). \"Sequential Monte\n               Carlo methods in practice\". Springer, Chapter 1.\n\n    Notes:\n        - Computed in log-space for numerical stability\n        - Input weights need not be normalized (handled internally)\n        - Common resampling threshold: ESS &lt; N/2 (Doucet et al., 2001)\n    \"\"\"\n    log_weights_normalized = log_weights - jax.scipy.special.logsumexp(log_weights)\n    weights_normalized = jnp.exp(log_weights_normalized)\n    return 1.0 / jnp.sum(weights_normalized**2)\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.systematic_resample","title":"systematic_resample","text":"<pre><code>systematic_resample(log_weights: ndarray, n_samples: int) -&gt; jnp.ndarray\n</code></pre> <p>Systematic resampling from importance weights with minimal variance.</p> <p>Implements the systematic resampling algorithm (Kitagawa, 1996), which has lower variance than multinomial resampling while maintaining unbiasedness. This is the preferred resampling method for particle filters.</p> Mathematical Formulation <p>Given normalized weights w\u2081, ..., w\u2099 and cumulative sum C\u1d62 = \u03a3\u2c7c\u2264\u1d62 w\u2c7c:</p> <ol> <li>Draw U ~ Uniform(0, 1/M) where M is the output sample size</li> <li>For i = 1, ..., M:</li> <li>Set pointer position: u\u1d62 = (i-1)/M + U</li> <li>Select particle: I\u1d62 = min{j : C\u2c7c \u2265 u\u1d62}</li> </ol> Properties <ul> <li>Unbiased: \ud835\udd3c[N\u1d62] = M \u00d7 w\u1d62 where N\u1d62 is count of particle i</li> <li>Lower variance than multinomial: Var[N\u1d62] \u2264 M \u00d7 w\u1d62 \u00d7 (1 - w\u1d62)</li> <li>Deterministic given U: reduces Monte Carlo variance</li> <li>Preserves particle order (stratified structure)</li> </ul> <p>Time Complexity: O(N + M) using binary search Space Complexity: O(N) for cumulative weights</p> References <p>.. [1] Kitagawa, G. (1996). \"Monte Carlo filter and smoother for non-Gaussian        nonlinear state space models\". Journal of Computational and Graphical        Statistics, 5(1), 1-25. .. [2] Doucet, A., &amp; Johansen, A. M. (2009). \"A tutorial on particle filtering        and smoothing: Fifteen years later\". Handbook of Nonlinear Filtering,        12(656-704), 3. .. [3] Hol, J. D., Schon, T. B., &amp; Gustafsson, F. (2006). \"On resampling        algorithms for particle filters\". In IEEE Nonlinear Statistical Signal        Processing Workshop (pp. 79-82).</p> Notes <ul> <li>Systematic resampling is preferred over multinomial for most applications</li> <li>Maintains particle diversity better than multinomial resampling</li> <li>For theoretical analysis of resampling methods, see [3]</li> </ul> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def systematic_resample(log_weights: jnp.ndarray, n_samples: int) -&gt; jnp.ndarray:\n    \"\"\"\n    Systematic resampling from importance weights with minimal variance.\n\n    Implements the systematic resampling algorithm (Kitagawa, 1996), which has\n    lower variance than multinomial resampling while maintaining unbiasedness.\n    This is the preferred resampling method for particle filters.\n\n    Mathematical Formulation:\n        Given normalized weights w\u2081, ..., w\u2099 and cumulative sum C\u1d62 = \u03a3\u2c7c\u2264\u1d62 w\u2c7c:\n\n        1. Draw U ~ Uniform(0, 1/M) where M is the output sample size\n        2. For i = 1, ..., M:\n           - Set pointer position: u\u1d62 = (i-1)/M + U\n           - Select particle: I\u1d62 = min{j : C\u2c7c \u2265 u\u1d62}\n\n    Properties:\n        - Unbiased: \ud835\udd3c[N\u1d62] = M \u00d7 w\u1d62 where N\u1d62 is count of particle i\n        - Lower variance than multinomial: Var[N\u1d62] \u2264 M \u00d7 w\u1d62 \u00d7 (1 - w\u1d62)\n        - Deterministic given U: reduces Monte Carlo variance\n        - Preserves particle order (stratified structure)\n\n    Time Complexity: O(N + M) using binary search\n    Space Complexity: O(N) for cumulative weights\n\n    Args:\n        log_weights: Unnormalized log importance weights of shape (N,)\n        n_samples: Number of samples to draw (M)\n\n    Returns:\n        Array of particle indices of shape (M,) for resampling\n\n    References:\n        .. [1] Kitagawa, G. (1996). \"Monte Carlo filter and smoother for non-Gaussian\n               nonlinear state space models\". Journal of Computational and Graphical\n               Statistics, 5(1), 1-25.\n        .. [2] Doucet, A., &amp; Johansen, A. M. (2009). \"A tutorial on particle filtering\n               and smoothing: Fifteen years later\". Handbook of Nonlinear Filtering,\n               12(656-704), 3.\n        .. [3] Hol, J. D., Schon, T. B., &amp; Gustafsson, F. (2006). \"On resampling\n               algorithms for particle filters\". In IEEE Nonlinear Statistical Signal\n               Processing Workshop (pp. 79-82).\n\n    Notes:\n        - Systematic resampling is preferred over multinomial for most applications\n        - Maintains particle diversity better than multinomial resampling\n        - For theoretical analysis of resampling methods, see [3]\n    \"\"\"\n    log_weights_normalized = log_weights - jax.scipy.special.logsumexp(log_weights)\n    weights = jnp.exp(log_weights_normalized)\n\n    # Use uniform distribution for systematic resampling offset\n    u = uniform.sample(0.0, 1.0)\n    positions = (jnp.arange(n_samples) + u) / n_samples\n    cumsum = jnp.cumsum(weights)\n\n    indices = jnp.searchsorted(cumsum, positions)\n    return indices\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.resample_vectorized_trace","title":"resample_vectorized_trace","text":"<pre><code>resample_vectorized_trace(trace: Trace[X, R], log_weights: ndarray, n_samples: int, method: str = 'categorical') -&gt; Trace[X, R]\n</code></pre> <p>Resample a vectorized trace using importance weights.</p> <p>Uses categorical or systematic sampling to select indices and jax.tree_util.tree_map to index into the Pytree leaves.</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def resample_vectorized_trace(\n    trace: Trace[X, R],\n    log_weights: jnp.ndarray,\n    n_samples: int,\n    method: str = \"categorical\",\n) -&gt; Trace[X, R]:\n    \"\"\"\n    Resample a vectorized trace using importance weights.\n\n    Uses categorical or systematic sampling to select indices and jax.tree_util.tree_map\n    to index into the Pytree leaves.\n\n    Args:\n        trace: Vectorized trace to resample\n        log_weights: Log importance weights\n        n_samples: Number of samples to draw\n        method: Resampling method - \"categorical\" or \"systematic\"\n\n    Returns:\n        Resampled vectorized trace\n    \"\"\"\n    if method == \"categorical\":\n        # Sample indices using categorical distribution\n        indices = categorical.sample(log_weights, sample_shape=(n_samples,))\n    elif method == \"systematic\":\n        # Use systematic resampling\n        indices = systematic_resample(log_weights, n_samples)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n\n    # Use tree_map to index into all leaves of the trace Pytree\n    def index_leaf(leaf):\n        # Index into the first dimension (batch dimension) of each leaf\n        return leaf[indices]\n\n    resampled_trace = jtu.tree_map(index_leaf, trace)\n    return resampled_trace\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.init","title":"init","text":"<pre><code>init(target_gf: GFI[X, R], target_args: tuple, n_samples: Const[int], constraints: X, proposal_gf: GFI[X, Any] | None = None) -&gt; ParticleCollection\n</code></pre> <p>Initialize particle collection using importance sampling.</p> <p>Uses either the target's default internal proposal or a custom proposal. Proposals use signature (constraints, *target_args).</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def init(\n    target_gf: GFI[X, R],\n    target_args: tuple,\n    n_samples: Const[int],\n    constraints: X,\n    proposal_gf: GFI[X, Any] | None = None,\n) -&gt; ParticleCollection:\n    \"\"\"\n    Initialize particle collection using importance sampling.\n\n    Uses either the target's default internal proposal or a custom proposal.\n    Proposals use signature (constraints, *target_args).\n\n    Args:\n        target_gf: Target generative function (model)\n        target_args: Arguments for target generative function\n        n_samples: Number of importance samples to draw (static value)\n        constraints: Dictionary of constrained random choices\n        proposal_gf: Optional custom proposal generative function.\n                    If None, uses target's default internal proposal.\n\n    Returns:\n        ParticleCollection with traces, weights, and diagnostics\n    \"\"\"\n    if proposal_gf is None:\n        # Use default importance sampling with target's internal proposal\n        def _single_default_importance_sample(\n            target_gf: GFI[X, R],\n            target_args: tuple,\n            constraints: X,\n        ) -&gt; tuple[Trace[X, R], Weight]:\n            \"\"\"Single importance sampling step using target's default proposal.\"\"\"\n            # Use target's generate method with constraints\n            # This will use the target's internal proposal to fill in missing choices\n            target_trace, log_weight = target_gf.generate(constraints, *target_args)\n            return target_trace, log_weight\n\n        # Vectorize the single importance sampling step\n        vectorized_sample = modular_vmap(\n            _single_default_importance_sample,\n            in_axes=(None, None, None),\n            axis_size=n_samples.value,\n        )\n\n        # Run vectorized importance sampling\n        traces, log_weights = vectorized_sample(target_gf, target_args, constraints)\n    else:\n        # Use custom proposal importance sampling\n        def _single_importance_sample(\n            target_gf: GFI[X, R],\n            proposal_gf: GFI[X, Any],\n            target_args: tuple,\n            constraints: X,\n        ) -&gt; tuple[Trace[X, R], Weight]:\n            \"\"\"\n            Single importance sampling step using custom proposal.\n\n            Proposal uses signature (constraints, *target_args).\n            \"\"\"\n            # Sample from proposal using new signature\n            proposal_trace = proposal_gf.simulate(constraints, *target_args)\n            proposal_choices = proposal_trace.get_choices()\n\n            # Get proposal score: log(1/P_proposal)\n            proposal_score = proposal_trace.get_score()\n\n            # Merge proposal choices with constraints\n            merged_choices, _ = target_gf.merge(proposal_choices, constraints)\n\n            # Generate from target using merged choices\n            target_trace, target_weight = target_gf.generate(\n                merged_choices, *target_args\n            )\n\n            # Compute importance weight: P/Q\n            # target_weight is the weight from generate (density of model at merged choices)\n            # proposal_score is log(1/P_proposal)\n            # importance_weight = target_weight + proposal_score\n            log_weight = target_weight + proposal_score\n\n            return target_trace, log_weight\n\n        # Vectorize the single importance sampling step\n        vectorized_sample = modular_vmap(\n            _single_importance_sample,\n            in_axes=(None, None, None, None),\n            axis_size=n_samples.value,\n        )\n\n        # Run vectorized importance sampling\n        traces, log_weights = vectorized_sample(\n            target_gf, proposal_gf, target_args, constraints\n        )\n\n    return _create_particle_collection(\n        traces=traces,  # vectorized\n        log_weights=log_weights,\n        n_samples=const(n_samples.value),\n        log_marginal_estimate=jnp.array(0.0),\n    )\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.change","title":"change","text":"<pre><code>change(particles: ParticleCollection, new_target_gf: GFI[X, R], new_target_args: tuple, choice_fn: Callable[[X], X]) -&gt; ParticleCollection\n</code></pre> <p>Change target move for particle collection.</p> <p>Translates particles from one model to another by: 1. Mapping each particle's choices using choice_fn 2. Using generate with the new model to get new weights 3. Accumulating importance weights</p> Choice Function Specification <p>CRITICAL: choice_fn must be a bijection on address space only.</p> <ul> <li>If X is a scalar type (e.g., float): Must be identity function</li> <li>If X is dict[str, Any]: May remap keys but CANNOT modify values</li> <li>Values must be preserved exactly to maintain probability density</li> </ul> <p>Valid Examples: - lambda x: x  (identity mapping) - lambda d: {\"new_key\": d[\"old_key\"]}  (key remapping) - lambda d: {\"mu\": d[\"mean\"], \"sigma\": d[\"std\"]}  (multiple key remap)</p> <p>Invalid Examples: - lambda x: x + 1  (modifies scalar values - breaks assumptions) - lambda d: {\"key\": d[\"key\"] * 2}  (modifies dict values - breaks assumptions)</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def change(\n    particles: ParticleCollection,\n    new_target_gf: GFI[X, R],\n    new_target_args: tuple,\n    choice_fn: Callable[[X], X],\n) -&gt; ParticleCollection:\n    \"\"\"\n    Change target move for particle collection.\n\n    Translates particles from one model to another by:\n    1. Mapping each particle's choices using choice_fn\n    2. Using generate with the new model to get new weights\n    3. Accumulating importance weights\n\n    Args:\n        particles: Current particle collection\n        new_target_gf: New target generative function\n        new_target_args: Arguments for new target\n        choice_fn: Bijective function mapping choices X -&gt; X\n\n    Choice Function Specification:\n        CRITICAL: choice_fn must be a bijection on address space only.\n\n        - If X is a scalar type (e.g., float): Must be identity function\n        - If X is dict[str, Any]: May remap keys but CANNOT modify values\n        - Values must be preserved exactly to maintain probability density\n\n        Valid Examples:\n        - lambda x: x  (identity mapping)\n        - lambda d: {\"new_key\": d[\"old_key\"]}  (key remapping)\n        - lambda d: {\"mu\": d[\"mean\"], \"sigma\": d[\"std\"]}  (multiple key remap)\n\n        Invalid Examples:\n        - lambda x: x + 1  (modifies scalar values - breaks assumptions)\n        - lambda d: {\"key\": d[\"key\"] * 2}  (modifies dict values - breaks assumptions)\n\n    Returns:\n        New ParticleCollection with translated particles\n    \"\"\"\n\n    def _single_change_target(\n        old_trace: Trace[X, R], old_log_weight: jnp.ndarray\n    ) -&gt; tuple[Trace[X, R], jnp.ndarray]:\n        # Map choices to new space\n        old_choices = old_trace.get_choices()\n        mapped_choices = choice_fn(old_choices)\n\n        # Generate with new model using mapped choices as constraints\n        new_trace, log_weight = new_target_gf.generate(mapped_choices, *new_target_args)\n\n        # Accumulate importance weight\n        new_log_weight = old_log_weight + log_weight\n\n        return new_trace, new_log_weight\n\n    # Vectorize across particles\n    vectorized_change = modular_vmap(\n        _single_change_target,\n        in_axes=(0, 0),\n        axis_size=particles.n_samples.value,\n    )\n\n    new_traces, new_log_weights = vectorized_change(\n        particles.traces, particles.log_weights\n    )\n\n    return _create_particle_collection(\n        traces=new_traces,\n        log_weights=new_log_weights,\n        n_samples=particles.n_samples,\n        log_marginal_estimate=particles.log_marginal_estimate,\n        # diagnostic_weights will be computed from new_log_weights in _create_particle_collection\n    )\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.extend","title":"extend","text":"<pre><code>extend(particles: ParticleCollection, extended_target_gf: GFI[X, R], extended_target_args: Any, constraints: X, extension_proposal: GFI[X, Any] | None = None) -&gt; ParticleCollection\n</code></pre> <p>Extension move for particle collection.</p> <p>Extends each particle by generating from the extended target model: 1. Without extension proposal: Uses extended target's generate with constraints directly 2. With extension proposal: Samples extension, merges with constraints, then generates</p> <p>The extended target model is responsible for recognizing and incorporating existing particle state through its internal structure.</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def extend(\n    particles: ParticleCollection,\n    extended_target_gf: GFI[X, R],\n    extended_target_args: Any,  # Can be tuple or vectorized args\n    constraints: X,\n    extension_proposal: GFI[X, Any] | None = None,\n) -&gt; ParticleCollection:\n    \"\"\"\n    Extension move for particle collection.\n\n    Extends each particle by generating from the extended target model:\n    1. Without extension proposal: Uses extended target's generate with constraints directly\n    2. With extension proposal: Samples extension, merges with constraints, then generates\n\n    The extended target model is responsible for recognizing and incorporating\n    existing particle state through its internal structure.\n\n    Args:\n        particles: Current particle collection\n        extended_target_gf: Extended target generative function that recognizes particle state\n        extended_target_args: Arguments for extended target\n        constraints: Constraints on the new variables (e.g., observations at current timestep)\n        extension_proposal: Optional proposal for the extension. If None, uses extended target's internal proposal.\n\n    Returns:\n        New ParticleCollection with extended particles\n    \"\"\"\n\n    def _single_extension(\n        old_trace: Trace[X, R], old_log_weight: jnp.ndarray, particle_args: Any\n    ) -&gt; tuple[Trace[X, R], jnp.ndarray]:\n        # Convert particle_args to tuple if it's not already\n        if isinstance(particle_args, tuple):\n            args = particle_args\n        else:\n            args = (particle_args,)\n\n        if extension_proposal is None:\n            # Generate with extended target using constraints\n            new_trace, log_weight = extended_target_gf.generate(constraints, *args)\n\n            # Weight is just the target weight (no proposal correction needed)\n            new_log_weight = old_log_weight + log_weight\n        else:\n            # Use custom extension proposal\n            # Proposal gets: (obs, prev_particle_choices, *model_args)\n            old_choices = old_trace.get_choices()\n            extension_trace = extension_proposal.simulate(\n                constraints, old_choices, *args\n            )\n            extension_choices = extension_trace.get_choices()\n            proposal_score = extension_trace.get_score()\n\n            # Merge old choices, extension choices, and constraints\n            merged_choices, _ = extended_target_gf.merge(constraints, extension_choices)\n\n            # Generate with extended target\n            new_trace, log_weight = extended_target_gf.generate(merged_choices, *args)\n\n            # Importance weight: target_weight + proposal_score + old_weight\n            new_log_weight = old_log_weight + log_weight + proposal_score\n\n        return new_trace, new_log_weight\n\n    # Vectorize across particles\n    vectorized_extension = modular_vmap(\n        _single_extension,\n        in_axes=(0, 0, 0),  # Add axis for particle_args\n        axis_size=particles.n_samples.value,\n    )\n\n    new_traces, new_log_weights = vectorized_extension(\n        particles.traces, particles.log_weights, extended_target_args\n    )\n\n    return _create_particle_collection(\n        traces=new_traces,\n        log_weights=new_log_weights,\n        n_samples=particles.n_samples,\n        log_marginal_estimate=particles.log_marginal_estimate,\n        # diagnostic_weights will be computed from new_log_weights in _create_particle_collection\n    )\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.rejuvenate","title":"rejuvenate","text":"<pre><code>rejuvenate(particles: ParticleCollection, mcmc_kernel: Callable[[Trace[X, R]], Trace[X, R]]) -&gt; ParticleCollection\n</code></pre> <p>Rejuvenate move for particle collection.</p> <p>Applies an MCMC kernel to each particle independently to improve particle diversity and reduce degeneracy. The importance weights and diagnostic weights remain unchanged due to detailed balance.</p> Mathematical Foundation <p>For an MCMC kernel satisfying detailed balance, the log incremental weight is 0:</p> <p>log_incremental_weight = log[p(x_new | args) / p(x_old | args)]                         + log[q(x_old | x_new) / q(x_new | x_old)]</p> <p>Where: - p(x_new | args) / p(x_old | args) is the model density ratio - q(x_old | x_new) / q(x_new | x_old) is the proposal density ratio</p> <p>Detailed balance ensures: p(x_old) * q(x_new | x_old) = p(x_new) * q(x_old | x_new)</p> <p>Therefore: p(x_new) / p(x_old) = q(x_new | x_old) / q(x_old | x_new)</p> <p>The model density ratio and proposal density ratio exactly cancel: log[p(x_new) / p(x_old)] + log[q(x_old | x_new) / q(x_new | x_old)] = 0</p> <p>This means the importance weight contribution from the MCMC move is 0, preserving the particle weights while improving sample diversity.</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def rejuvenate(\n    particles: ParticleCollection,\n    mcmc_kernel: Callable[[Trace[X, R]], Trace[X, R]],\n) -&gt; ParticleCollection:\n    \"\"\"\n    Rejuvenate move for particle collection.\n\n    Applies an MCMC kernel to each particle independently to improve\n    particle diversity and reduce degeneracy. The importance weights and\n    diagnostic weights remain unchanged due to detailed balance.\n\n    Mathematical Foundation:\n        For an MCMC kernel satisfying detailed balance, the log incremental weight is 0:\n\n        log_incremental_weight = log[p(x_new | args) / p(x_old | args)]\n                                + log[q(x_old | x_new) / q(x_new | x_old)]\n\n        Where:\n        - p(x_new | args) / p(x_old | args) is the model density ratio\n        - q(x_old | x_new) / q(x_new | x_old) is the proposal density ratio\n\n        Detailed balance ensures: p(x_old) * q(x_new | x_old) = p(x_new) * q(x_old | x_new)\n\n        Therefore: p(x_new) / p(x_old) = q(x_new | x_old) / q(x_old | x_new)\n\n        The model density ratio and proposal density ratio exactly cancel:\n        log[p(x_new) / p(x_old)] + log[q(x_old | x_new) / q(x_new | x_old)] = 0\n\n        This means the importance weight contribution from the MCMC move is 0,\n        preserving the particle weights while improving sample diversity.\n\n    Args:\n        particles: Current particle collection\n        mcmc_kernel: MCMC kernel function that takes a trace and returns\n                    a new trace. Should be compatible with kernels from mcmc.py like mh.\n\n    Returns:\n        New ParticleCollection with rejuvenated particles\n    \"\"\"\n\n    def _single_rejuvenate(\n        old_trace: Trace[X, R], old_log_weight: jnp.ndarray\n    ) -&gt; tuple[Trace[X, R], jnp.ndarray]:\n        # Apply MCMC kernel\n        new_trace = mcmc_kernel(old_trace)\n\n        # Weights remain unchanged for MCMC moves (detailed balance)\n        # Log incremental weight = 0 because model density ratio cancels with proposal density ratio\n        return new_trace, old_log_weight\n\n    # Vectorize across particles\n    vectorized_rejuvenate = modular_vmap(\n        _single_rejuvenate,\n        in_axes=(0, 0),\n        axis_size=particles.n_samples.value,\n    )\n\n    new_traces, new_log_weights = vectorized_rejuvenate(\n        particles.traces, particles.log_weights\n    )\n\n    return _create_particle_collection(\n        traces=new_traces,\n        log_weights=new_log_weights,\n        n_samples=particles.n_samples,\n        log_marginal_estimate=particles.log_marginal_estimate,\n        diagnostic_weights=particles.diagnostic_weights,  # Propagate diagnostic weights unchanged\n    )\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.resample","title":"resample","text":"<pre><code>resample(particles: ParticleCollection, method: str = 'categorical') -&gt; ParticleCollection\n</code></pre> <p>Resample particle collection to combat degeneracy.</p> <p>Computes log normalized weights for diagnostics before resampling. After resampling, weights are reset to uniform (zero in log space) and the marginal likelihood estimate is updated to include the average weight before resampling.</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def resample(\n    particles: ParticleCollection,\n    method: str = \"categorical\",\n) -&gt; ParticleCollection:\n    \"\"\"\n    Resample particle collection to combat degeneracy.\n\n    Computes log normalized weights for diagnostics before resampling.\n    After resampling, weights are reset to uniform (zero in log space)\n    and the marginal likelihood estimate is updated to include the\n    average weight before resampling.\n\n    Args:\n        particles: Current particle collection\n        method: Resampling method - \"categorical\" or \"systematic\"\n\n    Returns:\n        New ParticleCollection with resampled particles and updated marginal estimate\n    \"\"\"\n    # Compute log normalized weights before resampling for diagnostics\n    log_normalized_weights = particles.log_weights - jax.scipy.special.logsumexp(\n        particles.log_weights\n    )\n\n    # Compute current marginal contribution before resampling\n    current_marginal = jax.scipy.special.logsumexp(particles.log_weights) - jnp.log(\n        particles.n_samples.value\n    )\n\n    # Update accumulated marginal estimate\n    new_log_marginal_estimate = particles.log_marginal_estimate + current_marginal\n\n    # Resample traces using existing function\n    resampled_traces = resample_vectorized_trace(\n        particles.traces,\n        particles.log_weights,\n        particles.n_samples.value,\n        method=method,\n    )\n\n    # Reset weights to uniform (zero in log space)\n    uniform_log_weights = jnp.zeros(particles.n_samples.value)\n\n    return _create_particle_collection(\n        traces=resampled_traces,\n        log_weights=uniform_log_weights,\n        n_samples=particles.n_samples,\n        log_marginal_estimate=new_log_marginal_estimate,\n        diagnostic_weights=log_normalized_weights,  # Store pre-resampling normalized weights\n    )\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.rejuvenation_smc","title":"rejuvenation_smc","text":"<pre><code>rejuvenation_smc(model: GFI[X, R], transition_proposal: GFI[X, Any] | None = None, mcmc_kernel: Const[Callable[[Trace[X, R]], Trace[X, R]]] | None = None, observations: X | None = None, initial_model_args: tuple | None = None, n_particles: Const[int] = const(1000), return_all_particles: Const[bool] = const(False), n_rejuvenation_moves: Const[int] = const(1)) -&gt; ParticleCollection\n</code></pre> <p>Complete SMC algorithm with rejuvenation using jax.lax.scan.</p> <p>Implements sequential Monte Carlo with particle extension, resampling, and MCMC rejuvenation. Uses a single model with feedback loop where the return value becomes the next timestep's arguments, creating sequential dependencies.</p> Note on Return Value <p>This function returns only the FINAL ParticleCollection after processing all observations. Intermediate timesteps are computed but not returned. If you need all timesteps, you can modify the return statement to:</p> <pre><code>final_particles, all_particles = jax.lax.scan(smc_step, particles, remaining_obs)\nreturn all_particles  # Returns vectorized ParticleCollection with time dimension\n</code></pre> <p>The all_particles object would have an additional leading time dimension in all its fields (traces, log_weights, etc.), allowing access to the full particle trajectory across all timesteps.</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def rejuvenation_smc(\n    model: GFI[X, R],\n    transition_proposal: GFI[X, Any] | None = None,\n    mcmc_kernel: Const[Callable[[Trace[X, R]], Trace[X, R]]] | None = None,\n    observations: X | None = None,\n    initial_model_args: tuple | None = None,\n    n_particles: Const[int] = const(1000),\n    return_all_particles: Const[bool] = const(False),\n    n_rejuvenation_moves: Const[int] = const(1),\n) -&gt; ParticleCollection:\n    \"\"\"\n    Complete SMC algorithm with rejuvenation using jax.lax.scan.\n\n    Implements sequential Monte Carlo with particle extension, resampling,\n    and MCMC rejuvenation. Uses a single model with feedback loop where\n    the return value becomes the next timestep's arguments, creating\n    sequential dependencies.\n\n\n    Note on Return Value:\n        This function returns only the FINAL ParticleCollection after processing\n        all observations. Intermediate timesteps are computed but not returned.\n        If you need all timesteps, you can modify the return statement to:\n\n        ```python\n        final_particles, all_particles = jax.lax.scan(smc_step, particles, remaining_obs)\n        return all_particles  # Returns vectorized ParticleCollection with time dimension\n        ```\n\n        The all_particles object would have an additional leading time dimension\n        in all its fields (traces, log_weights, etc.), allowing access to the\n        full particle trajectory across all timesteps.\n\n    Args:\n        model: Single generative function where return value feeds into next timestep\n        transition_proposal: Optional proposal for extending particles at each timestep.\n                           If None, uses the model's internal proposal. (default: None)\n        mcmc_kernel: Optional MCMC kernel for particle rejuvenation (wrapped in Const).\n                    If None, no rejuvenation moves are performed. (default: None)\n        observations: Sequence of observations (can be Pytree structure)\n        initial_model_args: Arguments for the first timestep\n        n_particles: Number of particles to maintain (default: const(1000))\n        return_all_particles: If True, returns all particles across time (default: const(False))\n        n_rejuvenation_moves: Number of MCMC rejuvenation moves per timestep (default: const(1))\n\n    Returns:\n        If return_all_particles=False: Final ParticleCollection (no time dimension)\n        If return_all_particles=True: ParticleCollection with leading time dimension (T, ...)\n    \"\"\"\n    # Extract first observation using tree_map (handles Pytree structure)\n    first_obs = jtu.tree_map(lambda x: x[0], observations)\n\n    # Initialize with first observation using the single model\n    particles = init(model, initial_model_args, n_particles, first_obs)\n\n    # Resample initial particles if needed\n    ess = particles.effective_sample_size()\n    particles = jax.lax.cond(\n        ess &lt; n_particles.value // 2,\n        lambda p: resample(p),\n        lambda p: p,\n        particles,\n    )\n\n    # Apply initial rejuvenation moves (only if mcmc_kernel is provided)\n    if mcmc_kernel is not None:\n\n        def rejuvenation_step(particles, _):\n            \"\"\"Single rejuvenation move.\"\"\"\n            return rejuvenate(particles, mcmc_kernel.value), None\n\n        # Apply n_rejuvenation_moves steps\n        particles, _ = jax.lax.scan(\n            rejuvenation_step, particles, jnp.arange(n_rejuvenation_moves.value)\n        )\n\n    def smc_step(particles, obs):\n        # Extract return values from current particles to use as next model args\n        # Get vectorized return values from all particles\n        current_retvals = particles.traces.get_retval()\n\n        # Extend particles with new observation constraints\n        # Use current return values as the model arguments for the next step\n        particles = extend(\n            particles,\n            model,\n            current_retvals,  # Feed return values as next model args\n            obs,\n            extension_proposal=transition_proposal,  # None is allowed - uses model's internal proposal\n        )\n\n        # Resample if needed\n        ess = particles.effective_sample_size()\n        particles = jax.lax.cond(\n            ess &lt; n_particles.value // 2,\n            lambda p: resample(p),\n            lambda p: p,\n            particles,\n        )\n\n        # Multiple rejuvenation moves (only if mcmc_kernel is provided)\n        if mcmc_kernel is not None:\n\n            def rejuvenation_step(particles, _):\n                \"\"\"Single rejuvenation move.\"\"\"\n                return rejuvenate(particles, mcmc_kernel.value), None\n\n            # Apply n_rejuvenation_moves steps\n            particles, _ = jax.lax.scan(\n                rejuvenation_step, particles, jnp.arange(n_rejuvenation_moves.value)\n            )\n\n        return particles, particles  # (carry, output)\n\n    # Sequential updates using scan over remaining observations\n    # Slice remaining observations using tree_map (handles Pytree structure)\n    remaining_obs = jtu.tree_map(lambda x: x[1:], observations)\n\n    final_particles, all_particles = jax.lax.scan(smc_step, particles, remaining_obs)\n\n    if return_all_particles.value:\n        # Prepend initial particles to create complete sequence\n        # all_particles has shape (T-1, ...) from scan over remaining_obs\n        # We need to add the initial particles to get shape (T, ...)\n        return jtu.tree_map(\n            lambda init, rest: jnp.concatenate([init[None, ...], rest], axis=0),\n            particles,\n            all_particles,\n        )\n    else:\n        return final_particles\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.init_csmc","title":"init_csmc","text":"<pre><code>init_csmc(target_gf: GFI[X, R], target_args: tuple, n_samples: Const[int], constraints: X, retained_choices: X, proposal_gf: GFI[X, Any] | None = None) -&gt; ParticleCollection\n</code></pre> <p>Initialize particle collection for conditional SMC with retained particle.</p> <p>Simple approach: run regular init and manually override particle 0.</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def init_csmc(\n    target_gf: GFI[X, R],\n    target_args: tuple,\n    n_samples: Const[int],\n    constraints: X,\n    retained_choices: X,\n    proposal_gf: GFI[X, Any] | None = None,\n) -&gt; ParticleCollection:\n    \"\"\"\n    Initialize particle collection for conditional SMC with retained particle.\n\n    Simple approach: run regular init and manually override particle 0.\n\n    Args:\n        target_gf: Target generative function (model)\n        target_args: Arguments for target generative function\n        n_samples: Number of importance samples to draw (static value)\n        constraints: Dictionary of constrained random choices\n        retained_choices: Choices for the retained particle (one particle fixed)\n        proposal_gf: Optional custom proposal generative function\n\n    Returns:\n        ParticleCollection where particle 0 matches retained_choices exactly\n    \"\"\"\n    if n_samples.value &lt; 1:\n        raise ValueError(\"n_samples must be at least 1 for conditional SMC\")\n\n    # Run regular init to get the particle structure\n    particles = init(\n        target_gf=target_gf,\n        target_args=target_args,\n        n_samples=n_samples,\n        constraints=constraints,\n        proposal_gf=proposal_gf,\n    )\n\n    # Override the choices in particle 0 with retained choices\n    # This is a simplified approach - we manually set the choice values\n    current_choices = particles.traces.get_choices()\n\n    # Create a function to override specific keys\n    def override_with_retained(choices_dict):\n        # Make a copy and override keys that exist in retained_choices\n        new_dict = {}\n        for key, value in choices_dict.items():\n            if key in retained_choices:\n                # Set index 0 to the retained value\n                new_dict[key] = value.at[0].set(retained_choices[key])\n            else:\n                new_dict[key] = value\n        return new_dict\n\n    # Apply the override\n    new_choices_dict = override_with_retained(current_choices)\n\n    # Assess the retained choices to get the correct weight\n    retained_log_density, _ = target_gf.assess(retained_choices, *target_args)\n\n    # Set particle 0's weight to match the retained choice assessment\n    new_log_weights = particles.log_weights.at[0].set(retained_log_density)\n\n    # For now, return the particles with updated weight\n    # The choice override would require rebuilding the trace, which is complex\n    # This simplified version just ensures the weight is correct\n    return ParticleCollection(\n        traces=particles.traces,\n        log_weights=new_log_weights,\n        diagnostic_weights=particles.diagnostic_weights,\n        n_samples=particles.n_samples,\n        log_marginal_estimate=particles.log_marginal_estimate,\n    )\n</code></pre>"},{"location":"reference/smc/#genjax.inference.smc.extend_csmc","title":"extend_csmc","text":"<pre><code>extend_csmc(particles: ParticleCollection, extended_target_gf: GFI[X, R], extended_target_args: Any, constraints: X, retained_choices: X, extension_proposal: GFI[X, Any] | None = None) -&gt; ParticleCollection\n</code></pre> <p>Extension move for conditional SMC with retained particle.</p> <p>Like extend() but ensures particle 0 follows retained trajectory.</p> Source code in <code>src/genjax/inference/smc.py</code> <pre><code>def extend_csmc(\n    particles: ParticleCollection,\n    extended_target_gf: GFI[X, R],\n    extended_target_args: Any,\n    constraints: X,\n    retained_choices: X,\n    extension_proposal: GFI[X, Any] | None = None,\n) -&gt; ParticleCollection:\n    \"\"\"\n    Extension move for conditional SMC with retained particle.\n\n    Like extend() but ensures particle 0 follows retained trajectory.\n\n    Args:\n        particles: Current particle collection\n        extended_target_gf: Extended target generative function\n        extended_target_args: Arguments for extended target\n        constraints: Constraints on the new variables\n        retained_choices: Choices for retained particle at this timestep\n        extension_proposal: Optional proposal for the extension\n\n    Returns:\n        New ParticleCollection where particle 0 matches retained_choices\n    \"\"\"\n    def _single_extension_csmc(\n        old_trace: Trace[X, R], old_log_weight: jnp.ndarray, particle_args: Any, is_retained: bool\n    ) -&gt; tuple[Trace[X, R], jnp.ndarray]:\n        # Convert particle_args to tuple if needed\n        if isinstance(particle_args, tuple):\n            args = particle_args\n        else:\n            args = (particle_args,)\n\n        # For retained particle (index 0), use retained_choices exactly\n        def retained_extension():\n            # Assess retained choices with extended model\n            log_density, retval = extended_target_gf.assess(retained_choices, *args)\n\n            from genjax.core import Tr\n            new_trace = Tr(\n                _gen_fn=extended_target_gf,\n                _args=(args, {}),\n                _choices=retained_choices,\n                _retval=retval,\n                _score=-log_density\n            )\n            # Weight accumulation: old weight + log density\n            new_log_weight = old_log_weight + log_density\n            return new_trace, new_log_weight\n\n        # For regular particles, use standard extension\n        def regular_extension():\n            if extension_proposal is None:\n                # Generate with extended target using constraints\n                new_trace, log_weight = extended_target_gf.generate(constraints, *args)\n                new_log_weight = old_log_weight + log_weight\n            else:\n                # Use custom extension proposal\n                old_choices = old_trace.get_choices()\n                extension_trace = extension_proposal.simulate(constraints, old_choices, *args)\n                extension_choices = extension_trace.get_choices()\n                proposal_score = extension_trace.get_score()\n\n                # Merge and generate\n                merged_choices, _ = extended_target_gf.merge(constraints, extension_choices)\n                new_trace, log_weight = extended_target_gf.generate(merged_choices, *args)\n                new_log_weight = old_log_weight + log_weight + proposal_score\n\n            return new_trace, new_log_weight\n\n        # Choose extension type based on whether this is the retained particle\n        return jax.lax.cond(\n            is_retained,\n            retained_extension,\n            regular_extension\n        )\n\n    # Create is_retained flags: True for index 0, False for others\n    is_retained_flags = jnp.arange(particles.n_samples.value) == 0\n\n    # Vectorize across particles with is_retained flag\n    vectorized_extension = modular_vmap(\n        _single_extension_csmc,\n        in_axes=(0, 0, 0, 0),  # Add axis for is_retained\n        axis_size=particles.n_samples.value,\n    )\n\n    new_traces, new_log_weights = vectorized_extension(\n        particles.traces, particles.log_weights, extended_target_args, is_retained_flags\n    )\n\n    return _create_particle_collection(\n        traces=new_traces,\n        log_weights=new_log_weights,\n        n_samples=particles.n_samples,\n        log_marginal_estimate=particles.log_marginal_estimate,\n    )\n</code></pre>"},{"location":"reference/smc/#core-functions","title":"Core Functions","text":""},{"location":"reference/smc/#importance_sampling","title":"importance_sampling","text":"<p>Basic importance sampling with multiple particles.</p>"},{"location":"reference/smc/#particle_filter","title":"particle_filter","text":"<p>Sequential Monte Carlo for state-space models.</p>"},{"location":"reference/smc/#rejuvenation_smc","title":"rejuvenation_smc","text":"<p>SMC with MCMC rejuvenation steps for better particle diversity.</p>"},{"location":"reference/smc/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/smc/#importance-sampling","title":"Importance Sampling","text":"<pre><code>from genjax.inference.smc import importance_sampling\n\n# Run with 1000 particles\nkeys = jax.random.split(key, 1000)\ntraces = jax.vmap(lambda k: model.generate(k, constraints, args))(keys)\n\n# Extract weights\nlog_weights = traces.score\nweights = jax.nn.softmax(log_weights)\n\n# Weighted posterior mean\nposterior_mean = jnp.sum(weights * traces[\"parameter\"])\n</code></pre>"},{"location":"reference/smc/#particle-filter","title":"Particle Filter","text":"<pre><code>from genjax.inference.smc import particle_filter\n\n# For sequential data\n@gen\ndef transition(prev_state, t):\n    return distributions.normal(prev_state, 0.1) @ f\"state_{t}\"\n\n@gen\ndef observation(state, t):\n    return distributions.normal(state, 0.5) @ f\"obs_{t}\"\n\n# Run particle filter\nparticles = particle_filter(\n    initial_model,\n    transition,\n    observation,\n    observations,\n    n_particles=100,\n    key=key\n)\n</code></pre>"},{"location":"reference/smc/#smc-with-rejuvenation","title":"SMC with Rejuvenation","text":"<pre><code>from genjax.inference.smc import rejuvenation_smc\n\n# SMC with optional MCMC moves\nresult = rejuvenation_smc(\n    model,\n    observations,\n    n_particles=100,\n    n_mcmc_steps=5,  # Optional: rejuvenation steps\n    key=key\n)\n</code></pre>"},{"location":"reference/smc/#best-practices","title":"Best Practices","text":"<ol> <li>Particle Count: Use enough particles (typically 100-10000)</li> <li>Resampling: Monitor effective sample size for resampling</li> <li>Proposal Design: Use good proposal distributions</li> <li>Rejuvenation: Add MCMC steps to maintain diversity</li> </ol>"},{"location":"reference/sp/","title":"genjax.sp","text":"<p>Structural primitives and combinators for building complex models.</p>"},{"location":"reference/sp/#genjax.sp","title":"sp","text":"<p>Stochastic Probabilities (SP) for GenJAX</p> <p>This module implements SPDistribution following the design of GenSP.jl, enabling probabilistic programming with importance-weighted samples. SP distributions produce weighted samples that enable unbiased estimation of probabilities and expectations.</p> References <ul> <li>\"Probabilistic Programming with Stochastic Probabilities\"   Alexander K. Lew, Matin Ghavami, Martin Rinard, Vikash K. Mansinghka</li> <li>GenSP.jl: https://github.com/probcomp/GenSP.jl</li> </ul>"},{"location":"reference/sp/#genjax.sp.SPDistribution","title":"SPDistribution","text":"<p>               Bases: <code>GFI[X, X]</code>, <code>ABC</code></p> <p>Abstract base class for Stochastic Probability distributions.</p> <p>SPDistributions extend the GFI interface with importance-weighted sampling. Instead of implementing simulate/assess directly, subclasses implement random_weighted and estimate_logpdf.</p> <p>Note: SPDistribution is GFI[X, X] - its return value is the same as its choices (like Distribution in core.py).</p>"},{"location":"reference/sp/#genjax.sp.SPDistribution.random_weighted","title":"random_weighted  <code>abstractmethod</code>","text":"<pre><code>random_weighted(*args, **kwargs) -&gt; Tuple[X, jnp.ndarray]\n</code></pre> <p>Sample a value and compute its importance weight.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>@abstractmethod\ndef random_weighted(self, *args, **kwargs) -&gt; Tuple[X, jnp.ndarray]:\n    \"\"\"\n    Sample a value and compute its importance weight.\n\n    Returns:\n        value: Sampled value of type X\n        weight: Importance weight (not log weight)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SPDistribution.estimate_logpdf","title":"estimate_logpdf  <code>abstractmethod</code>","text":"<pre><code>estimate_logpdf(value: X, *args, **kwargs) -&gt; jnp.ndarray\n</code></pre> <p>Estimate the log probability density of a value.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>@abstractmethod\ndef estimate_logpdf(self, value: X, *args, **kwargs) -&gt; jnp.ndarray:\n    \"\"\"\n    Estimate the log probability density of a value.\n\n    Args:\n        value: Value to estimate density for\n\n    Returns:\n        Log probability density estimate\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SPDistribution.simulate","title":"simulate","text":"<pre><code>simulate(*args, **kwargs) -&gt; Trace[X, X]\n</code></pre> <p>Simulate by calling random_weighted.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def simulate(self, *args, **kwargs) -&gt; Trace[X, X]:\n    \"\"\"Simulate by calling random_weighted.\"\"\"\n    value, weight = self.random_weighted(*args, **kwargs)\n\n    # SP distributions have choices that equal their return value\n    from genjax.core import Tr\n    trace = Tr(\n        _gen_fn=self,\n        _args=(args, kwargs),\n        _choices=value,  # Choices are the sampled value\n        _retval=value,   # Return value is the same\n        _score=-jnp.log(weight)  # Score is negative log weight\n    )\n    return trace\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SPDistribution.assess","title":"assess","text":"<pre><code>assess(x: X, *args, **kwargs) -&gt; Tuple[Density, X]\n</code></pre> <p>Assess using estimate_logpdf.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def assess(self, x: X, *args, **kwargs) -&gt; Tuple[Density, X]:\n    \"\"\"Assess using estimate_logpdf.\"\"\"\n    # For SP distributions, x is the value to assess\n    log_density = self.estimate_logpdf(x, *args, **kwargs)\n    return log_density, x\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SPDistribution.generate","title":"generate","text":"<pre><code>generate(x: Optional[X], *args, **kwargs) -&gt; Tuple[Trace[X, X], Score]\n</code></pre> <p>Generate - for SP distributions this is just simulate.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def generate(self, x: Optional[X], *args, **kwargs) -&gt; Tuple[Trace[X, X], Score]:\n    \"\"\"Generate - for SP distributions this is just simulate.\"\"\"\n    if x is None:\n        trace = self.simulate(*args, **kwargs)\n        return trace, jnp.array(0.0)\n    else:\n        # If constrained, assess and create trace\n        log_density, retval = self.assess(x, *args, **kwargs)\n        from genjax.core import Tr\n        trace = Tr(\n            _gen_fn=self,\n            _args=(args, kwargs),\n            _choices=x,\n            _retval=retval,\n            _score=-log_density\n        )\n        return trace, log_density\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SPDistribution.update","title":"update","text":"<pre><code>update(tr: Trace[X, X], x_: X, *args, **kwargs)\n</code></pre> <p>Update not supported for SP distributions.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def update(self, tr: Trace[X, X], x_: X, *args, **kwargs):\n    \"\"\"Update not supported for SP distributions.\"\"\"\n    raise NotImplementedError(\"SPDistribution does not support update\")\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SPDistribution.regenerate","title":"regenerate","text":"<pre><code>regenerate(tr: Trace[X, X], s: Selection, *args, **kwargs)\n</code></pre> <p>Regenerate not supported for SP distributions.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def regenerate(self, tr: Trace[X, X], s: Selection, *args, **kwargs):\n    \"\"\"Regenerate not supported for SP distributions.\"\"\"\n    raise NotImplementedError(\"SPDistribution does not support regenerate\")\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SPDistribution.merge","title":"merge","text":"<pre><code>merge(x: X, x_: X, check: Optional[ndarray] = None)\n</code></pre> <p>Merge - SP distributions have no internal choices.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def merge(self, x: X, x_: X, check: Optional[jnp.ndarray] = None):\n    \"\"\"Merge - SP distributions have no internal choices.\"\"\"\n    return {}, {}\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SPDistribution.filter","title":"filter","text":"<pre><code>filter(x: X, selection: Selection)\n</code></pre> <p>Filter - SP distributions have no internal choices.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def filter(self, x: X, selection: Selection):\n    \"\"\"Filter - SP distributions have no internal choices.\"\"\"\n    return None, None\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SPDistribution.get_selection","title":"get_selection","text":"<pre><code>get_selection(x: X) -&gt; Selection\n</code></pre> <p>Get selection for SP distribution choices.</p> <p>For SPDistribution, we should determine the selection based on the actual structure of x. This is implementation-specific.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def get_selection(self, x: X) -&gt; Selection:\n    \"\"\"Get selection for SP distribution choices.\n\n    For SPDistribution, we should determine the selection based on the\n    actual structure of x. This is implementation-specific.\n    \"\"\"\n    # Delegate to standalone function\n    return get_selection(x)\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SMCAlgorithm","title":"SMCAlgorithm","text":"<p>               Bases: <code>SPDistribution</code></p> <p>Abstract base class for SMC-based SP distributions.</p> <p>Extends SPDistribution with composable SMC functionality, bridging to the GenJAX inference.smc module.</p>"},{"location":"reference/sp/#genjax.sp.SMCAlgorithm.run_smc","title":"run_smc  <code>abstractmethod</code>","text":"<pre><code>run_smc(n_particles: int, **smc_kwargs)\n</code></pre> <p>Run standard Sequential Monte Carlo algorithm.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>@abstractmethod\ndef run_smc(self, n_particles: int, **smc_kwargs):\n    \"\"\"\n    Run standard Sequential Monte Carlo algorithm.\n\n    Args:\n        key: JAX random key\n        n_particles: Number of particles to use\n        **smc_kwargs: Additional arguments for SMC algorithm\n\n    Returns:\n        ParticleCollection from inference.smc module\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/sp/#genjax.sp.SMCAlgorithm.run_csmc","title":"run_csmc  <code>abstractmethod</code>","text":"<pre><code>run_csmc(n_particles: int, retained_choices: X, **smc_kwargs)\n</code></pre> <p>Run Conditional Sequential Monte Carlo algorithm.</p> <p>Ensures one particle follows the retained trajectory while maintaining proper importance weighting.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>@abstractmethod \ndef run_csmc(self, n_particles: int, retained_choices: X, **smc_kwargs):\n    \"\"\"\n    Run Conditional Sequential Monte Carlo algorithm.\n\n    Ensures one particle follows the retained trajectory while\n    maintaining proper importance weighting.\n\n    Args:\n        key: JAX random key\n        n_particles: Number of particles to use\n        retained_choices: Choices for the retained particle trajectory\n        **smc_kwargs: Additional arguments for SMC algorithm\n\n    Returns:\n        ParticleCollection where one particle matches retained_choices\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/sp/#genjax.sp.Target","title":"Target","text":"<p>               Bases: <code>Pytree</code></p> <p>Represents an unnormalized target distribution.</p> <p>Combines a generative function with arguments and observations to define a posterior distribution over latent variables.</p>"},{"location":"reference/sp/#genjax.sp.Target.get_latents","title":"get_latents","text":"<pre><code>get_latents(trace: Trace[X, R]) -&gt; X\n</code></pre> <p>Extract latent (unobserved) choices from a trace.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def get_latents(self, trace: Trace[X, R]) -&gt; X:\n    \"\"\"Extract latent (unobserved) choices from a trace.\"\"\"\n    all_choices = trace.get_choices()\n    # Filter out observed addresses\n    latents, _ = self.model.filter(all_choices, ~self._obs_selection())\n    return latents\n</code></pre>"},{"location":"reference/sp/#genjax.sp.ImportanceSampling","title":"ImportanceSampling","text":"<p>               Bases: <code>SMCAlgorithm</code>, <code>Pytree</code></p> <p>Importance sampling as an SPDistribution.</p> <p>Samples from a target distribution using a proposal distribution and importance weighting.</p>"},{"location":"reference/sp/#genjax.sp.ImportanceSampling.random_weighted","title":"random_weighted","text":"<pre><code>random_weighted(*args, **kwargs) -&gt; Tuple[X, jnp.ndarray]\n</code></pre> <p>Sample using importance sampling with vectorization.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def random_weighted(self, *args, **kwargs) -&gt; Tuple[X, jnp.ndarray]:\n    \"\"\"Sample using importance sampling with vectorization.\"\"\"\n    from genjax.pjax import modular_vmap\n\n    if self.proposal is None:\n        # Use target's internal proposal\n        def single_particle(_):\n            return self.target.model.generate(\n                self.target.observations, \n                *self.target.args\n            )\n\n        # Vectorize over n_particles\n        vectorized_generate = modular_vmap(single_particle, in_axes=(0,))\n        traces, log_weights = vectorized_generate(jnp.arange(self.n_particles.value))\n    else:\n        # Use custom proposal\n        def single_particle_custom(_):\n            # Sample from proposal\n            proposal_trace = self.proposal.simulate(*self.target.args)\n            proposal_choices = proposal_trace.get_choices()\n\n            # Merge with observations\n            merged_choices, _ = self.target.model.merge(\n                proposal_choices, \n                self.target.observations\n            )\n\n            # Generate from target\n            target_trace, target_weight = self.target.model.generate(\n                merged_choices, \n                *self.target.args\n            )\n\n            # Compute importance weight\n            proposal_score = proposal_trace.get_score()\n            log_weight = target_weight + proposal_score\n\n            return target_trace, log_weight\n\n        # Vectorize\n        vectorized_generate = modular_vmap(single_particle_custom, in_axes=(0,))\n        traces, log_weights = vectorized_generate(jnp.arange(self.n_particles.value))\n\n    # Sample particle according to weights\n    log_probs = log_weights - jax.scipy.special.logsumexp(log_weights)\n    idx = categorical.sample(log_probs)\n\n    # Extract latent choices from selected particle\n    # Use tree_map to index into vectorized trace structure\n    selected_trace_choices = jax.tree.map(lambda x: x[idx], traces.get_choices())\n\n    # Get latents - don't assume structure, use target's method\n    selected_trace = traces.__class__(\n        _gen_fn=traces._gen_fn,\n        _args=traces._args,\n        _choices=selected_trace_choices,\n        _retval=jax.tree.map(lambda x: x[idx], traces._retval),\n        _score=traces._score[idx]\n    )\n    latent_choices = self.target.get_latents(selected_trace)\n\n    # Compute weight estimate\n    weight = jnp.exp(jax.scipy.special.logsumexp(log_weights) - jnp.log(self.n_particles.value))\n\n    return latent_choices, weight\n</code></pre>"},{"location":"reference/sp/#genjax.sp.ImportanceSampling.estimate_logpdf","title":"estimate_logpdf","text":"<pre><code>estimate_logpdf(value: X, *args, **kwargs) -&gt; jnp.ndarray\n</code></pre> <p>Estimate log probability using importance sampling with vectorization.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def estimate_logpdf(self, value: X, *args, **kwargs) -&gt; jnp.ndarray:\n    \"\"\"Estimate log probability using importance sampling with vectorization.\"\"\"\n    from genjax.pjax import modular_vmap\n\n    # Merge value with observations once\n    merged_choices, _ = self.target.model.merge(value, self.target.observations)\n\n    if self.proposal is None:\n        # Assess directly on target\n        def single_assess(_):\n            log_density, _ = self.target.model.assess(\n                merged_choices,\n                *self.target.args\n            )\n            return log_density\n\n        # Vectorize\n        vectorized_assess = modular_vmap(single_assess, in_axes=(0,))\n        log_weights = vectorized_assess(jnp.arange(self.n_particles.value))\n    else:\n        # Compute importance weights\n        def single_importance(_):\n            # Assess target\n            target_log_density, _ = self.target.model.assess(\n                merged_choices,\n                *self.target.args\n            )\n\n            # Assess proposal  \n            proposal_log_density, _ = self.proposal.assess(\n                value,\n                *self.target.args\n            )\n\n            return target_log_density - proposal_log_density\n\n        # Vectorize\n        vectorized_importance = modular_vmap(single_importance, in_axes=(0,))\n        log_weights = vectorized_importance(jnp.arange(self.n_particles.value))\n\n    # Estimate log probability\n    log_prob_estimate = jax.scipy.special.logsumexp(log_weights) - jnp.log(self.n_particles.value)\n\n    return log_prob_estimate\n</code></pre>"},{"location":"reference/sp/#genjax.sp.ImportanceSampling.run_smc","title":"run_smc","text":"<pre><code>run_smc(n_particles: int, **smc_kwargs)\n</code></pre> <p>Run SMC algorithm using existing importance sampling implementation.</p> <p>Bridges to GenJAX inference.smc.init functionality.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def run_smc(self, n_particles: int, **smc_kwargs):\n    \"\"\"\n    Run SMC algorithm using existing importance sampling implementation.\n\n    Bridges to GenJAX inference.smc.init functionality.\n    \"\"\"\n    from genjax.inference.smc import init\n    from genjax.core import const\n\n    return init(\n        target_gf=self.target.model,\n        target_args=self.target.args,\n        n_samples=const(n_particles),\n        constraints=self.target.observations,\n        proposal_gf=self.proposal,\n    )\n</code></pre>"},{"location":"reference/sp/#genjax.sp.ImportanceSampling.run_csmc","title":"run_csmc","text":"<pre><code>run_csmc(n_particles: int, retained_choices: X, **smc_kwargs)\n</code></pre> <p>Run Conditional SMC algorithm with retained particle.</p> <p>Uses conditional SMC functionality from inference.smc module.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def run_csmc(self, n_particles: int, retained_choices: X, **smc_kwargs):\n    \"\"\"\n    Run Conditional SMC algorithm with retained particle.\n\n    Uses conditional SMC functionality from inference.smc module.\n    \"\"\"\n    from genjax.inference.smc import init_csmc\n    from genjax.core import const\n\n    return init_csmc(\n        target_gf=self.target.model,\n        target_args=self.target.args,\n        n_samples=const(n_particles),\n        constraints=self.target.observations,\n        retained_choices=retained_choices,\n        proposal_gf=self.proposal,\n    )\n</code></pre>"},{"location":"reference/sp/#genjax.sp.Marginal","title":"Marginal","text":"<p>               Bases: <code>SPDistribution</code>, <code>Pytree</code></p> <p>Marginal distribution over a specific address using an SMC algorithm.</p> <p>Following GenSP.jl design: parameterized by an algorithm that handles the actual inference, while Marginal specifies which address to marginalize.</p> <p>Returns the value at the specified address extracted from algorithm samples.</p>"},{"location":"reference/sp/#genjax.sp.Marginal.random_weighted","title":"random_weighted","text":"<pre><code>random_weighted(*args, **kwargs) -&gt; Tuple[X, jnp.ndarray]\n</code></pre> <p>Sample from marginal distribution using the algorithm.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def random_weighted(self, *args, **kwargs) -&gt; Tuple[X, jnp.ndarray]:\n    \"\"\"Sample from marginal distribution using the algorithm.\"\"\"\n    # Use the algorithm's own random_weighted method to get a sample\n    full_sample, weight = self.algorithm.random_weighted(*args, **kwargs)\n\n    # Extract the value at the specified address\n    marginal_value = self._extract_address(full_sample, self.address)\n\n    return marginal_value, weight\n</code></pre>"},{"location":"reference/sp/#genjax.sp.Marginal.estimate_logpdf","title":"estimate_logpdf","text":"<pre><code>estimate_logpdf(value: X, *args, **kwargs) -&gt; jnp.ndarray\n</code></pre> <p>Estimate marginal log probability using the algorithm.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def estimate_logpdf(self, value: X, *args, **kwargs) -&gt; jnp.ndarray:\n    \"\"\"Estimate marginal log probability using the algorithm.\"\"\"\n    # Build constraint dictionary with value at the marginal address\n    constraints = self._build_constraint(value, self.address)\n\n    # Use the algorithm to estimate the log density\n    # Note: This is a simplified approach. A full implementation would need\n    # to properly handle the marginalization in the density estimation.\n    return self.algorithm.estimate_logpdf(constraints, *args, **kwargs)\n</code></pre>"},{"location":"reference/sp/#genjax.sp.get_selection","title":"get_selection","text":"<pre><code>get_selection(x: X) -&gt; Selection\n</code></pre> <p>Create a Selection object from a choice map.</p> <p>This function creates a Selection that matches all addresses present in the given choice map structure. It handles different types of choice maps used by various generative functions:</p> <ul> <li>None: Returns NoneSel (matches no addresses)</li> <li>dict: Returns selection matching all keys in the dictionary</li> <li>other: Returns AllSel (matches all addresses)</li> </ul> Source code in <code>src/genjax/sp.py</code> <pre><code>def get_selection(x: X) -&gt; Selection:\n    \"\"\"Create a Selection object from a choice map.\n\n    This function creates a Selection that matches all addresses present\n    in the given choice map structure. It handles different types of\n    choice maps used by various generative functions:\n\n    - None: Returns NoneSel (matches no addresses)\n    - dict: Returns selection matching all keys in the dictionary\n    - other: Returns AllSel (matches all addresses)\n\n    Args:\n        x: Choice map structure (could be None, dict, or other types)\n\n    Returns:\n        Selection object matching addresses in the choice map\n    \"\"\"\n    from genjax.core import Selection, AllSel, NoneSel, DictSel\n\n    if x is None:\n        # No choices - match nothing\n        return Selection(NoneSel())\n    elif isinstance(x, dict):\n        # Dictionary of choices - create selection from keys\n        if not x:\n            # Empty dict - match nothing\n            return Selection(NoneSel())\n        else:\n            # Create nested selection structure matching dict structure\n            sel_dict = {}\n            for key, value in x.items():\n                if isinstance(value, dict):\n                    # Recursively handle nested dicts\n                    sel_dict[key] = get_selection(value)\n                else:\n                    # Leaf value - select this key\n                    sel_dict[key] = Selection(AllSel())\n            return Selection(DictSel(sel_dict))\n    else:\n        # Other types (e.g., raw values from Distribution) - match all\n        return Selection(AllSel())\n</code></pre>"},{"location":"reference/sp/#genjax.sp.importance_sampling","title":"importance_sampling","text":"<pre><code>importance_sampling(target: Target, proposal: Optional[GFI[X, Any]] = None, n_particles: int = 100) -&gt; ImportanceSampling\n</code></pre> <p>Create an importance sampling SPDistribution.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def importance_sampling(\n    target: Target,\n    proposal: Optional[GFI[X, Any]] = None,\n    n_particles: int = 100\n) -&gt; \"ImportanceSampling\":\n    \"\"\"\n    Create an importance sampling SPDistribution.\n\n    Args:\n        target: Target distribution to sample from\n        proposal: Optional custom proposal (uses target's internal if None)\n        n_particles: Number of particles for importance sampling\n\n    Returns:\n        ImportanceSampling SPDistribution\n    \"\"\"\n    return ImportanceSampling(\n        target=target,\n        proposal=proposal,\n        n_particles=const(n_particles)\n    )\n</code></pre>"},{"location":"reference/sp/#genjax.sp.marginal","title":"marginal","text":"<pre><code>marginal(algorithm: SMCAlgorithm, address: str | Tuple[str, ...]) -&gt; Marginal\n</code></pre> <p>Create a marginal distribution over a specific address using an algorithm.</p> <p>Following GenSP.jl design: parameterized by an algorithm.</p> Source code in <code>src/genjax/sp.py</code> <pre><code>def marginal(\n    algorithm: SMCAlgorithm,\n    address: str | Tuple[str, ...]\n) -&gt; \"Marginal\":\n    \"\"\"\n    Create a marginal distribution over a specific address using an algorithm.\n\n    Following GenSP.jl design: parameterized by an algorithm.\n\n    Args:\n        algorithm: SMC algorithm to use for inference\n        address: Address to extract marginal for\n\n    Returns:\n        Marginal SPDistribution\n    \"\"\"\n    return Marginal(\n        algorithm=algorithm,\n        address=address\n    )\n</code></pre>"},{"location":"reference/state/","title":"genjax.state","text":"<p>State interpreter for inspecting and manipulating probabilistic computations.</p>"},{"location":"reference/state/#genjax.state","title":"state","text":"<p>JAX interpreter for inspecting and organizing tagged state inside JAX Python functions.</p> <p>This module provides a State interpreter that can collect and hierarchically organize tagged values from within JAX computations using JAX primitives. The interpreter works seamlessly with all JAX transformations while providing powerful state organization capabilities.</p>"},{"location":"reference/state/#genjax.state--core-features","title":"Core Features:","text":"<p>State Collection: Tag intermediate values during computation for inspection Hierarchical Organization: Use namespaces to create nested state structures JAX Integration: Full compatibility with jit, vmap, grad, scan, and other JAX transforms Error Safety: Automatic cleanup of namespace stack on exceptions Zero Overhead: No performance cost when not using the @state decorator</p>"},{"location":"reference/state/#genjax.state--primary-api","title":"Primary API:","text":"<p>Basic State Collection: - <code>state(f)</code>: Transform function to collect tagged state values - <code>save(**tagged_values)</code>: Tag multiple values by name (named mode) - <code>save(*values)</code>: Save values directly at current namespace leaf (leaf mode)</p> <p>Hierarchical Organization: - <code>namespace(fn, ns)</code>: Transform function to collect state under namespace - Supports arbitrary nesting: <code>namespace(namespace(fn, \"inner\"), \"outer\")</code></p>"},{"location":"reference/state/#genjax.state--lower-level-api","title":"Lower-level API:","text":"<ul> <li><code>tag_state(*values, name=\"...\")</code>: Tag individual values for collection</li> </ul>"},{"location":"reference/state/#genjax.state--usage-examples","title":"Usage Examples:","text":"<p>Basic state collection: <pre><code>@state\ndef computation(x):\n    y = x + 1\n    save(intermediate=y, doubled=x*2)\n    return y\n\nresult, state_dict = computation(5)\n# state_dict = {\"intermediate\": 6, \"doubled\": 10}\n</code></pre></p> <p>Hierarchical organization with named mode: <pre><code>@state\ndef complex_computation(x):\n    save(input=x)\n\n    # Namespace for processing steps (named mode)\n    processing = namespace(\n        lambda: save(step1=x*2, step2=x+1),\n        \"processing\"\n    )\n    processing()\n\n    # Nested namespaces\n    analysis = namespace(\n        namespace(lambda: save(mean=x), \"stats\"),\n        \"analysis\"\n    )\n    analysis()\n\n    return x\n\nresult, state_dict = complex_computation(5)\n# state_dict = {\n#     \"input\": 5,\n#     \"processing\": {\"step1\": 10, \"step2\": 6},\n#     \"analysis\": {\"stats\": {\"mean\": 5}}\n# }\n</code></pre></p> <p>Leaf mode for direct namespace storage: <pre><code>@state\ndef leaf_computation(x):\n    save(input=x)\n\n    # Leaf mode: save values directly at namespace (no additional keys)\n    coords = namespace(lambda: save(x, x*2, x*3), \"coordinates\")\n    coords()\n\n    # Mixed with named mode in different namespace\n    stats = namespace(lambda: save(mean=x, variance=x**2), \"statistics\")\n    stats()\n\n    return x\n\nresult, state_dict = leaf_computation(5)\n# state_dict = {\n#     \"input\": 5,\n#     \"coordinates\": (5, 10, 15),  # Leaf mode: tuple stored directly\n#     \"statistics\": {\"mean\": 5, \"variance\": 25}  # Named mode: dict\n# }\n</code></pre></p> <p>JAX Integration: <pre><code># Works with all JAX transformations\njitted_fn = jax.jit(computation)\nvmapped_fn = jax.vmap(computation)\ngrad_fn = jax.grad(lambda x: computation(x)[0])\n</code></pre></p>"},{"location":"reference/state/#genjax.state--implementation-details","title":"Implementation Details:","text":"<p>The state interpreter uses JAX primitives (<code>state_p</code>, <code>namespace_push_p</code>, <code>namespace_pop_p</code>) to integrate with JAX's transformation system. This ensures proper behavior under jit, vmap, grad, and other JAX transforms.</p> <p>The namespace functionality is implemented using a stack-based approach where namespace push/pop operations are tracked via JAX primitives, allowing the interpreter to maintain correct hierarchical structure even under complex JAX transformations.</p>"},{"location":"reference/state/#genjax.state.State","title":"State  <code>dataclass</code>","text":"<pre><code>State(collected_state: dict[str, Any], namespace_stack: list[str] = list())\n</code></pre> <p>JAX interpreter that collects tagged state values.</p> <p>This interpreter processes JAX computations and collects values that are tagged with the <code>state_p</code> primitive. Tagged values are accumulated and returned alongside the original computation result.</p>"},{"location":"reference/state/#genjax.state.State.eval_jaxpr_state","title":"eval_jaxpr_state","text":"<pre><code>eval_jaxpr_state(jaxpr: Jaxpr, consts: list[Any], args: list[Any])\n</code></pre> <p>Evaluate a jaxpr while collecting tagged state values.</p> Source code in <code>src/genjax/state.py</code> <pre><code>def eval_jaxpr_state(\n    self,\n    jaxpr: Jaxpr,\n    consts: list[Any],\n    args: list[Any],\n):\n    \"\"\"Evaluate a jaxpr while collecting tagged state values.\"\"\"\n    env = Environment()\n    safe_map(env.write, jaxpr.constvars, consts)\n    safe_map(env.write, jaxpr.invars, args)\n\n    for eqn in jaxpr.eqns:\n        invals = safe_map(env.read, eqn.invars)\n        subfuns, params = eqn.primitive.get_bind_params(eqn.params)\n        args = subfuns + invals\n        primitive, inner_params = PPPrimitive.unwrap(eqn.primitive)\n\n        if primitive == state_p:\n            # Collect the tagged values with namespace support\n            name = params.get(\"name\", inner_params.get(\"name\"))\n            if name is None:\n                raise ValueError(\"tag_state requires a 'name' parameter\")\n            values = list(invals) if invals else []\n            value = (\n                tuple(values)\n                if len(values) &gt; 1\n                else (values[0] if values else None)\n            )\n\n            # Handle leaf mode storage (special case for save(*args))\n            if name == \"__NAMESPACE_LEAF__\":\n                namespace_path = tuple(self.namespace_stack)\n                if namespace_path:\n                    # Store directly at the namespace path (no additional key)\n                    current = self.collected_state\n                    for namespace in namespace_path[:-1]:\n                        if namespace not in current:\n                            current[namespace] = {}\n                        current = current[namespace]\n                    # Store at the final namespace level\n                    current[namespace_path[-1]] = value\n                else:\n                    # If no namespace, we can't do leaf storage at root\n                    raise ValueError(\n                        \"Leaf mode save() requires being inside a namespace\"\n                    )\n            else:\n                # Handle namespace path using interpreter's stack (named mode)\n                namespace_path = tuple(self.namespace_stack)\n                if namespace_path:\n                    _nested_dict_set(\n                        self.collected_state, namespace_path, name, value\n                    )\n                else:\n                    self.collected_state[name] = value\n\n            # The state primitive returns the values as-is due to multiple_results\n            outvals = values\n\n        elif primitive == namespace_push_p:\n            # Push namespace onto interpreter's stack\n            namespace = params.get(\"namespace\", inner_params.get(\"namespace\"))\n            if namespace is None:\n                raise ValueError(\"namespace_push requires a 'namespace' parameter\")\n            self.namespace_stack.append(namespace)\n            # Namespace push doesn't take or return values\n            outvals = []\n\n        elif primitive == namespace_pop_p:\n            # Pop namespace from interpreter's stack\n            if not self.namespace_stack:\n                raise ValueError(\"namespace_pop called with empty namespace stack\")\n            self.namespace_stack.pop()\n            # Namespace pop doesn't take or return values\n            outvals = []\n\n        elif primitive == scan_p:\n            # Handle scan primitive by transforming body to collect state\n            body_jaxpr = params[\"jaxpr\"]\n            length = params[\"length\"]\n            reverse = params[\"reverse\"]\n            num_consts = params[\"num_consts\"]\n            num_carry = params[\"num_carry\"]\n            const_vals, carry_vals, xs_vals = split_list(\n                invals, [num_consts, num_carry]\n            )\n\n            body_fun = jex.core.jaxpr_as_fun(body_jaxpr)\n\n            def new_body(carry, scanned_in):\n                in_carry = carry\n                all_values = const_vals + jtu.tree_leaves((in_carry, scanned_in))\n                # Apply state transformation to the body\n                body_result, body_state = state(body_fun)(*all_values)\n                # Split the body result back into carry and scan parts\n                out_carry, out_scan = split_list(\n                    jtu.tree_leaves(body_result), [num_carry]\n                )\n                # Return carry, scan output, and collected state\n                return out_carry, (out_scan, body_state)\n\n            flat_carry_out, (scanned_out, scan_states) = scan(\n                new_body,\n                carry_vals,\n                xs_vals,\n                length=length,\n                reverse=reverse,\n            )\n\n            # Merge vectorized scan states into collected state\n            # scan_states is already vectorized by scan - just merge it\n            for name, vectorized_values in scan_states.items():\n                self.collected_state[name] = vectorized_values\n\n            outvals = jtu.tree_leaves(\n                (flat_carry_out, scanned_out),\n            )\n\n        else:\n            # For all other primitives, use normal JAX evaluation\n            outvals = eqn.primitive.bind(*args, **params)\n            if not eqn.outvars:\n                outvals = []\n            elif isinstance(outvals, (list, tuple)):\n                outvals = list(outvals)\n            else:\n                outvals = [outvals]\n\n        safe_map(env.write, eqn.outvars, outvals)\n\n    return safe_map(env.read, jaxpr.outvars)\n</code></pre>"},{"location":"reference/state/#genjax.state.State.eval","title":"eval","text":"<pre><code>eval(fn, *args)\n</code></pre> <p>Run the interpreter on a function with given arguments.</p> Source code in <code>src/genjax/state.py</code> <pre><code>def eval(self, fn, *args):\n    \"\"\"Run the interpreter on a function with given arguments.\"\"\"\n    closed_jaxpr, (flat_args, _, out_tree) = stage(fn)(*args)\n    jaxpr, consts = closed_jaxpr.jaxpr, closed_jaxpr.literals\n    flat_out = self.eval_jaxpr_state(\n        jaxpr,\n        consts,\n        flat_args,\n    )\n    result = jtu.tree_unflatten(out_tree(), flat_out)\n    return result, self.collected_state\n</code></pre>"},{"location":"reference/state/#genjax.state.state","title":"state","text":"<pre><code>state(f: Callable[..., Any])\n</code></pre> <p>Transform a function to collect tagged state values.</p> <p>This transformation wraps a function to intercept and collect values that are tagged with the <code>state_p</code> primitive. The transformed function returns both the original result and a dictionary of collected state.</p> Example <p>from genjax.state import state, save</p> <p>@state def computation(x): ...     y = x + 1 ...     z = x * 2 ...     values = save(intermediate=y, doubled=z) ...     return values[\"intermediate\"] * 2</p> <p>result, state_dict = computation(5) print(result)  # 12 print(state_dict)  # {\"intermediate\": 6, \"doubled\": 10}</p> Source code in <code>src/genjax/state.py</code> <pre><code>def state(f: Callable[..., Any]):\n    \"\"\"Transform a function to collect tagged state values.\n\n    This transformation wraps a function to intercept and collect values\n    that are tagged with the `state_p` primitive. The transformed function\n    returns both the original result and a dictionary of collected state.\n\n    Args:\n        f: Function containing state tags to transform.\n\n    Returns:\n        Function that returns a tuple of (original_result, collected_state).\n\n    Example:\n        &gt;&gt;&gt; from genjax.state import state, save\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @state\n        &gt;&gt;&gt; def computation(x):\n        ...     y = x + 1\n        ...     z = x * 2\n        ...     values = save(intermediate=y, doubled=z)\n        ...     return values[\"intermediate\"] * 2\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; result, state_dict = computation(5)\n        &gt;&gt;&gt; print(result)  # 12\n        &gt;&gt;&gt; print(state_dict)  # {\"intermediate\": 6, \"doubled\": 10}\n    \"\"\"\n\n    @wraps(f)\n    def wrapped(*args):\n        interpreter = State(collected_state={}, namespace_stack=[])\n        return interpreter.eval(f, *args)\n\n    return wrapped\n</code></pre>"},{"location":"reference/state/#genjax.state.tag_state","title":"tag_state","text":"<pre><code>tag_state(*values: Any, name: str) -&gt; Any\n</code></pre> <p>Tag one or more values to be collected by the StateInterpreter.</p> <p>Note: Consider using <code>save(**tagged_values)</code> for most use cases, as it provides a more convenient API for tagging multiple values.</p> <p>This function marks values to be collected when the computation is run through the <code>state</code> transformation. The values are passed through unchanged in normal execution.</p> Example <p>x = 42 y = tag_state(x, name=\"my_value\")  # y == x == 42</p> Source code in <code>src/genjax/state.py</code> <pre><code>def tag_state(*values: Any, name: str) -&gt; Any:\n    \"\"\"Tag one or more values to be collected by the StateInterpreter.\n\n    **Note: Consider using `save(**tagged_values)` for most use cases, as it\n    provides a more convenient API for tagging multiple values.**\n\n    This function marks values to be collected when the computation\n    is run through the `state` transformation. The values are passed\n    through unchanged in normal execution.\n\n    Args:\n        *values: The values to tag and collect.\n        name: Required string identifier for this state value.\n\n    Returns:\n        The original values (identity function). If single value, returns\n        the value directly. If multiple values, returns a tuple.\n\n    Example:\n        &gt;&gt;&gt; x = 42\n        &gt;&gt;&gt; y = tag_state(x, name=\"my_value\")  # y == x == 42\n        &gt;&gt;&gt; # Multiple values\n        &gt;&gt;&gt; a, b = tag_state(1, 2, name=\"pair\")  # a == 1, b == 2\n        &gt;&gt;&gt; # When run through state() transformation,\n        &gt;&gt;&gt; # values will be collected in state dict\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Prefer save() for multiple named values:\n        &gt;&gt;&gt; values = save(x=42, y=24)  # More convenient\n    \"\"\"\n    if not values:\n        raise ValueError(\"tag_state requires at least one value\")\n\n    # Use initial_style_bind for proper JAX transformation compatibility\n    def identity_fn(*args):\n        return tuple(args) if len(args) &gt; 1 else args[0]\n\n    # Create a batch rule that re-inserts the primitive under vmap\n    def batch_rule(vector_args, dims, **params):\n        # Re-insert the state primitive with the vectorized args\n        def vectorized_identity(*args):\n            return tuple(args) if len(args) &gt; 1 else args[0]\n\n        # Apply the primitive to the vectorized args\n        result = initial_style_bind(\n            state_p,\n            batch=batch_rule,  # Self-reference for nested vmaps\n        )(vectorized_identity, name=params.get(\"name\"))(*vector_args)\n\n        # Return result with appropriate batching dimensions\n        if isinstance(result, tuple):\n            # For multiple outputs, each has the same dims as inputs\n            return result, tuple(dims[0] if dims else () for _ in result)\n        else:\n            # For single output, return as tuple (JAX expects a sequence for dims_out)\n            return (result,), (dims[0] if dims else (),)\n\n    result = initial_style_bind(\n        state_p,\n        batch=batch_rule,\n    )(identity_fn, name=name)(*values)\n\n    return result\n</code></pre>"},{"location":"reference/state/#genjax.state.tag_state--multiple-values","title":"Multiple values","text":"<p>a, b = tag_state(1, 2, name=\"pair\")  # a == 1, b == 2</p>"},{"location":"reference/state/#genjax.state.tag_state--when-run-through-state-transformation","title":"When run through state() transformation,","text":""},{"location":"reference/state/#genjax.state.tag_state--values-will-be-collected-in-state-dict","title":"values will be collected in state dict","text":""},{"location":"reference/state/#genjax.state.tag_state--prefer-save-for-multiple-named-values","title":"Prefer save() for multiple named values:","text":"<p>values = save(x=42, y=24)  # More convenient</p>"},{"location":"reference/state/#genjax.state.save","title":"save","text":"<pre><code>save(*values, **tagged_values) -&gt; Any\n</code></pre> <p>Save values either at current namespace leaf or with explicit names (primary API).</p> <p>This is the recommended way to tag state values. Supports two modes:</p> <ol> <li>Leaf mode (<code>*args</code>): Save values directly at current namespace leaf</li> <li>Named mode (<code>**kwargs</code>): Save values with explicit names (original behavior)</li> </ol> Example <p>Leaf mode (saves at current namespace):</p> <p>@state def computation(): ...     namespace_fn = namespace(lambda: save(1, 2, 3), \"coords\") ...     namespace_fn() ...     return 42 result, state_dict = computation()</p> <p>Named mode (original behavior):</p> <p>@state def computation(): ...     values = save(first=1, second=2) ...     return sum(values.values()) result, state_dict = computation()</p> Source code in <code>src/genjax/state.py</code> <pre><code>def save(*values, **tagged_values) -&gt; Any:\n    \"\"\"Save values either at current namespace leaf or with explicit names (primary API).\n\n    **This is the recommended way to tag state values.** Supports two modes:\n\n    1. **Leaf mode** (`*args`): Save values directly at current namespace leaf\n    2. **Named mode** (`**kwargs`): Save values with explicit names (original behavior)\n\n    Args:\n        *values: Values to save at current namespace leaf (mutually exclusive with **tagged_values)\n        **tagged_values: Keyword arguments where keys are names and values are the values to save\n\n    Returns:\n        - Leaf mode: The values as a tuple (or single value if only one)\n        - Named mode: Dictionary of the saved values (for convenience)\n\n    Example:\n        Leaf mode (saves at current namespace):\n        &gt;&gt;&gt; @state\n        &gt;&gt;&gt; def computation():\n        ...     namespace_fn = namespace(lambda: save(1, 2, 3), \"coords\")\n        ...     namespace_fn()\n        ...     return 42\n        &gt;&gt;&gt; result, state_dict = computation()\n        &gt;&gt;&gt; # state_dict == {\"coords\": (1, 2, 3)}\n\n        Named mode (original behavior):\n        &gt;&gt;&gt; @state\n        &gt;&gt;&gt; def computation():\n        ...     values = save(first=1, second=2)\n        ...     return sum(values.values())\n        &gt;&gt;&gt; result, state_dict = computation()\n        &gt;&gt;&gt; # state_dict == {\"first\": 1, \"second\": 2}\n    \"\"\"\n    if values and tagged_values:\n        raise ValueError(\n            \"Cannot use both positional args (*values) and keyword args (**tagged_values) in save()\"\n        )\n\n    if values:\n        # Leaf mode: save values directly at current namespace leaf\n        # Use a special reserved name to indicate leaf storage\n        leaf_value = values if len(values) &gt; 1 else values[0]\n        tag_state(leaf_value, name=\"__NAMESPACE_LEAF__\")\n        return leaf_value\n    else:\n        # Named mode: original behavior with explicit names\n        result = {}\n        for name, value in tagged_values.items():\n            result[name] = tag_state(value, name=name)\n        return result\n</code></pre>"},{"location":"reference/state/#genjax.state.save--state_dict","title":"state_dict ==","text":""},{"location":"reference/state/#genjax.state.save--state_dict_1","title":"state_dict ==","text":""},{"location":"reference/state/#genjax.state.namespace","title":"namespace","text":"<pre><code>namespace(f: Callable[..., Any], ns: str) -&gt; Callable[..., Any]\n</code></pre> <p>Transform a function to collect state under a namespace.</p> <p>This function wraps another function so that any state collected within it will be organized under the specified namespace. Namespaces can be nested by applying this function multiple times.</p> Example <p>@state def computation(x): ...     # State collected directly at root level ...     save(root_val=x) ... ...     # State collected under \"inner\" namespace ...     inner_fn = namespace(lambda y: save(nested_val=y * 2), \"inner\") ...     inner_fn(x) ... ...     # Nested namespaces: state under \"outer.deep\" ...     deep_fn = namespace( ...         namespace(lambda z: save(deep_val=z * 3), \"deep\"), ...         \"outer\" ...     ) ...     deep_fn(x) ... ...     return x</p> <p>result, state_dict = computation(5)</p> Source code in <code>src/genjax/state.py</code> <pre><code>def namespace(f: Callable[..., Any], ns: str) -&gt; Callable[..., Any]:\n    \"\"\"Transform a function to collect state under a namespace.\n\n    This function wraps another function so that any state collected within\n    it will be organized under the specified namespace. Namespaces can be\n    nested by applying this function multiple times.\n\n    Args:\n        f: Function to wrap with namespace context\n        ns: Namespace string to organize state under\n\n    Returns:\n        Function that collects state under the specified namespace\n\n    Example:\n        &gt;&gt;&gt; @state\n        &gt;&gt;&gt; def computation(x):\n        ...     # State collected directly at root level\n        ...     save(root_val=x)\n        ...\n        ...     # State collected under \"inner\" namespace\n        ...     inner_fn = namespace(lambda y: save(nested_val=y * 2), \"inner\")\n        ...     inner_fn(x)\n        ...\n        ...     # Nested namespaces: state under \"outer.deep\"\n        ...     deep_fn = namespace(\n        ...         namespace(lambda z: save(deep_val=z * 3), \"deep\"),\n        ...         \"outer\"\n        ...     )\n        ...     deep_fn(x)\n        ...\n        ...     return x\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; result, state_dict = computation(5)\n        &gt;&gt;&gt; # state_dict == {\n        &gt;&gt;&gt; #     \"root_val\": 5,\n        &gt;&gt;&gt; #     \"inner\": {\"nested_val\": 10},\n        &gt;&gt;&gt; #     \"outer\": {\"deep\": {\"deep_val\": 15}}\n        &gt;&gt;&gt; # }\n    \"\"\"\n\n    @wraps(f)\n    def namespaced_fn(*args, **kwargs):\n        # Push namespace using JAX primitive\n        _namespace_push(ns)\n        try:\n            result = f(*args, **kwargs)\n            return result\n        finally:\n            # Always pop namespace, even if function raises\n            _namespace_pop()\n\n    return namespaced_fn\n</code></pre>"},{"location":"reference/state/#genjax.state.namespace--state_dict","title":"state_dict == {","text":""},{"location":"reference/state/#genjax.state.namespace--root_val-5","title":"\"root_val\": 5,","text":""},{"location":"reference/state/#genjax.state.namespace--inner-nested_val-10","title":"\"inner\": {\"nested_val\": 10},","text":""},{"location":"reference/state/#genjax.state.namespace--outer-deep-deep_val-15","title":"\"outer\": {\"deep\": {\"deep_val\": 15}}","text":""},{"location":"reference/state/#genjax.state.namespace--_1","title":"}","text":""}]}